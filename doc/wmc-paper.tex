\documentclass[letterpaper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xnewcommand}

%\newtheorem{proposition}{Proposition}

\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lit}{\ell}

\newcommand{\progname}[1]{\textsc{#1}}
\newcommand{\dfour}{\progname{D4}}
\newcommand{\Dfour}{\progname{D4}}

\newcommand{\approximate}[1]{\hat{#1}}
\newcommand{\approxP}{\approximate{P}}
\newcommand{\approxx}{\approximate{x}}
\newcommand{\approxy}{\approximate{y}}
\newcommand{\approxw}{\approximate{w}}
\newcommand{\approxW}{\approximate{W}}
\newcommand{\approxs}{\approximate{s}}
\newcommand{\round}{\mathit{Round}}
\newcommand{\aerror}{\delta}
\newcommand{\digitprecision}{\Delta}
\newcommand{\minvalue}{\omega}
\newcommand{\maxValue}{\Omega}
\newcommand{\roundepsilon}{\varepsilon}
\newcommand{\xmin}{x_{\textrm{min}}}
\newcommand{\xmax}{x_{\textrm{max}}}

\newcommand{\varset}{X}
\newcommand{\dependencyset}{{\cal V}}
\newcommand{\assign}{\alpha}
\newcommand{\modelset}{{\cal M}}
\newcommand{\entails}{\models}


\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Arithmetic Considerations in Weighted Model Counting}

\titlerunning{Arithmetic Considerations for Weighted Model Counting}

\author{Randal E. Bryant}{Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA}{Randy.Bryant@cs.cmu.edu}{https://orcid.org/0000-0001-5024-6613}{}

\authorrunning{R. E. Bryant} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Randal E. Bryant} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Weighted model counting computes the sum of the weights
  associated with the satisfying assignments for a Boolean formula,
  where the weight of an assignment is given by the product of the
  weights associated with the positive and negated variables
  comprising the assignment.  Weighted model counting finds
  applications in a wide variety of domains including machine learning
  and quantitative risk assignment.

  Most weighted model counting programs avoid enumerating all
  assignments to a formula by converting it, either explicitly or
  implicity, into a set of conjunction and disjunction operations,
  such that the count can be evaluating using multiplication for
  conjunctions and addition for disjunctions.

  Although computing these operations with rational arithmetic
  yields exact results, the precision required for most applications
  does not warrant the required space and time.  On
  the other hand, the precision when using floating-point arithmetic
  cannot be guaranteed, especially when some of the weights are
  negative.

  In this paper, we prove that, when all weights are nonnegative, the
  precision of floating-point arithmetic will degrade, in the worst case, as the logarithm
  of the number of variables.  This result, along with our
  experiments, demonstrate that a high level of precision can be
  achieved using a software floating-point implementation.  For
  formulas with negative weights, we show that interval floating-point
  arithmetic can reliably detect cases where a desired precision
  cannot be achieved.  Using a program that combines floating-point,
  interval, and rational arithmetic, we achieve the twin goals of
  efficiency and guaranteed precision.
\end{abstract}

\section{Boolean Formulas and Weighted Model Counting}

We consider Boolean formulas over a set of variables $\varset$ in
\emph{negation normal form}, where negation can only be applied to the
variables.  We refer to a variable $x$ or its negation $\obar{x}$ as a
\emph{literal}.  We use the symbol $\lit$ to indicate an arbitrary
literal.  The set of all formulas is defined recursively to consist of
literals, \emph{conjunctions} of the form $\phi_1 \land \phi_2$, and
\emph{disjunctions} of the form $\phi_1 \lor \phi_2$.  The set of
variables occuring in formula $\phi$ is denoted
$\dependencyset(\phi)$.  Typically, a formula is represented as a
directed acyclic graph, allowing a sharing of subformulas.  We
therefore defined the \emph{size} of a formula to be the number of
unique subformulas.

A (total) assignment is a mapping $\assign \colon \varset \rightarrow
\{0, 1\}$.  An assignment $\assign$ is said to be a \emph{model} of
formula $\phi$ if the formula evaluates to $1$ under that assignment.
The set of models of a formula $\phi$ is written $\modelset(\phi)$.
We can also consider an assignment to be a set of literals, where $x \in \assign$
when $\assign(x) = 1$, and $\obar{x} \in \assign$ when
$\assign(x) = 0$, for each variable $x \in \varset$.

\emph{Weighted model counting} is defined in terms of a \emph{weight
  assignment} $w$, associating rational values $w(x)$ and
$w(\obar{x})$ for each variable $x \in \varset$.
The weight of an
assignment $\assign$ is then defined to be the product of its literal weights, and the weight
of a formula is the sum of the weights of its satisfying assignments:
\begin{eqnarray}
  w(\phi) & = & \sum_{\assign \in \modelset(\phi)} \prod_{\lit \in \assign} w(\lit) \label{eqn:model:count}
\end{eqnarray}

Computing the weighted count of an arbitrary formula is thought to be intractable.  However, it becomes
feasible when the formula is the in deterministic decomposable negation-normal form (d-DNNF):
\begin{enumerate}
\item The formula is in negation-normal form.  
\item All conjunctions are \emph{decomposable}.  That is, every subformula of the form $\phi_1 \land \phi_2$
  satisfies $\dependencyset(\phi_1) \cap \dependencyset(\phi_2) = \emptyset$.
\item All disjunctions are \emph{deterministic}.  That is, every subformula of the form $\phi_1 \lor \phi_2$ satisfies
  $\modelset(\phi_1) \cap \modelset(\phi_2) = \emptyset$.
\end{enumerate}
As an important subclass of d-DNNF, a formula is said to be in 
decision decomposable negation-normal form (decision-DNNF) when every occurrence of a disjunction has the form 
$(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ for some decision variable $x$.

There are several ways to compute the weighted count of d-DNNF formula $\phi$, all
based on an \emph{algebraic evaluation} of $\phi$ to compute a value $W(\phi)$:
\begin{enumerate}
\item Each literal $\lit$ is a assigned a rational value $W(\lit)$.
\item Each subformula $\phi_1 \land \phi_2$ is evaluated as $W(\phi_1 \land \phi_2) = W(\phi_1) \cdot W(\phi_2)$.
\item Each subformula $\phi_1 \lor \phi_2$ is evaluated as $W(\phi_1 \lor \phi_2) = W(\phi_1) + W(\phi_2)$.
\end{enumerate}
The number of rational operations in this evaluation is linear in the size of the formula.

There are three ways to use this evaluation to perform weighted model counting for weight assignment $w$:
\begin{enumerate}
\item If the weight assignment for each variable and its complement satisfies $w(x) + w(\obar{x}) = 1$,
  then letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item Formula $\phi$ is said to be \emph{smooth} if every disjunction $\phi_1 \lor \phi_2$ satisfies
  $\dependencyset(\phi_1) = \dependencyset(\phi_2)$.  For a smooth formula, 
letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item If the weight assignment for each variable and its complement satisfies $w(x) + w(\obar{x}) \not = 0$,
  we can apply \emph{rescaling}, first computing $s(x) = w(x) + w(\obar{x})$ for each variable $x$
  and letting $W(x) = w(x)/s(x)$ and $W(\obar{x}) = w(\obar{x})/s(x)$.  
  Following the algebraic evaluation $W(\phi)$, the weighted count is computed as:
  \begin{eqnarray}
w(\phi) &=& \prod_{x\in\varset} s(x) \; \cdot \; W(\phi) \label{eqn:rescale}
  \end{eqnarray}
\end{enumerate}

An arbitrary formula can be smoothed by inserting \emph{smoothing terms} of the form $x \lor \obar{x}$.
That is,
  a disjunction $\phi_1 \lor \phi_2$ having some variable $x \in \dependencyset(\phi_1)$ but
  $x \not\in \dependencyset(\phi_2)$ is rewritten as $\phi_1 \lor [(x \lor \obar{x}) \land \phi_2]$.
  
  These three methods can be combined by rescaling some variables and
  inserting smoothing terms for others.  In particular, for any
  variable $x$ having $w(x) + w(\obar{x}) = 0$, we can insert smoothing terms to enable the weighted count to be computed using rescaling.
This case is easily handled, since
inserting a smoothing term for $x$
  will effectively cause the subformula to evaluate to $0$.

\section{Approximations and Number Representations}

When approximating rational number $x$ with value $\approximate{x}$, we
define the \emph{approximation error} $\aerror(\approxx, x)$ as:
\begin{eqnarray}
\aerror[\approxx, x] & = & \left\{ \begin{array}{ll}
  \frac{|\approxx - x|}{|x|}  & x \not = 0\\[0.8em]
  |\approxx| & x = 0\\
  \end{array} \right. \label{eqn:approx:error}
\end{eqnarray}
That is, we consider relative error when $x$ is nonzero, and absolute error when $x=0$.
This value will equal 0 when $\approxx=x$, and it will be greater for weaker approximations.

We then define the \emph{digit precision} of the approximation as
\begin{eqnarray}
\digitprecision(\approxx, x) & = & \max(0, -\log_{10} \aerror[\approxx, x]) \label{eqn:digitprecision} 
\end{eqnarray}
This value will range from $0$ for a poor approximation, up to $\infty$ when $\approxx=x$.
Intuitively, it indicates the number of digits in a decimal representation of $\approxx$ that are
trustworthy.

The rational arithmetic library MPQ represents a rational number $x$
as a pair $p$, $q$, such that $p$ and $q$ are relatively prime, and $x
= p/q$.  The space for both $p$ and $q$ is allocated dynamically, and
so their size is limited only by the avaiable memory.  We can
therefore use MPQ to compute an exact representation of a weighted
count from a d-DNNF formula, but the space and time required can be very high.

We consider floating-point numbers of the form
\begin{eqnarray}
x & = & (-1)^s \; \times \; f \; \times 2^{e} \label{eqn:floating-point}
\end{eqnarray}
where:
\begin{itemize}
\item Sign bit $s$ equals $0$ for nonnegative numbers and $1$ for negative numbers
\item Fraction $f$ is encoded as a $p$-bit binary number with an implicit binary point on the left.  That is $0 \leq f \leq 1-2^{-p}$.
\item Exponent $e$ is an integer, possibly with some limitation on its range.
\end{itemize}
As examples, consider two different floating-point formats:
\begin{itemize}
\item The IEEE~754 double-precision format uses a slightly different representation, but it maps to the notation of Equation~\ref{eqn:floating-point} with $p=53$ and an exponent range of
  $-1021 \leq e \leq 1024$.  Unfortunately, the small exponent range limits the suitability of
  this representation for weighted model counting.
\item The MPF software floating-point library allows the value of $p$ to be set arbitrarily.
Our default configuration has
$p=128$. On most 64-bit architectures, it represents
  the exponent as a 64-bit signed number.  This provides an ample exponent range.
\end{itemize}

When converting a rational number $x$ into a floating-point encoding, its value must be rounded to a
value $\round(x)$.  In doing so, it introduces a \emph{rounding
error}.  
Letting $\roundepsilon = 2^{-p}$, and assuming that
the exponent range suffices to represent $x$,
we can assume that
$\aerror[\round(x), x] \leq \roundepsilon$.
As examples, the double-precision representation has $\roundepsilon = 2^{-53} \approx 1.11 \times 10^{-16}$, while the 128-bit MPF format
has $\roundepsilon = 2^{-128} \approx 2.94 \times 10^{-39}$.  As a consequence, the upper bound of $\digitprecision(\round(x), x)$ is around $15.95$ and $38.53$, respectively.

Floating-point arithmetic is implemented in such a way that any
operation effectively computes an exact result and then rounds it to
encode the result as a floating-point value.  The maximum error from a sequence of operations
therefore tends to accumulate in multiples of $\roundepsilon$.
This yields error bounds of the form $\aerror[\approxx, x] \leq t\,\roundepsilon$,
which we  refer to as having at most $t$ units of
rounding error.

As a third representation, the MPFI interval arithmetic library
represent a contiguous ranges of numbers $[\xmin, \xmax]$, where
$\xmin$ and $\xmax$ are floating-point numbers.  The endpoints of the
range are represented using the MPFR package, and extension of MPF to
provide more careful rounding.  Arithmetic on intervals is performed
in such a way that the result of any operation is an interval that
spans the range of possible values of that operation applied to any
combination of values spanned by the argument intervals.

For interval $[\xmin, \xmax]$, we define the interval approxmiation error $\aerror([\xmin, \xmax])$ as
\begin{eqnarray}
\aerror([\xmin, \xmax]) & = & \left\{ \begin{array}{ll}
  \frac{|\xmax - \xmin|}{\min(|\xmin|, |\xmax|)}  & 0 \not \in [\xmin, \xmax]\\[0.8em]
  |\xmax - \xmin| &  0 \in [\xmin, \xmax]\\
  \end{array} \right. \label{eqn:interval:error}
\end{eqnarray}
For any values $\approxx, x \in [\xmin, \xmax]$, we can see that
$\aerror[\approxx, x] \leq \aerror(([\xmin, \xmax])$.
We then define the digit precision of the interval as:
\begin{eqnarray}
\digitprecision([\xmin, \xmax]) & = & \max(0, -\log_{10} \aerror([\xmin, \xmax]) \label{eqn:interval:digitprecision} 
\end{eqnarray}
This value will range from $0.0$ for a very large interval to $\infty$ when the interval is tight with $\xmin = \xmax$.

\section{Error Analysis: Nonnegative Weights}
\label{sect:nonneg}

Here we evaluate how rounding errors accumulate via a series of
arithmetic operations when all arguments are nonnegative.
That is, we assume the exact arguments $x$ and $y$ to each operation satisfy $x \geq 0$ and $y \geq 0$.
Rounding never causes a result to be negated, and therefore, we can
consider the approximations $\approxx$ of  $x$ and $\approxy$ of $y$ to satisfy $\approxx \geq 0$ and $\approxy \geq 0$.
In addition, neither the multiplication nor the addition of
nonnegative values can yield a negative result.  We can therefore
assume that all actual and approximate values under consideration will
be nonnegative.

Suppose for nonnegative values $s$ and $t$, we have
$\aerror[\approxx, x] \leq s\, \roundepsilon$ and
$\aerror[\approxy, y] \leq t\, \roundepsilon$, respectively.
Therefore
$0 \leq (1-s\,\roundepsilon)\, x \leq \approxx \leq (1+s\,\roundepsilon)\, x$ and
$0 \leq (1-t\,\roundepsilon)\, y \leq \approxy \leq (1+t\,\roundepsilon)\, y$.


\subsection{Multiplication}

When $\approxx$ and $\approxy$ are multiplied, let us impose the constraint that $s\,t \leq 1/\roundepsilon$.
Their product will satisfy
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2)$, and our additional constraint gives
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t+1)\,\roundepsilon)$.
A similar analysis gives
$(x\cdot y) (1 - (s+t+1)\,\roundepsilon) \leq \approxx \cdot \approxy$, and therefore
$\aerror[\approxx \cdot \approxy, x \cdot y] \leq (s+t+1)\,\roundepsilon$.
Rounding this result can introduce an additional error of at most $\roundepsilon$, and therefore
$\aerror[\round(\approxx \cdot \approxy), x \cdot y] \leq (s+t+2)\,\roundepsilon$.

Thus, for suitable values of $s$, $t$, and $\roundepsilon$,
multiplication, at most, propagates the errors of its arguments as their sum and adds two units of rounding error.

\subsection{Addition}

When $\approxx$ and $\approxy$ are added, their sum will satisfy
$(x + y) (1 - r\,\roundepsilon) \leq \approxx + \approxy \leq (x + y) (1 + r\,\roundepsilon)$, where $r = \frac{sx + ty}{x+y}$.  That is, the resulting error $r$ will bounded by a weighted average
of those of its arguments.  For all values of $x$ and $y$, $r$ cannot exceed the maximum of $s$ and $t$.
Rounding the sum can add at most one unit of rounding error, and so we have
$\aerror[\round(\approxx + \approxy), x + y] \leq (\max(s,t)+1)\,\roundepsilon$.

Thus, addition, at most, propagates the maximum error of its arguments and adds a unit of rounding error.

\subsection{Evaluating a decision-DNNF formula}
\label{sect:error:formula}

Suppose we use floating-point arithmetic to compute the sums and products in an algebraic evaluation of a decision-DNNF formula $\phi$.
We assume that the value $W(\lit)$ for each literal $\lit$ is represented by a floating-point number $\approxW(\lit)$ such that
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$.  We also assume $W(\lit) \geq 0$.
We can then bound the error of the computed value $W(\phi)$ as follows:
\begin{lemma}
  The algebraic evaluation of a decision-DNNF formula $\phi$ having $|\dependencyset(\phi)| = n$, with $n \leq 1/(2\sqrt{\roundepsilon})$ using floating-point arithmetic will yield an approximation $\approxW(\phi)$ satisfying
  $\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.
  \label{lemma:approx:pos}
\end{lemma}

The proof of this lemma proceeds by induction on the structure of $\phi$:
\begin{enumerate}
\item For literal $\lit$ with weight $W(\lit)$, its approximation $\approxW(\lit)$ satisfies
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$, which is within the error bound of $(4n-2)\,\roundepsilon$ for $n=1$.
\item For formula $\phi$ of the form $\phi_1 \land \phi_2$, there must be some $k$, with $1 \leq k < n$, such that $|\dependencyset(\phi_1)| = k$
  and $|\dependencyset(\phi_2)| = n-k$.
\begin{enumerate}
  \item Let us first test whether the conditions for the induction hypothesis hold: for $s = 4k-2$ and $t = 4(n-k)-2$
    we require that $s\,t \leq 1/\roundepsilon$.  We can see that $s\,t \leq 16\,k\,(n-k)$. This quantity will be maximized when $k = n/2$,
    and therefore $s \, t \leq 4\,n^2$.
    Given our limit on $n$ with respect to $\roundepsilon$, we have $s \, t \leq 1/\roundepsilon$.
  \item
We can therefore assume by induction that 
 $\aerror[\approxW(\phi_1), W(\phi_1)] \leq (4 k-2) \,\roundepsilon$
  and $\aerror[\approxW(\phi_2), W(\phi_2)] \leq (4 (n-k)-2) \,\roundepsilon$.  Their product, after rounding
  will satisfy 
$\aerror[\approxW(\phi_1 \land \phi_2), W(\phi_1 \land \phi_2)] \leq [(4 k -2) + (4 (n-k) -2) + 2]\,\roundepsilon = (4n-2) \,\roundepsilon$.
\end{enumerate}
\item For a sum of the form
  $\phi = (x \land \phi_1) \lor (\obar{x} \land \phi_2)$, let us use the notation $\lit_1 = x$ and $\lit_2 = \obar{x}$
  and consider the two subformulas $\lit_i \land \phi_i$ for $i \in \{1,2\}$.
  Since all products are decomposable, we must have $x \not \in \dependencyset(\phi_i)$,
  and therefore $|\dependencyset(\phi_i)| = n-1$.
  By induction, we can therefore assume that
  $\aerror[\approxW(\phi_i), W(\phi_i)] \leq (4(n-1)-2) \,\roundepsilon = (4n-6)\,\roundepsilon$.  Rounding the literal weights will yield
  $\aerror[\approxW(\lit_i), W(\lit_i)] \leq \roundepsilon$.  Let $y_i$ denote the product $W(\lit_i) \cdot W(\phi_i)$ for $i \in \{1,2\}$.
  Its rounded value will satisfy
  $\aerror[\approxy_i, y_i] \leq (4n-3) \,\roundepsilon$.  Summing $\approxy_1$ and $\approxy_2$ and rounding the result will therefore give
  an approximation $\approxW(\phi)$ to $W(\phi) = y_1 + y_2$ with
$\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.  
\end{enumerate}

Observe that this proof relies on the decomposability of the
conjunctions to bound the error induced by multiplication operations.
It relies on the decision structure of the formula only to bound the
depth of the additions.

In the event of rescaling, we must also consider the error introduced
when computing the product $P = \prod_{x\in\varset} s(x)$.  We assume that
each term $s(x)$ is represented by a floating-point value
$\approxs(x)$ such that $\aerror[\approxs(x), s(x)] \leq
\roundepsilon$.  In practice, this requires using rational arithmetic
to represent $w(x)$ and $w(\obar{x})$ and to compute their sum.  The
only error introduced will then be when converting the sum into
a floating-point representation.

We can then bound the error of the product as
\begin{lemma}
  The computation of the product $P = \prod_{x\in\varset} s(x)$ having
$|\varset| = n$, with $n \leq 1/(2\sqrt{\roundepsilon})$ using floating-point arithmetic will yield an approximation $\approxP$ satisfying
  $\aerror[\approxP, P] \leq (3n-2)\,\roundepsilon$.
  \label{lemma:approx:product}
\end{lemma}

The proof of this lemma proceeds much like that for Lemma~\ref{lemma:approx:pos}.  We assume an arbitrary association of the subproducts, and so $P$ can be computed as
$P_1 \cdot P_2$, where $P_1$ is the product of $k$ elements and $P_2$ is the product of $n-k$ elements, with $1 < k < n$.
The smaller coefficient of $3$ arises due to the lack of any addition operations.

Combining these two results, we can state the following result about weighted model counting when all weights are nonnegative:
\begin{theorem}
  \label{thm:approx:pos}
Computing the weighted model count of a
a decision-DNNF formula $\phi$ having $|\dependencyset(\phi)| = n$, using floating-point arithmetic with a $p$-bit fraction, such that $\log_2 n \leq p/2-1$
will yield an approximation $\approxw(\phi)$ to the true weighted count $w(\phi)$, such that
\begin{eqnarray}
\digitprecision(\approxw(\phi), w(\phi)) & \geq & p\,\log_{10} 2 - c - \log_{10} n \label{eqn:precision:wmc}
\end{eqnarray}
where $c = \log_{10} 7$ when rescaling is required and $c = \log_{10} 4$ when no rescaling is required
\end{theorem}

Let us examine the practical implications of this theorem.  In particular, assume we use the MPF library with $p=128$.
Only a single formula from the 2024 weighted model competition had over one million variables, and so we can assume a
bound of $n \leq 10^7$.  The conditions for Equation~\ref{eqn:dnnf:digitprecision} to hold will readily be satisfied, since
$\log_2 n \leq 23.3$ and $p/2-1 = 63$.  We will therefore have 
$\digitprecision(\approxw(\phi), w(\phi)) \geq 38.53 - 0.85 - 7 = 30.68$.  That is, even though the floating-point format provides only around 38 digits of precision for a single rounding,
computing the weighted model count of a decision-DNNF formula over 10 million variables, and using rescaling, is still guaranteed to yield a result with 30 digits of precision.

In scaling to larger formulas, each time
the number of variables is increased by a factor of 10, at most one
digit of precision is lost.
Importantly, this bound holds regardless of the formula size, as well as for any weight assignment, 
as long as all weights are nonnegative.

\subsection{Allowing non-decision disjunctions}

Our previous requirement that each disjunction have a decision
variable had the effect of limiting the depth of the sum operations in
a formula to $n$.  Without this requirement, the depth can become
exponential.  For example, consider a d-DNNF formula encoding a
tautology over $n$ variables, constructed as follows.  For each of the
$2^n$ possible assignment $\assign$, we use conjunction operations to
encode $\phi_{\assign} = \bigwedge_{\lit \in \assign} \lit$.  We then
use a sequence of $2^n-1$ disjunctions to combine these products to
form $\phi$.  The accumulated error from these operations could grow
exponentially with $n$.  Of course, such a formula is completely
impractical for even modest values of $n$, but this construct
demonstrates that, without further restriction on the structure of a
d-DNNF formula, we cannot guarantee a reasonable bound on the error incurred by an algebraic evaluation.

The only
cases we know where non-decision conjunctions occur in d-DNNF formulas is with projected model counting, where
instances of a decision variable or its complement are removed by projection.
In such cases, the bound of Equation~\ref{eqn:precision:wmc} holds,
but we must include all variables, included those projected away, in the
variable count $n$.

\subsection{Experimental Validation}

\begin{figure}
\centering{%
\begin{tikzpicture}
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10}, 
      legend style={at={(0.30,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/upos-mpf+vars}
    \input{data-formatted/epos-mpf+vars}
    \input{data-formatted/prodp-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$+$},
      \scriptsize \textsf{MC2024, Exponential$+$},
      \scriptsize \textsf{Optimized Product}
    }
    \addplot[mark=none] coordinates{(1,38.05) (1e7,31.05)};
    \node[right] at (axis cs: 2, 35.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Nonnegative Weights.  The precision is guaranteed to be greater than the bound.
We set as a target to have digit precisions of at least 30.0.}
\label{fig:pos:mpf}
\end{figure}

To experimentally test the bound of Equation~\ref{eqn:precision:wmc},
we evaluated 200 benchmark formulas from public and private portions
of the 2024 Weighted Model Counting Competition.  We attempted to
convert these into decision-DNNF using version 2 of the the D4
knowledge compiler.  We were able to compile 100 of them with a time
limit on 3600 minutes on a machine with 64~GB of random-access memory.
We define a problem \emph{instance} to be a combination of a formula
plus a weight assignment for all of its literals.

For each of these formulas, we then generated two different collections of instances, each with five weight assignments:
\begin{itemize}
\item \textsf{Uniform$+$}: For each variable $x$, weight $w(x)$ is represented by a 9-digit decimal number selected uniformly in the range
  $[10^{-9},\,1-10^{-9}]$. The weight for $\obar{x}$ is then set to
  $w(\obar{x}) = 1-w(x)$.  Such a weight assignment is typical of those used in recent weighted model counting competitions.
\item \textsf{Exponential$+$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$
  are drawn independently from an exponential distribution in the range
  $[10^{-9},\,10^{9})$.  Each weight is represented by a decimal number with at most 9 digits to the left of the decimal point, and at most 9 digits to the right.
\end{itemize}
For each instance, we evaluated
the weighted count using MPF to get an approximate weight $\approxw$ and
using MPQ to get an exact weight $w$.  We then evaluated the digit precision according to Equation~\ref{eqn:digitprecision}.
In addition, we evaluated formulas of the form $\bigwedge_{1\leq i \leq 10^d} x_i$ for values of $d$ ranging from $0$ to $6$ using a single weight for every variable.
We swept a parameter space of weights of the form $1 + k\cdot 10^{-9}$ for $1 \leq k \leq 100$ and chose the value of $k$ that minimized the digit precision.

Figure~\ref{fig:pos:mpf} shows the results of these evaluations for
the three collections.  For the two collections of weight assignments for each 2024 competition formula,
we show only the minimum precision for each of the five randomly
generated  weight assignments.  Each data point
represents one combination of formula and weight selection method and is placed
along the X-axis according to the number of variables
and on the Y-axis according to the computed digit precision.

As can
be expected, all data points stay above the precision bound.  Indeed,
most exceed the bound by several decimal digits.  Our bound assumes
that rounding either consistently decreases or consistently
increases each computed result by a single unit of rounding error.  In
practice, rounding goes in both directions, and
therefore the computed results stay closer to the true value.

The optimized products, on the other hand, demonstrate that particular
combinations of formula and weight assignment can come very close to
the precision bound.


\section{Error Analysis: Negative Weights}
\label{sect:neg}

The analysis of Section~\ref{sect:nonneg} is no longer valid when
some literals have negative weights.  With a floating-point
representation, summing combinations of negative and nonnegative
values can cause \emph{cancellation}, where arbitary levels of
precision are lost.  Consider, for example, the computation
$s + T - T$, where $s$ and $T$ are nonnegative, but $s \ll T$.  Using
bounded-precision arithmetic, evaluating the sum as $s + (T - T)$ will yield a value
that is very close to $s$.
Evaluating it as $(s + T) - T$, however, can yield $0$ or some other value that bears little relation to $s$.

Cancellation can arise when evaluating decision-DNNF formulas to such a degree that no bounded precision $p$ that grows sublinearly with $n$ will suffice.
In particular, consider the formula
\begin{eqnarray}
\phi_n  & = & z \land \left[\bigwedge_{i = 1}^{n} \obar{x}_i \; \lor \; \bigwedge_{i = 1}^{n} x_i\right] \quad \lor \quad \obar{z} \land \left [\bigwedge_{i = 1}^{n} x_i\right] \label{eqn:max:precision}
\end{eqnarray}
with the following weight assignment:
\begin{center}
\begin{displaymath}
\begin{array}{llll}
  w(z) & = & +1 \\
  w(\obar{z}) & = & -1 \\
  w(x_i) & = & 10^9 & 1 \leq i \leq n \\
  w(\obar{x}_i) & = & 10^{-9} & 1 \leq i \leq n \\
\end{array}
\end{displaymath}
\end{center}
Then computing $w(\phi_n)$  evaluates the sum $(s + T) - T$, where
$s = 10^{-9n}$ and $T = 10^{9n}$.  Avoiding cancellation requires using a floating-point representation with a fraction of at least
$p = n \cdot 18 \cdot \log_2 10 \approx n \cdot 59.79$ bits.
On the other hand, using MPQ, $w(\phi_{10^7})$ can be computed exactly in around 35 seconds, even though the final step requires a total of 1.87~gigabytes to store the arguments
$s+T$, $-T$, and the result $s$.


\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10},
      legend style={at={(0.32,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/uposneg-mpf+vars}
    \input{data-formatted/eposneg-mpf+vars}
    \input{data-formatted/bposneg-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$\pm$},
      \scriptsize \textsf{MC2024, Exponential$\pm$},
      \scriptsize \textsf{MC2024, Limits$\pm$}
    }
    \addplot[mark=none, dashed] coordinates{(1,38.05) (1e7,31.05)};
    \node[right] at (axis cs: 2, 35.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Negative Weights.  There is no guaranteed bound for precision, but many cases remain above the nonnegative weight bound.
The Limits~$\pm$ weight assignment was designed to maximize precision loss.}
\label{fig:posneg:mpf}
\end{figure}

Interestingly, however, using MPF with a fixed precision of $p=128$ does surprisingly well on many formulas, even in the presence of negative weights.
In Figure~\ref{fig:posneg:mpf}, we see a similar plot to that of Figure~\ref{fig:pos:mpf}.  The first two collections of weight assignments are simple generalizations of those used earlier:
\begin{itemize}
\item \textsf{Uniform$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes drawn independently
from a uniform distribution in the range  $[10^{-9},\,1-10^{-9}]$.  Each is negated with probability $0.5$.
\item \textsf{Exponential$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes
  drawn independently from an exponential distribution in the range $[10^{-9},\,10^{9}]$.  Each is negated with probability $0.5$.
\end{itemize}
As can be see for these plots, the results mostly stay above the precision bound of Equation~\ref{eqn:digitprecision},
even though that bound does not apply.  All stay above the target precision of $30.0$.

On deeper inspection, we can see that setting up the conditions for a
cancellation of the form $(s + T) - T'$, where $T \approx T'$, require
1) a large dynamic range among the computed values to give widely different values $s$ and $T$, and 2) sufficient
homogeneity in the computed values that we can get two values $T$ and
$T'$ such that $T \approx T'$.  A uniform distribution has neither of
these properties.  An exponential distribution has a large dynamic
range, but the computed values tend to be very heterogenous.

To create a weight assignment that has more chance of cancellation, we devised the following asignment:
\begin{itemize}
\item\textsf{Limits$\pm$}:  Each weight $w(x)$ and $w(\obar{x})$ has a magnitude of either $10^{-9}$ or $10^{+9}$, and they are each set negative with probability $0.5$.
  However, we exclude assignments with $w(x) + w(\obar{x}) = 0$.
\end{itemize}
The idea here is to give large dynamic ranges plus a high degree of
homogeneity.  The plots for this assignment in
Figure~\ref{fig:posneg:mpf} demonstrate that it achieves the desired
effect.  Although over one-half of the assignments yield computed
values that remain above the precision bound, many have digit
precision below the target of $30.0$.  We can also see how our choice
of weights leads to two bands of low precision: those where $s$ and
$T$ differ by a factor of around $10^{18}$, and those where they
differ by a factor of around $10^{36}$.

\section{Reliable and Efficient Weight Computation}

Our results with the formula of Equation~\ref{eqn:max:precision} and
with the \textsf{Limits$\pm$} weight assignment show that simply
computing weights with fixed-precision floating-point arithmetic can
yield inaccurate results.  Even worse, such an approach cannot even
detect whether the computed result provides any level of accuracy.

One way to achieve accuracy is to perform all computations with
rational arithmetic.  Unfortunately, this can be very time consuming,
and it provides a higher level of accuracy than is required for most
applications.  Furthermore, the time and memory required can exceed
the resources available.

A second strategy is to use floating point arithmetic for problems
where all weights are nonnegative and rational arithmetic otherwise.
This would provide guaranteed accuracy, but it can still be resource
intensive.  On the other hand, the example of
Equation~\ref{eqn:max:precision} shows us that some problems can only
be solved with rational arithmetic.




\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              xmode=normal,xmin=0, xmax=50,
                              xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              xticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              ymode=normal, ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Estimated Precision}, ylabel={Actual Precision}
            ]

    \draw[ fill={blue!30}, opacity=0.2] (axis cs: 0,30) rectangle (axis cs: 30,50);
    \node at (axis cs:15,40) {Overly pessimistic};
    

    \input{data-formatted/uposneg-mpfi-est+act}
    \input{data-formatted/eposneg-mpfi-est+act}
    \input{data-formatted/bposneg-mpfi-est+act}
    \legend{
      \scriptsize \textsf{MC2024, Uniform $\pm$},
      \scriptsize \textsf{MC2024, Exponential $\pm$},
      \scriptsize \textsf{MC2024, Limits $\pm$}
    }
    \addplot[mark=none] coordinates{(0,0) (50,50)};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Predictive Accuracy of MPFI Interval Arithmetic.  MPFI never has a higher estimate than the actual, but it can
incorrectly predict a precision less than the target of 30.0.}
\label{fig:mpfi}
\end{figure}

\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.40,0.98)}},
      legend cell align={left},
                              xmode=log,xmin=0.001, xmax=10000.0,
                              xtick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              xticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              ymode=log, ymin=0.001, ymax=10000.0,
                              ytick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              yticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              xlabel={MPQ Runtime (secs.)}, ylabel={Hybrid Runtime (secs.)}
            ]

    \input{data-formatted/combo-mpf+mpq}
    \input{data-formatted/combo-mpfi+mpq}
    \input{data-formatted/combo-mpfi2+mpq}
    \input{data-formatted/combo-mpq+mpq}
    \legend{
      \scriptsize \textsf{MC2024, Hybrid-MPF},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-128},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-256},
      \scriptsize \textsf{MC2024, Hybrid-MPQ}
    }
    \addplot[mark=none] coordinates{(0.001,0.001) (10000.0,10000.0)};
    \node[left] at (axis cs: 10000, 1150) {$1.0\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.01, 0.001) (10000.0, 1000.0)};
    \node[left] at (axis cs: 10000, 115) {$0.1\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.1, 0.001)  (10000.0, 100.0)};
    \node[left] at (axis cs: 10000, 10) {$0.01\times$};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Runtime for hybrid method vs.~for MPQ.  When the value can be computed with MPF or MPFI, the runtime is signficantly better than using MPQ.  When it must resort to MPQ, it incurs some overhead.} 
\label{fig:runtime}
\end{figure}

\subsection{Comparing Implementation Strategies}

\begin{table}
  \caption{Performance Comparision of Different Implementation Strategies}
  \label{tab:compare}
  \centering{
  \begin{tabular}{llrrrrrr}
    \toprule
    \multicolumn{1}{c}{Strategy} & & \multicolumn{1}{c}{MPF} & \multicolumn{1}{c}{MPFI-128} & \multicolumn{1}{c}{MPFI-256} & \multicolumn{1}{c}{MPQ} & \multicolumn{1}{c}{Combined} & \multicolumn{1}{c}{Avg (s)}\\
    \midrule
    \input{method_table}
    \\[-1em]
   \bottomrule
  \end{tabular}
  } % Centering
\end{table}

\newpage
\bibliography{references}

\end{document}

Comparisons:
\begin{center}
  \begin{tabular}{llr}
    \toprule
    $M_1$ & $M_2$ & Ratio \\
    \midrule
    MPQ & MPF & \avgMpqMpf \\
    MPF & Hybrid & \avgMpfHybrid \\    
    MPQ & Hybrid & \avgMpqHybrid \\
    \bottomrule
  \end{tabular}
\end{center}


%%
%% End of file
