\documentclass[letterpaper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}

%\newtheorem{proposition}{Proposition}

\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lit}{\ell}

\newcommand{\progname}[1]{\textsc{#1}}
\newcommand{\dfour}{\progname{D4}}
\newcommand{\Dfour}{\progname{D4}}

\newcommand{\approximate}[1]{\hat{#1}}
\newcommand{\approxx}{\approximate{x}}
\newcommand{\approxy}{\approximate{y}}
\newcommand{\approxw}{\approximate{w}}
\newcommand{\approxs}{\approximate{s}}
\newcommand{\round}{\mathit{Round}}
\newcommand{\aerror}{\delta}
\newcommand{\digitprecision}{\Delta}
\newcommand{\minvalue}{\omega}
\newcommand{\maxValue}{\Omega}
\newcommand{\roundepsilon}{\varepsilon}

\newcommand{\varset}{X}
\newcommand{\dependencyset}{{\cal V}}
\newcommand{\assign}{\alpha}
\newcommand{\modelset}{{\cal M}}
\newcommand{\entails}{\models}


\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Arithmetic Considerations in Weighted Model Counting}

\titlerunning{Arithmetic Considerations for Weighted Model Counting}

\author{Randal E. Bryant}{Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA}{Randy.Bryant@cs.cmu.edu}{https://orcid.org/0000-0001-5024-6613}{}

\authorrunning{R. E. Bryant} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Randal E. Bryant} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
\end{abstract}

\section{Preliminaries}

When approximating real number $x$ with value $\approximate{x}$, we define the \emph{approximation error} $\aerror(\approxx, x)$ as:
\begin{eqnarray}
\aerror[\approxx, x] & = & \left\{ \begin{array}{ll}
  \frac{|\approxx - x|}{|x|} \label{eqn:approx:error} & x \not = 0\\[0.8em]
  |\approxx| & x = 0\\
  \end{array} \right.
\end{eqnarray}
That is, we consider relative error when $x$ is nonzero, and absolute error when $x=0$.  This value will equal 0 when $\approxx=x$, and it will be greater for weaker approximations.

We then define the \emph{digit precision} of the approximation as
\begin{eqnarray}
\digitprecision(\approxx, x) & = & \max(0, -\log_{10} \aerror[\approxx, x]) \label{eqn:digitprecision} 
\end{eqnarray}
This value will range from $0$ for a poor approximation, up to $\infty$ when $\approxx=x$.
When both $\approxx$ and $x$ are written as (possibly infinite-length)
decimal numbers, $\digitprecision(\approxx,x)$ indicates the number of
leading digits they have in common.  

We consider floating-point numbers of the form
\begin{eqnarray}
x & = & (-1)^s \; \times \; f \; \times 2^{e} \label{eqn:floating-point}
\end{eqnarray}
where:
\begin{itemize}
\item Sign bit $s$ equals $0$ for nonnegative numbers and $1$ for negative numbers
\item Fraction $f$ is a $p$-bit binary number with an implicit binary point on the left.  That is $0 \leq f \leq 1-2^{-p}$.
\item Exponent $e$ is an integer, possibly with some limitation on its range.
\end{itemize}
As examples, consider these two different floating-point formats:
\begin{itemize}
\item The IEEE~754 double-precision format uses a slightly different representation, but it maps to the notation of Equation~\ref{eqn:floating-point} with $p=53$ and an exponent range of
  $-1021 \leq e \leq 1024$.  This ignores its support for denormalized numbers, infinities, and NaN (not-a-number).  Unfortunately, the small exponent range limits the suitability of
  this representation for weighted model counting.
\item As will be shown, the MPF software floating-point package can be very effective for weighted model counting.
We configured it to have $p=128$. On most 64-bit architectures, it represents
  the exponent as a 64-bit signed number.  This provides an ample exponent range.
\end{itemize}

When representing a real-number $x$, its value must be rounded to a
value $\round(x)$.  In doing so, it introduces a \emph{rounding
error}.  We use $\roundepsilon$ to denote the maximum value of
$\aerror[\round(x), x]$.  Depending how the rounding
is performed, $\roundepsilon$ is either $2^{-p}$ or $2^{-p-1}$.  Here, we assume that the exponent range suffices to represent $x$.
As examples, the double-precision representation has $\roundepsilon = 2^{-54} \approx 5.55 \times 10^{-17}$, while the 128-bit MPF format
has $\roundepsilon = 2^{-128} \approx 2.94 \times 10^{-39}$.  As a consequence, the lower bound of $\digitprecision(\round(x), x)$ is around $15.95$ and $38.53$, respectively.

\section{Error analysis: Nonnegative weights}

Here we evaluate how rounding errors accumulate via a series of arithmetic operations.  We assume that each operation is performed in a way that effectively computes the exact
arithmetic value, but this value is then rounded to produce the final result.  We assume that $\roundepsilon$ is sufficiently small that $\roundepsilon^2 \approx 0$.

Suppose during some computation, an accumulation of rounding errors has yielded an approximation $\approxx$ to a true value $x > 0$.
Let $\aerror[\approxx, x] \leq s\, \roundepsilon$, i.e., $(1-s\,\roundepsilon)\, x \leq \approxx \leq (1+s\,\roundepsilon)\, x$.  Similarly, let true value $y > 0$ be
approximated by a value $\approxy$ such that 
$\aerror[\approxy, y] \leq t\, \roundepsilon$, i.e., $(1-t\,\roundepsilon)\, y \leq \approxy \leq (1+t\,\roundepsilon)\, y$.
Furthermore, we assume that zero is never approximated, and so $\approxx = 0$ when $x = 0$, and similarly for $y$.

In the following, we consider only the case where the true values of
all arguments are nonnegative.  That is $x \geq 0$ and $y \geq 0$.
Rounding never causes a result to be negated, and therefore, we can
consider the approximations to satisfy $\approxx \geq 0$ and $\approxy
\geq 0$.  In addition, neither the multiplication nor the addition of
nonnegative values can yield a negative result.  We can therefore
assume that all actual and approximate values under consideration will
be nonnegative.

\subsection{Multiplication}

When $\approxx$ and $\approxy$ are multiplied, their product will satisfy
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2)$.
We assume that $s$ and $t$ are small enough that $s\, t\,\roundepsilon^2 \approx 0$, and therefore
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t)\,\roundepsilon)$.  A similar analysis gives
$(x\cdot y) (1 - (s+t)\,\roundepsilon) \leq \approxx \cdot \approxy$, and therefore
$\aerror[\approxx \cdot \approxy, x \cdot y] \leq (s+t)\,\roundepsilon$.
Rounding this result introduces an addition error of $\roundepsilon$, and therefore
$\aerror[\round(\approxx \cdot \approxy), x \cdot y] \leq (s+t+1)\,\roundepsilon$.

Thus, mulitiplication, at most, propagates the errors of its arguments as their sum plus an additional unit of rounding error.

\subsection{Addition}

When $\approxx$ and $\approxy$ are added, their sum will satisfy
$(x + y) (1 - r\,\roundepsilon) \leq \approxx + \approxy \leq (x + y) (1 + r\,\roundepsilon)$, where $r = \frac{sx + ty}{x+y}$.  That is, the resulting error $r$ will bounded by a weighted average
of those of its arguments.  For all values of $x$ and $y$, $r$ cannot exceed the maximum of $s$ and $t$.
Rounding the sum can add one more unit of rounding error, and so we have
$\aerror[\round(\approxx + \approxy), x + y] \leq (r+1)\,\roundepsilon$.

Thus, addition, at most,  propagates the maximum error of its arguments plus an additional unit of rounding error.

\subsection{Evaluating a smooth, decision-DNNF formula}

Consider a Boolean formula over a set of Boolean variables $\varset$
with operations conjunction $\land$, disjunction $\lor$, and negation
$\neg$, satisfying the following conditions:
\begin{itemize}
\item The formula is in negation normal form, i.e., negation is only
  applied to the variables.  We refer to positive and negated
  variables as \emph{literals}.  The negation of variable $x$ is
  written $\obar{x}$. We use the symbol $\lit$ to denote a literal.
\item Conjunctions are \emph{decomposable}.  That is, for formula $\phi$, let $\dependencyset(\phi)$ denote the set of variables that occur in it.  Then for a conjunction of the form
  $\phi_1 \land \phi_2$, we require $\dependencyset(\phi_1) \cap \dependencyset(\phi_2) = \emptyset$.
\item Every disjunction has a \emph{decision variable}.  That is, disjunctions are of the form $(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ for some decision variable $x$. 
\item The formula is \emph{smooth}: Every disjunction of the form $(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ 
also satisfies the property that $\dependencyset(\phi_1) = \dependencyset(\phi_2)$.
\end{itemize}

A (total) assignment is a mapping $\assign \colon \varset \rightarrow \{0, 1\}$.  An assignment $\assign$ is said to be a \emph{model} of formula
$\phi$ if the formula evaluates to $1$ under that assignment.  The
set of models of a formula $\phi$ is written $\modelset(\phi)$.  We
can also consider an assignment to be a set of literals, where
$x \in \assign$ when $\assign(x) = 1$, and  $\obar{x} \in \assign$ when $\assign(x) = 0$, for each variable $x$.

With \emph{weighted model counting}, each literal $\lit$ is assigned a
real-valued weight $w(\lit)$.  We then compute the weight of an
assignment $\assign$ to be the product of its literals. and the weight
of a formula to be the sum of the weights of its satisfying assignments:
\begin{eqnarray}
  w(\phi) & = & \sum_{\assign \in \modelset(\phi)} \prod_{\lit \in \assign} w(\lit) \label{eqn:model:count}
\end{eqnarray}

It can be shown that the weighted model count for a smooth, decision-DNNF formula can be computed as follows:
\begin{enumerate}
\item The weight for literal $\lit$ is computed as $w(\lit)$.
\item The weight for  conjunction $\phi$ of the form $\phi_1 \land \phi_2$ is computed as $w(\phi) = w(\phi_1) \cdot w(\phi_2)$.
\item The weight for disjunction $\phi$ of the form 
  $(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ is computed as
  $w(\phi) = [w(x) \cdot w(\phi_1)] + [w(\obar{x}) \cdot w(\phi_2)]$.
\end{enumerate}

Suppose we use floating-point arithmetic to compute the sums and products in evaluating a smooth, decision-DNNF formula $\phi$, yielding
an approximation $\approxw(\phi)$ to the true value $w(\phi)$.  When all literal weights are nonnegative, we can bound the error of this approximation as follows:
\begin{theorem}
  For smooth, decision-DNNF formula $\phi$ having $|\dependencyset(\phi)| = n$, such that every literal $\lit$ has weight $w(\lit) \geq 0$,
the approximation $\approxw(\phi)$ satisfies
  $\aerror[\approxw(\phi), w(\phi)] \leq (3n-1)\,\roundepsilon$.
  \label{thm:approx:pos}
\end{theorem}

The proof of this theorem proceeds by induction on the structure of $\phi$:
\begin{enumerate}
\item For Literal with weight $w(\lit)$, its approximation $\approxw(\lit)$ will be due to a single rounding error:
$\aerror[\approxw(\lit), w(\lit)] \leq \roundepsilon$, which is within the error bound of $(3n-1)\,\roundepsilon$ for $n=1$.
\item For formula $\phi$ of the form $\phi_1 \land \phi_2$, there must be some $k$, with $1 \leq k < n$, such that $|\dependencyset(\phi_1)| = k$
  and $|\dependencyset(\phi_2)| = n-k$.  We can assume by induction that 
$\aerror[\approxw(\phi_1), w(\phi_1)] \leq (3 k-1) \,\roundepsilon$
  and $\aerror[\approxw(\phi_2), w(\phi_2)] \leq (3 (n-k)-1) \,\roundepsilon$.  Their product
  will satisfy 
  $\aerror[\approxw(\phi_2) \cdot \approxw(\phi_2), w(\phi_2) \cdot w(\phi_2)] \leq (3 n -2) \,\roundepsilon$.
  Rounding this result can add one additional unit of rounding error, yielding
$\aerror[\approxw(\phi_1 \land \phi_2), w(\phi_1 \land \phi_2)] \leq (3 n -1) \,\roundepsilon$.
\item For a sum of the form
  $\phi = (x \land \phi_1) \lor (\obar{x} \land \phi_2)$, let us use the notation $\lit_1 = x$ and $\lit_2 = \obar{x}$
  and consider the two subformulas $\lit_i \land \phi_i$ for $i \in \{1,2\}$.
  Since all products are decomposable, we must have $x \not \in \dependencyset(\phi_i)$,
  and therefore $|\dependencyset(\phi_i)| = n-1$.
  By induction, we can therefore assume that
  $\aerror[\approxw(\phi_i), w(\phi_i)] \leq (3(n-1)-1) \,\roundepsilon = (3n-4)\,\roundepsilon$.  Rounding the literal weights will yield
  $\aerror[\approxw(\lit_i), w(\lit_i)] \leq \roundepsilon$.  Let $y_i$ denote the product $w(\lit_i) \cdot w(\phi_i)$ for $i \in \{1,2\}$.
  The computed products will satisfy
  $\aerror[\approxw(\lit_i) \cdot \approxw(\phi_i), y_i] \leq (3n-3) \,\roundepsilon$.
  Rounding each product will give values $\approxy_i$ that approximate $y_i$, such that
  $\aerror[\approxy_i, y_i] \leq (3n-2) \,\roundepsilon$.  Summing $\approxy_1$ and $\approxy_2$ and rounding the result will therefore give
  an approximation $\approxw(\phi)$ to $w(\phi) = y_1 + y_2$ with
$\aerror[\approxw(\phi), w(\phi)] \leq (3n-1)\,\roundepsilon$.  
\end{enumerate}

This theorem shows that, in many cases, floating-point arithmetic will suffice for weighted model counting.  In particular, the digit precision
resulting when computing an approximation for the weighted model count for $n$-variable formula $\phi$ will satisfy:
\begin{eqnarray}
\digitprecision(\approxw(\phi), w(\phi)) & \geq & K_{\roundepsilon} - \log_{10}(n) \label{eqn:precision:wmc}
\end{eqnarray}
where $K_{\roundepsilon} = -\log_{10}(3\,\roundepsilon)$.  Using a
double-precision representation (and assuming the range is
sufficient) gives $K_{\roundepsilon} > 15.77$.  MPF with a
128-bit fraction gives $K_{\roundepsilon} > 38.05$.  Each time
the number of variables is increased by a factor of 10, at most one
digit of precision is lost.  For example, only one formula in the 2024
weighted model counting competition had over one million variables.
Even setting an upper bound of $n = 10^7$, we see that the computation using
MPF will yield at least 31 significant digits.  That should suffice for most applications of weighted model counting.
Importantly, this bound holds independent of the formula size, as well as of the values of the weights, as long as they are nonnegative.

\subsection{Eliminating smoothing}

As long as no variable $x$ has $w(x) + w(\obar{x}) = 0$, we can 
compute the weighted model count of non-smoothed formula $\phi$ by a method we call
\emph{rescaling}.  That is, for each variable $x$, define $s(x) = w(x)
+ w(\obar{x})$.  We then rescale the weights for $x$ and its
complement as $w'(x) = w(x)/s(x)$ and $w'(\obar{x}) = w(\obar{x})/s(x)$.  The formula is evaluated as before, but with
scaled literal weights, to compute a scaled weight for the formula $w'(\phi)$.  We then
compute $w(\phi) = \prod_{x\in\varset} s(x) \; \cdot \; w'(\phi)$.

This approach has the advantage that it avoids the need to add
smoothing terms to the formula, a time-consuming process that can
increase the formula size by up to a factor of $n$.

Let us consider the possible error introduced by rescaling.  To maximize precision, we
use a rational arithmetic package to exactly compute $s(x)$, $w'(x)$, and $w'(\obar{x})$.  We then round these to produce
approximations $\approxs(x)$, $\approxw'(x)$, and $\approxw'(\obar{x})$ with at most one unit of rounding error.
Computing the product $y = \prod_{x\in\varset} s(x)$ will yield an approximation $\approxy$ such that
$\aerror[\approxy, y] \leq (2n-1) \,\roundepsilon$, due to the rounding errors of the arguments, plus the $n-1$ rounding errors of the successive products.
The formula evaluation will yield an approximation $\approxw'(\phi)$, such that
$\aerror[\approxw'(\phi), w'(\phi)] \leq (3n-1) \,\roundepsilon$.  The final product will combine these and add one more unit of rounding error to give
$\aerror[\approxw(\phi), w(\phi)] \leq (5n-1) \,\roundepsilon$.

Thus, computing the count by rescaling can have greater error than by smoothing, but the impact will be very small.
Compared to smoothing, rescaling will reduce the bound of Equation~\ref{eqn:precision:wmc} by $\log_{10} 5/3 \approx 0.22$ digits of precision.

\subsection{Allowing non-decision disjunctions}

Weighted model counting can be computed as we have shown, as long as
all sum operations are \emph{deterministic}.  That is, the formula can
contain disjunctions of the form $\phi_1 \lor \phi_2$, as long as
$\modelset(\phi_1) \cap \modelset(\phi_2) = \emptyset$.  This class of formulas is referred to as ``d-DNNF.''

Our previous requirement that each disjunction have a decision variable
had the effect of limiting the depth of the sum operations in a
formula to $n$.  Without this requirement, the depth can become exponential.
For example, consider a d-DNNF formula encoding a tautology over $n$
variables, constructed as follows.  For each possible assignment
$\assign$, we use conjunction operations to encode 
$\phi_{\assign} = \bigwedge_{\lit \in \assign} \lit$.  We then use a sequence of $2^n-1$ disjunctions to combine these products to form $\phi$.
The accumulated error from these operations could grow exponentially with $n$.
Of course, such a formula is completely impractical for $n \geq 30$, and there are better ways to encode tautology.

The only
cases we know where non-decision conjunctions occur in d-DNNF formulas is with projected model counting, where
instances of a decision variable or its complement are removed by projection.
In such cases, the bound of Equation~\ref{eqn:precision:wmc} holds,
but we must include all variables, included those projected away, in the
variable count $n$.

\subsection{Experimental Validation}

\begin{figure}
\centering{%
\begin{tikzpicture}
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10}, 
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e7,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/upos-mpf+vars}
    \input{data-formatted/epos-mpf+vars}
    \input{data-formatted/prodp-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$+$},
      \scriptsize \textsf{MC2024, Exponential$+$},
      \scriptsize \textsf{Optimized Product}
    }
    \addplot[mark=none] coordinates{(1,38.05) (1e7,31.05)};
    \node[right] at (axis cs: 2, 35.5) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Nonnegative Weights.  The precision is guaranteed to be greater than the bound.}
\label{fig:pos:mpf}
\end{figure}

To evaluate the implications of Equation~\ref{eqn:precision:wmc}, we
evaluated the benchmark formulas from the 2024 Weighted Model Counting
Competition.  We attempted to convert these into decision-DNNF using
version 2 of the the D4 knowledge compiler.  We were able to compile
100 of them with a time limit on 3600 minutes on a machine with 64~GB
of random-access memory.  For each of these formulas, we then generated two different collections, each with five sets of literal weights:
\begin{itemize}
\item \textsf{Uniform$+$}: For each variable $x$, weight $w(x)$
  is drawn from a uniform distribution in the range
  $[10^{-9},\,1-10^{-9}]$. The weight for $\obar{x}$ is then set to
  $w(\obar{x}) = 1-w(x)$.  Such a weight assignment is typical of those used in recent weighted model counting competitions
\item \textsf{Exponential$+$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$
  are drawn from a exponential distribution in the range
  $[10^{-9},\,10^{9}]$.
\end{itemize}
For each combination of formula and weight assignment, we evaluated
the weights using MPF to get an approximate weight $\approxw$ and
using MPQ to get an exact weight $w$.  We then evaluated the digit precision according to Equation~\ref{eqn:digitprecision}.
In addition, we evaluated formulas of the form $\bigwedge_{1\leq i \leq 10^p} x_i$ for values of $p$ ranging from $0$ to $6$ using a single weight for every variable.
We swept a parameter space of weights of the form $1 + k\cdot 10^{-9}$ for $1 \leq k \leq 100$ and chose the value of $k$ that minimized the digit precision.

Figure~\ref{fig:pos:mpf} shows the results of these evaluations for
the three collections.  For the two collections of weight assignments for eadh 2024 competition formula,
we selected the minimum precision from each of the five randomly
generated sets of weights.  Each data point
represents one combination of formula and weight selection method and is placed
along the X-axis according to the number of variables
and on the Y-axis according to the computed digit precision.

As can
be expected, all data points stay above the precision bound.  Indeed,
most exceed the bound by several decimal digits.  Our bound assumes
that the rounding either consistently decreases or consistently
increases each computed result by a single unit of rounding error.  In
practice, we expect the rounding to go in both directions and
therefore tend to yield results closer to the true value.

The optimized products, on the other hand, demonstrate that particular
combinations of formula and weight assignment can come very close to
the precision bound.


\section{Negative weights}




\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e7,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/uposneg-mpf+vars}
    \input{data-formatted/eposneg-mpf+vars}
    \input{data-formatted/bposneg-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform $\pm$},
      \scriptsize \textsf{MC2024, Exponential $\pm$},
      \scriptsize \textsf{MC2024, Limits $\pm$}
    }
    \addplot[mark=none, dashed] coordinates{(1,38.05) (1e7,31.05)};
    \node[right] at (axis cs: 2, 35.5) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Negative Weights.  There is no guaranteed bound for precision, but many cases remain above the bound for nonnegative weights.}
\label{fig:posneg:mpf}
\end{figure}

\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              xmode=normal,xmin=0, xmax=50,
                              xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              xticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              ymode=normal, ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Estimated Precision}, ylabel={Actual Precision}
            ]

    \draw[ fill={blue!30}, opacity=0.2] (axis cs: 0,30) rectangle (axis cs: 30,50);
    \node at (axis cs:15,40) {Overly pessimistic};
    

    \input{data-formatted/uposneg-mpfi-est+act}
    \input{data-formatted/eposneg-mpfi-est+act}
    \input{data-formatted/bposneg-mpfi-est+act}
    \legend{
      \scriptsize \textsf{MC2024, Uniform $\pm$},
      \scriptsize \textsf{MC2024, Exponential $\pm$},
      \scriptsize \textsf{MC2024, Limits $\pm$}
    }
    \addplot[mark=none] coordinates{(0,0) (50,50)};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Predictive Accuracy of MPFI.  MPFI never has a higher estimate than the actual, but it can
incorrectly predict a precision less than the target of 30.0.}
\label{fig:mpfi}
\end{figure}

\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              xmode=log,xmin=0.01, xmax=3000.0,
                              xtick={1e-2, 1e-1, 1, 10, 1e2, 1e3},
                              xticklabels={$0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$},
                              ymode=log, ymin=0.01, ymax=3000.0,
                              ytick={1e-2, 1e-1, 1, 10, 1e2, 1e3},
                              xticklabels={$0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$},
                              xlabel={MPQ Runtime (secs.)}, ylabel={Hybrid Runtime (secs.)}
            ]

    \input{data-formatted/combo-mpf+mpq}
    \input{data-formatted/combo-mpfi+mpq}
    \input{data-formatted/combo-mpq+mpq}
    \legend{
      \scriptsize \textsf{MC2024, Hybrid-MPF},
      \scriptsize \textsf{MC2024, Hybrid-MPFI},
      \scriptsize \textsf{MC2024, Hybrid-MPQ}
    }
    \addplot[mark=none] coordinates{(0.01,0.01) (10000.0,10000.0)};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Runtime for hybrid method vs.~for MPQ.  When the value can be computed with MPF or MPFI, the runtime is slightly better than just using MPQ.  When it must resort to MPQ, it incurs some overhead.} 
\label{fig:runtime}
\end{figure}




\newpage
\bibliography{references}

\end{document}

%%
%% End of file
