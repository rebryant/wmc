\documentclass[letterpaper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xnewcommand}

%\newtheorem{proposition}{Proposition}

\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lit}{\ell}

\newcommand{\progname}[1]{\textsc{#1}}
\newcommand{\dfour}{\progname{D4}}
\newcommand{\Dfour}{\progname{D4}}

\newcommand{\approximate}[1]{\hat{#1}}
\newcommand{\approxP}{\approximate{P}}
\newcommand{\approxx}{\approximate{x}}
\newcommand{\approxy}{\approximate{y}}
\newcommand{\approxw}{\approximate{w}}
\newcommand{\approxW}{\approximate{W}}
\newcommand{\approxs}{\approximate{s}}
\newcommand{\round}{\mathit{Round}}
\newcommand{\aerror}{\delta}
\newcommand{\digitprecision}{\Delta}
\newcommand{\minvalue}{\omega}
\newcommand{\maxValue}{\Omega}
\newcommand{\roundepsilon}{\varepsilon}
\newcommand{\xmin}{x_{\textrm{min}}}
\newcommand{\xmax}{x_{\textrm{max}}}

\newcommand{\varset}{X}
\newcommand{\dependencyset}{{\cal V}}
\newcommand{\assign}{\alpha}
\newcommand{\modelset}{{\cal M}}
\newcommand{\entails}{\models}


\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Numeric Considerations in Weighted Model Counting}

\titlerunning{Numeric Considerations for Weighted Model Counting}

\author{Randal E. Bryant}{Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA}{Randy.Bryant@cs.cmu.edu}{https://orcid.org/0000-0001-5024-6613}{}

\authorrunning{R. E. Bryant} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Randal E. Bryant} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\ccsdesc[500]{Theory of computation~Automated reasoning}
\keywords{Weighted model counting, floating-point arithmetic, interval arithmetic}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 Weighted model counting computes the sum of the rational-valued weights
  associated with the satisfying assignments for a Boolean formula,
  where the weight of an assignment is given by the product of the
  weights associated with the positive and negated variables
  comprising the assignment.  Weighted model counting finds
  applications in a wide variety of domains including machine learning
  and quantitative risk assignment.

  Most weighted model counting programs operate by (explicitly or
  implicitly) converting the input formula into a form that enables
  \emph{algebraic} evaluation, using multiplication for conjunctions
  and addition for disjunctions.  Performing this evaluation using
  floating-point arithmetic can yield inaccurate results, and it
  cannot quantify the level of precision achieved.  Computing with
  rational arithmetic gives exact results, but it is costly in both
  time and space.

  We prove that, when all weights are nonnegative, the precision loss of
  algebraic evaluation using floating point increases only
  logarithmically in the number of variables.  We then show experimentally
  that software-based floating-point arithmetic can provide
  a high and guaranteed level of accuracy for this case.  For problems
  with negative weights, we show that a combination of interval
  floating-point arithmetic and rational arithmetic can achieve the
  twin goals of efficiency and guaranteed precision.
\end{abstract}

\section{Introduction}

Model counting extends traditional Boolean satisfiability (SAT) solving by
asking not just whether a formula can be satisfied, but to compute the
number of satisfying assignments~\cite{gomes:hs:2009}.  Model counting is a challenging
problem---more challenging than the already NP-hard Boolean
satisfiability~\cite{valiant:siam:1979}.

Weighted model counting extends standard model counting by having
rational-valued weights associated with the assignments, and then
computing the sum of the weights of the satisfying assignments.  The
most common variant has weights $w(x)$ and $w(\obar{x})$
associated with each variable $x$ and its negation $\obar{x}$.  The
weight of an assignment is then the product of the weights associated
with the positive and negated variables comprising the assignment.
Standard model counting can be seen as a special case of weighted model
counting with all variables and their negations having unit weights.

Weighted model counting has applications in a variety of domains,
including probabilistic inference~\cite{chavira:ai:2008}, Bayesian
inference~\cite{sang:aaai:2005}, and probabilistic
planning~\cite{domshlak:jair:2007}.  In addition, many of the
applications for binary decision diagrams (BDDs) for
combinatorics~\cite{knuth:bdd:2011} and for quantitative risk
assessment~\cite{xing:wiley:2015} are, in fact, applications of
weighted model counting based on BDD representations of Boolean
formulas.

Despite the intractability, a variety of weighted model counting
programs have been developed that work well in practice.  They
generally fall into two categories~\cite{shaw:kr:2024}. \emph{Top-down} programs
recursively branch on the variables of a formula, performing unit
propagation and conflict analysis similar to a CDCL SAT solver.  Most
of these programs operate as \emph{knowledge compilers}, converting
the input Boolean formula into a restricted form that enables efficient
weighted and unweighted counting~\cite{darwiche:aaai:2002,darwiche:ecai:2004,lagniez:ijcai:2017,muise:cai:2012,oztok:cp:2014,sharma:ijcai:2019}.
Others are based on \emph{bottom-up} approaches, especially using
multi-terminal BDDs~\cite{dudek:aaai:2020,dudek:sat:2021}.  In both
cases, the strategy is to convert the formula into a form for which
weighted model counting becomes tractable.

Weighted model counting can be computationally intensive.  In
experimental results described in this paper, some evaluations 
require over one billion multiplication and addition operations.
Furthermore, the computed values are often either too small or too
large in magnitude to encode with standard floating-point
representations.  We also want the computed results to provide a
guaranteed level of precision.  Unfortunately, the rounding errors
introduced by standard floating-point computations can lead to results
that bear little relation to the actual values.  In general, even when the results are accurate,
floating-point evaluation cannot quantify the level of precision achieved.

One way to guarantee high precision is to perform all of the
computations using a rational-arithmetic software package~\cite{knuth:rational:1981}, such as the
MPQ library within the GNU Multiprecision Arithmetic
Library (GMP)~\cite{granlund:gmp:2015}.  It represents each number $x$ as a
pair of multiprecision integers $p$ and $q$ with $x = p/q$.
%% It dynamically allocates
%% the memory for these two integers such that the only limitation on
%% their sizes is the available memory.  Assuming all of the weights
%% associated with the variables and their negations are rational,
MPQ
can compute the exact rational values of all multiplication and
addition operations, yielding an exact weighted count.  Unfortunately,
both the time and space required for storing and manipulating these numbers can be very
large.  In this paper, for example, we report experiments requiring over one gigabyte
to store the arguments and result of a single addition operation.
For most applications, rational arithmetic provides more precision than is required.
It would be preferable to have a floating-point
representation, but with a guaranteed level of
precision with respect to the actual weighted count.

In this paper, we apply two strategies to compute 
guaranteed-precision weighted model counts.  First, we consider the
case where the weights $w(x)$ and $w(\obar{x})$ for all variables $x$ are nonnegative, and
where the values are computed over a decision-DNNF Boolean
formula~\cite{beame:uai:2013,huang:jair:2007,oztok:cp:2014}.
We prove under these restrictions
that the degradation of
precision caused by rounding errors will be bounded by the logarithm
of the number of variables in the formula.  In practical terms, this
implies that floating-point arithmetic can be fully trusted in these
cases.  For example, by using the MPF software floating-point library
within GMP with a 128-bit fraction, we can guarantee that the computed
count for a formula with up to ten million variables will provide at least
30 decimal digits of precision.
This result has broad applicability.
For many applications
of weighted model counting, the weights are probabilities between
$0.0$ and $1.0$, or they are unit weights.  For these applications,  negative weights are never encountered.
Furthermore most top-down and bottom-up weighted model
counters either explicitly or implicitly operate on decision-DNNF
representations~\cite{beame:uai:2013}.

For formulas with negative weights, we demonstrate a family of
formulas where no bounded-precision numeric representation will
suffice.  Rational arithmetic provides the only option in such cases.
On the other hand, we show experimentally that floating-point
arithmetic suffices for standard ways to generate weight assignments,
but there is no way to be certain of the precision.  We also
demonstrate a strategy for generating random weight assignments that
often causes floating-point arithmetic to yield erroneous results.

We address the lack of certainty in a standard floating-point
evaluation by introducing \emph{interval} floating-point
arithmetic~\cite{hickey:jacm:2001} using the MPFI software
library~\cite{revol:rc:2005}. With this library, values are
approximated as intervals of the form $[\xmin, \xmax]$ such that the
true value lies within the interval, and both $\xmin$ and $\xmax$ are
represented in floating point.  The result of every operation is an
interval that is guaranteed to include the true result value, as long
as the argument intervals include their true values.  We show
experimentally that the intervals maintained during the computations
of weighted model counting are generally tight enough to provide
useful precision guarantees.

Putting these together, we present experimental results from a program
that performs weighted model counting of a decision-DNNF formula
generated by the D4 knowledge compiler~\cite{lagniez:ijcai:2017} using
a hybrid strategy.  When all weights are nonnegative, it uses MPF to
perform floating-point computations, relying on our precision
guarantee.  When some weights are negative, it performs up to two
levels of interval computation with MPFI, using increasing levels of
precision.  If these evaluations fail to provide the target precision,
it resorts to rational arithmetic using the MPQ\@.  The overall effect
is to achieve the twin goals of efficiency and guaranteed precision.
Although our experiments only use D4, similar results will hold for other top-down and bottom-up weighted model counters.

This work is motivated by both application need and technical
opportunity.  On the need side, there is evidence that the 
standard of precision for current weighted model counters is low.  In the 2020
weighted model counting competition, a count was considered correct if it was within $10\%$
of a precomputed result~\cite{fichte:jea:2020}.  That standard has
been raised to $0.1\%$ in more recent years~\cite{hecher:mc:2024}.
Even so, that is a very low standard for accuracy.  By
contrast, our work demonstrates the opportunity to compute
floating-point values that are guaranteed to have over 30 digits of
precision, i.e., to be within $10^{-28}\%$ of the true value,
while generally avoiding the high cost of rational arithmetic.

Regarding previous work, we are unaware of any 
systematic study of the numeric computations
performed in weighted model counting.  Special properties of
decision-DNNF formulas enable stronger precision guarantees than is
normally the case for floating-point arithmetic.  Some tools provide a
general capability to certify the accuracy of floating-point computations
via interval arithmetic~\cite{becker:fmcad:2016}, but we have not seen an experimental
evaluation of interval or rational arithmetic for
weighted model counting.

The paper is organized as follows.  In
Sections~\ref{sect:background:boolean} and
\ref{sect:background:numbers}, we cover background material in Boolean
formulas, weighted model counting, numeric error, and number
representations.  In Section~\ref{sect:nonneg} we present our
theoretical result concerning the case of nonnegative weights.  In
Section~\ref{sect:neg} we describe the challenges that negative
weights can present, but also experimental results showing that
floating-point arithmetic suffices in many cases.
Section~\ref{sect:hybrid} then describes the use of interval
arithmetic and our hybrid approach.  
Finally, Section~\ref{sect:conclusion}
presents some concluding remarks and  areas for
further research.


\section{Boolean Formulas and Weighted Model Counting}
\label{sect:background:boolean}

We consider Boolean formulas over a set of variables $\varset$ in
\emph{negation normal form}, where negation can only be applied to the
variables.  We refer to a variable $x$ or its negation $\obar{x}$ as a
\emph{literal}.  We use the symbol $\lit$ to indicate an arbitrary
literal.  The set of all formulas is defined recursively to consist of
literals, \emph{conjunctions} of the form $\phi_1 \land \phi_2$, and
\emph{disjunctions} of the form $\phi_1 \lor \phi_2$.  The set of
variables occurring in formula $\phi$ is denoted
$\dependencyset(\phi)$.  Typically, a formula is represented as a
directed acyclic graph, allowing a sharing of subformulas.  We
therefore define the \emph{size} of a formula to be the number of
unique subformulas.

A (total) assignment is a mapping $\assign \colon \varset \rightarrow
\{0, 1\}$.  Assignment $\assign$ is said to be a \emph{model} of
formula $\phi$ if the formula evaluates to $1$ under that assignment.
The set of models of a formula $\phi$ is written $\modelset(\phi)$.
We can also consider an assignment to be a set of literals, where $x \in \assign$
when $\assign(x) = 1$, and $\obar{x} \in \assign$ when
$\assign(x) = 0$, for each variable $x \in \varset$.

\emph{Weighted model counting} is defined in terms of a \emph{weight
  assignment} $w$, associating rational values $w(x)$ and
$w(\obar{x})$ with each variable $x \in \varset$.
The weight of an
assignment $\assign$ is then defined to be the product of its literal weights, and the weight
of a formula is the sum of the weights of its satisfying assignments:
\begin{eqnarray}
  w(\phi) & = & \sum_{\assign \in \modelset(\phi)} \;\;\prod_{\lit \in \assign} w(\lit) \label{eqn:wmc}
\end{eqnarray}

Computing the weighted count of an arbitrary formula is thought to be intractable.  However, it becomes
feasible when the formula is in \emph{deterministic decomposable} negation-normal form (d-DNNF):
\begin{enumerate}
\item The formula is in negation-normal form.  
\item All conjunctions are \emph{decomposable}~\cite{darwiche:jacm:2001,darwiche:jair:2002}.  That is, every subformula $\phi' = \phi_1 \land \phi_2$
  satisfies $\dependencyset(\phi_1) \cap \dependencyset(\phi_2) = \emptyset$.
\item All disjunctions are \emph{deterministic}~\cite{darwiche:jancl:2001,darwiche:jair:2002}.  That is, every subformula $\phi' = \phi_1 \lor \phi_2$ satisfies
  $\modelset(\phi_1) \cap \modelset(\phi_2) = \emptyset$.
\end{enumerate}
As an important subclass of d-DNNF, a formula is said to be in 
\emph{decision decomposable} negation-normal form (decision-DNNF) when every occurrence of a disjunction has the form 
$(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ for some variable $x$, referred to as the \emph{decision variable}.

There are several ways to compute the weighted count of d-DNNF formula $\phi$, all
based on an \emph{algebraic evaluation} of $\phi$ to compute a value $W(\phi)$:
\begin{enumerate}
\item Each literal $\lit$ is a assigned a rational value $W(\lit)$.
\item Each subformula $\phi' = \phi_1 \land \phi_2$ is evaluated as $W(\phi') = W(\phi_1) \cdot W(\phi_2)$.
\item Each subformula $\phi' = \phi_1 \lor \phi_2$ is evaluated as $W(\phi') = W(\phi_1) + W(\phi_2)$.
\end{enumerate}
The number of arithmetic operations in this evaluation is linear in the size of the formula.

The following methods use algebraic evaluation to compute a weighted model count $w(\phi)$ of formula $\phi$ for weight assignment $w$:
\begin{enumerate}
\item If the weight assignment satisfies $w(x) + w(\obar{x}) = 1$  for every variable $x$,
  then by letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item Formula $\phi$ is said to be \emph{smooth} if every disjunction $\phi_1 \lor \phi_2$ satisfies
  $\dependencyset(\phi_1) = \dependencyset(\phi_2)$~\cite{darwiche:jancl:2001,darwiche:jair:2002}.  For a smooth formula, 
by letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item If the weight assignment satisfies $w(x) + w(\obar{x}) \not = 0$ for every variable $x$,
  we can apply \emph{rescaling}, first computing $s(x) = w(x) + w(\obar{x})$ for each variable $x$
  and letting $W(x) = w(x)/s(x)$ and $W(\obar{x}) = w(\obar{x})/s(x)$.  
  Following the algebraic evaluation, the weighted count is computed as:
  \begin{eqnarray}
w(\phi) &=& \prod_{x\in\varset} s(x) \; \cdot \; W(\phi) \label{eqn:rescale}
  \end{eqnarray}
\end{enumerate}

An arbitrary formula can be smoothed by inserting \emph{smoothing terms} of the form $x \lor \obar{x}$~\cite{darwiche:jancl:2001}.
That is,
  a disjunction $\phi_1 \lor \phi_2$ having $x \in \dependencyset(\phi_1)$ but
  $x \not\in \dependencyset(\phi_2)$ is rewritten as $\phi_1 \lor [(x \lor \obar{x}) \land \phi_2]$.
  Adding smoothing terms can expand the size of a formula significantly, and it can be time consuming.
  More precisely, for $n = |X|$, and a formula with $m$ unique subformulas, it can require time $\Theta(m\cdot n)$ and increase the formula size by a factor of $n$.
  Some restricted formula classes allow more space- and time-efficient smoothing~\cite{shih:neurips:2019}, but the required properties do not hold for the formulas generated by most weighted model counters.
  
  These three methods can be combined by rescaling some variables
  and inserting smoothing terms for others.
  For example
  for any variable $x$ having $w(x) + w(\obar{x}) = 0$, we can insert
  smoothing terms, while rescaling for other variables $y$ such that $w(y) + w(\obar{y}) \not= 1$.
These particular smoothing terms
  effectively cause the subformula to evaluate to
  $0$.

\section{Approximations and Number Representations}
\label{sect:background:numbers}

When approximating rational number $x$ with value $\approximate{x}$, we
define the \emph{approximation error} $\aerror[\approxx, x]$ as:
\begin{eqnarray}
\aerror[\approxx, x] & = & \left\{ \begin{array}{ll}
  \frac{|\approxx - x|}{|x|}  & x \not = 0\\
  0 & x  = \approxx = 0\\
  1 & x = 0 \; \textrm{and} \; \approxx \not = 0
  \end{array} \right. \label{eqn:approx:error}
\end{eqnarray}
That is, we consider relative error when $x \not = 0$, and we do not allow any approximation for $x = 0$.
This value will equal 0 when $\approxx=x$, and it will be greater for weaker approximations.

We then define the \emph{digit precision} of the approximation as
\begin{eqnarray}
\digitprecision(\approxx, x) & = & \max(0, -\log_{10} \aerror[\approxx, x]) \label{eqn:digitprecision} 
\end{eqnarray}
This value will range from $0$ for a poor approximation, up to $+\infty$ when $\approxx=x$.
Intuitively, it indicates the number of matching digits in the decimal representations of $\approxx$ and $x$.

The rational arithmetic library MPQ represents a rational number $x$
as a pair $p$, $q$, such that $x = p/q$~\cite{knuth:rational:1981}.
The space for both $p$ and $q$ is allocated dynamically, and
so their size is limited only by the available memory.  We can
therefore use MPQ to compute an exact representation of a weighted
count from a d-DNNF formula, but the space and time required can be very high.

We consider floating-point numbers of the form
\begin{eqnarray}
x & = & (-1)^s \; \times \; f \; \times 2^{e} \label{eqn:floating-point}
\end{eqnarray}
where:
\begin{itemize}
\item Sign bit $s$ equals $0$ for nonnegative numbers and $1$ for negative numbers
\item Fraction $f$ is encoded as a $p$-bit binary number with an implicit binary point on the left.  That is $0 \leq f \leq 1-2^{-p}$.
\item Exponent $e$ is an integer, possibly with some limitation on its range.
\end{itemize}
As examples, consider two different floating-point formats:
\begin{itemize}
\item The IEEE~754 double-precision format uses a slightly different
  representation, but it maps to the notation of
  Equation~\ref{eqn:floating-point} with $p=53$ and an exponent range
  of $-1021 \leq e \leq 1024~\cite{overton:siam:2001}$.
  Unfortunately, the small exponent range (giving a magnitude range
  for the numbers of around $10^{\pm 308}$) limits the suitability of
  this representation for weighted model counting.  For example, as
  part of the evaluation of weighted model counting when all weights
  are nonnegative, described in Section~\ref{sect:nonneg}, we computed
  the counts for 980 combinations of formula and weight assignment
  using double-precision arithmetic and compared them to the exact
  values computed using rational arithmetic.  Fully 622 ($63.5\%$) of
  the double-precision weights had digit precision equal to $0.0$ due
  to the inadequate range of the representation.
\item The MPF software floating-point library allows the value of $p$
  to be set arbitrarily.  Our default configuration has $p=128$. On
  most 64-bit architectures, it represents the exponent as a 64-bit
  signed number.  This provides an ample exponent range, giving a
  magnitude range of over $10^{\pm 10^{18}}$.
  For example, the weighted model count for a tautology with $n$ variables, where all literals are assigned weight $w$, equals
  $2^n\cdot w^n = (2w)^n$.  Consider literal weight $w=10^{1000}$,
 far larger than can be represented in IEEE-754 double precision, and let $n$ equal one trillion, over five orders of magnitude
 larger than the largest formulas solved by current weighted model counters.  The weighted count is
 $(2 \times 10^{1000})^{10^{12}} \approx 10^{10^{15.00013}}$.  In the other direction, the conjunction of one trillion variables, each having a weight
 of $10^{-1000}$ has a weighted count of $10^{-10^{15}}$.  Even these extreme values are
 well within the range of the MPF representation.
\end{itemize}

When converting a rational number $x$ into a floating-point encoding, its value must be rounded to a
value $\round(x)$.  In doing so, it can introduce a \emph{rounding
error}.  
Letting $\roundepsilon = 2^{-p}$, and assuming that
the exponent range suffices to represent $x$,
we can assume that
$\aerror[\round(x), x] \leq \roundepsilon$~\cite{muller:hfpa:2018,knuth:fp:1981}.
As examples, the double-precision representation has $\roundepsilon = 2^{-53} \approx 1.11 \times 10^{-16}$, while the 128-bit MPF format
has $\roundepsilon = 2^{-128} \approx 2.94 \times 10^{-39}$.  As a consequence, the lower bounds of $\digitprecision(\round(x), x)$ are around $15.95$ and $38.53$, respectively.

Floating-point arithmetic is implemented in such a way that any
operation effectively computes an exact result and then rounds it to
encode the result as a floating-point value.  The maximum error from a sequence of operations
therefore tends to accumulate in multiples of $\roundepsilon$.
This yields error bounds of the form $\aerror[\approxx, x] \leq t\,\roundepsilon$,
which we  refer to as having at most $t$ units of
rounding error.

As a third representation, the MPFI interval arithmetic library
supports operations on
contiguous ranges of numbers $[\xmin, \xmax]$, where
$\xmin$ and $\xmax$ are floating-point numbers~\cite{revol:rc:2005}.  The endpoints of the
range are represented using the MPFR package, an extension of MPF to
provide more control over rounding~\cite{fousse:tms:2007}.  Arithmetic on intervals is performed
in such a way that the result of any operation is an interval that
spans the range of possible values of that operation applied to any
combination of values spanned by the argument intervals~\cite{hickey:jacm:2001,muller:hfpa:2018}.  The interval representation of $[\xmin, \xmax]$ is converted to
a single value as
the floating-point number closest to its midpoint $(\xmin + \xmax)/2$.

For interval $[\xmin, \xmax]$, we define the interval approximation error $\aerror([\xmin, \xmax])$ as
\begin{eqnarray}
\aerror([\xmin, \xmax]) & = & \left\{ \begin{array}{ll}
  \frac{\xmax - \xmin}{\min(|\xmin|, |\xmax|)}  & 0 \not \in [\xmin, \xmax]\\[0.8em]
  0 & \xmin = \xmax = 0 \\
  1 & \xmin = 0 \; \textrm{and} \; |\xmax - \xmin|  > 0 
  \end{array} \right. \label{eqn:interval:error}
\end{eqnarray}
For any values $\approxx, x \in [\xmin, \xmax]$, we can see that
$\aerror[\approxx, x] \leq \aerror([\xmin, \xmax])$.
We then define the digit precision of the interval as:
\begin{eqnarray}
\digitprecision([\xmin, \xmax]) & = & \max[0, -\log_{10} \aerror([\xmin, \xmax])] \label{eqn:interval:digitprecision} 
\end{eqnarray}
This value will range from $0.0$ for a very large interval, relative to the magnitudes of its endpoints, to $+\infty$ when the interval is tight with $\xmin = \xmax$.

\section{Error Analysis: Nonnegative Weights}
\label{sect:nonneg}

Here we evaluate how rounding errors accumulate via a series of
arithmetic operations when all arguments are nonnegative.
That is, assume the exact arguments $x$ and $y$ for each operation satisfy $x \geq 0$ and $y \geq 0$.
Rounding never causes a nonnegative number to become negative, and therefore 
the approximations $\approxx$ of  $x$ and $\approxy$ of $y$ satisfy $\approxx \geq 0$ and $\approxy \geq 0$.
None of the operations multiplication, addition, or division yield negative results when their arguments are nonnegative.
We can therefore
assume that all actual and approximate values under consideration are
nonnegative.

Our analysis builds on historic work for bounding the error produced
by a series of floating-point
multiplications~\cite{muller:hfpa:2018,wilkinson:nm:1960,wilkinson:rounding:1963} or
additions~\cite{higham:siam:1993}.  Our formulation simplifies the analysis
by weakening the
bound, and it applies only when
all arguments are nonnegative.

Suppose for nonnegative values of $x$ and $y$ and nonnegative values $s$ and $t$, we have
$\aerror[\approxx, x] \leq s\, \roundepsilon$ and
$\aerror[\approxy, y] \leq t\, \roundepsilon$, respectively.
Therefore
$(1-s\,\roundepsilon)\, x \leq \approxx \leq (1+s\,\roundepsilon)\, x$ and
$(1-t\,\roundepsilon)\, y \leq \approxy \leq (1+t\,\roundepsilon)\, y$.


\subsection{Multiplication}

Assume that $x > 0$ and $y > 0$ and consider the effect of multiplying their approximations
$\approxx$ and $\approxy$.  To simply the analysis, let us impose as an additional constraint that $s\,t \leq 1/\roundepsilon$.
The product $\approxx \cdot \approxy$  satisfies
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2)$, and we can use the additional constraint to replace $s\,t\,\roundepsilon^2$ by $\roundepsilon$,
giving
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t+1)\,\roundepsilon)$.
In the other direction, $\approxx \cdot \approxy$ satisfies
$(x\cdot y) (1 - (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2) \leq \approxx \cdot \approxy$.  We can drop the term
$s\,t\,\roundepsilon^2$ to give
$(x\cdot y) (1 - (s+t)\,\roundepsilon) \leq \approxx \cdot \approxy$.  These two bounds guarantee that
$\aerror[\approxx \cdot \approxy, x \cdot y] \leq (s+t+1)\,\roundepsilon$.
Rounding this result can introduce an additional error of at most $\roundepsilon$, and therefore
$\aerror[\round(\approxx \cdot \approxy), x \cdot y] \leq (s+t+2)\,\roundepsilon$.

When $x=0$ (respectively, $y=0$), we will have $\approxx=0$ (resp., $\approxy = 0$) and therefore $x \cdot y = \approxx \cdot \approxy = 0$.
We can therefore state that  for any nonnegative values of $x$ and $y$, the three conditions $\aerror[\approxx, x] \leq s\,\roundepsilon$,
$\aerror[\approxy, y] \leq t\,\roundepsilon$, and
$s\,t \leq 1/\roundepsilon$, imply that
$\aerror[\round(\approxx \cdot \approxy), x \cdot y] \leq (s+t+2)\,\roundepsilon$.

Thus, for values of $s$, $t$, and $\roundepsilon$ satisfying our
additional constraint, a multiplication operation, at most, propagates
the errors of its arguments as their sum and adds two units of
rounding error.

\subsection{Addition}

When positive values $x$ and $y$ are added, their approximations  $\approxx$ and $\approxy$ satisfy
$(x + y) (1 - r\,\roundepsilon) \leq \approxx + \approxy \leq (x + y) (1 + r\,\roundepsilon)$, where $r = \frac{s\,x + t\,y}{x+y}$.  That is, the resulting error $r$ is bounded by a weighted average
of those of its arguments.  For all values of $x$ and $y$, $r$ cannot exceed the maximum of $s$ and $t$.
Rounding the sum can add at most one unit of rounding error, and so we have
$\aerror[\round(\approxx + \approxy), x + y] \leq (\max(s,t)+1)\,\roundepsilon$.

If $x = 0$ (respectively, $y = 0$), we will have $\round(\approxx + \approxy) = \approxy$ (resp., $= \approxx$).
We can therefore state that for any nonnegative values of $x$ and $y$, the two conditions $\aerror[\approxx, x] \leq s\,\roundepsilon$ and
$\aerror[\approxy, y] \leq t\,\roundepsilon$ imply that 
$\aerror[\round(\approxx + \approxy), x + y] \leq (\max(s,t)+1)\,\roundepsilon$.

Thus, an addition operation, at most, propagates the maximum error of its arguments and adds a unit of rounding error.

\subsection{Evaluating a decision-DNNF formula}
\label{sect:error:formula}

Suppose we use floating-point arithmetic to compute the sums and
products in an algebraic evaluation of a decision-DNNF formula $\phi$.
We assume that the value $W(\lit)$ for each literal $\lit$ is
represented by a floating-point number $\approxW(\lit)$ such that
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$.  In practice,
this implies that rescaling must use rational arithmetic to
compute exact representations of $s(x)$, $w(x)/s(x)$, and
$w(\obar{x})/s(x)$ for each variable $x$, so that only one unit of
rounding error is introduced when representing each value $W(\lit)$.
We can then bound the error of the computed value $W(\phi)$ as follows:
\begin{lemma}
  The algebraic evaluation of a decision-DNNF formula $\phi$  having $|\dependencyset(\phi)| = n$, with
  $n \leq 1/(2\sqrt{\roundepsilon})$
  using floating-point arithmetic,
  and where all literals $\ell$ satisfy $W(\lit) \geq 0$,
  will yield an approximation $\approxW(\phi)$ satisfying
  $\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.
  \label{lemma:approx:pos}
\end{lemma}

The proof of this lemma proceeds by induction on the structure of $\phi$:
\begin{enumerate}
\item For literal $\lit$ with weight $W(\lit)$, its approximation $\approxW(\lit)$ satisfies
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$, which is within the error bound of $(4n-2)\,\roundepsilon$ for $n=1$.
\item For conjunction $\phi$ of the form $\phi_1 \land \phi_2$, there must be some $k$, with $1 \leq k < n$, such that $|\dependencyset(\phi_1)| = k$
  and $|\dependencyset(\phi_2)| = n-k$.
\begin{enumerate}
\item Let us first test whether the requirement that $n \leq 1/(2\sqrt{\roundepsilon})$ 
  guarantees that the conditions on $s$, $t$, and $\roundepsilon$ required for the multiplication bound  hold.
For $s = 4k-2$ and $t = 4(n-k)-2$
    we require that $s\,t \leq 1/\roundepsilon$.  We can see that $s\,t \leq 16\,k\,(n-k)$. This quantity will be maximized when $k = n/2$,
    and therefore $s \, t \leq 4\,n^2$.
    Given our limit on $n$ with respect to $\roundepsilon$, we have $s \, t \leq 1/\roundepsilon$.
  \item
    We can also see that if $n \leq 1/(2\sqrt{\roundepsilon})$, then both
    $k \leq 1/(2\sqrt{\roundepsilon})$ and  $n-k \leq 1/(2\sqrt{\roundepsilon})$.
  \item
We can therefore assume by induction that 
 $\aerror[\approxW(\phi_1), W(\phi_1)] \leq (4 k-2) \,\roundepsilon$
  and also that $\aerror[\approxW(\phi_2), W(\phi_2)] \leq (4 (n-k)-2) \,\roundepsilon$.  Their product, after rounding
  will satisfy 
$\aerror[\approxW(\phi_1 \land \phi_2), W(\phi_1 \land \phi_2)] \leq [(4 k -2) + (4 (n-k) -2) + 2]\,\roundepsilon = (4n-2) \,\roundepsilon$.
\end{enumerate}
\item For disjunction $\phi$ of the form
  $\phi = (x \land \phi_1) \lor (\obar{x} \land \phi_2)$, let us use the notation $\lit_1 = x$ and $\lit_2 = \obar{x}$
  and consider the two subformulas $\lit_i \land \phi_i$ for $i \in \{1,2\}$.
  Since all products are decomposable, we must have $x \not \in \dependencyset(\phi_i)$,
  and therefore $|\dependencyset(\phi_i)| = n-1$.  We can also see that the condition 
  $n \leq 1/(2\sqrt{\roundepsilon})$ implies that   $n-1 \leq 1/(2\sqrt{\roundepsilon})$.
  By induction, we can therefore assume that
  $\aerror[\approxW(\phi_i), W(\phi_i)] \leq (4(n-1)-2) \,\roundepsilon = (4n-6)\,\roundepsilon$.  Rounding the literal weights will yield
  $\aerror[\approxW(\lit_i), W(\lit_i)] \leq \roundepsilon$.  Let $y_i$ denote the product $W(\lit_i) \cdot W(\phi_i)$ for $i \in \{1,2\}$.
  Its rounded value will satisfy
  $\aerror[\approxy_i, y_i] \leq (4n-3) \,\roundepsilon$.  Summing $\approxy_1$ and $\approxy_2$ and rounding the result will therefore give
  an approximation $\approxW(\phi)$ to $W(\phi) = y_1 + y_2$ with
$\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.  
\end{enumerate}

Observe that this proof relies on the decomposability of the
conjunctions to bound the error induced by multiplication operations.
It relies on the decision structure of the formula only to bound the
depth of the additions.  It does not rely on the formula being deterministic.

In the event of rescaling, we must also consider the error introduced
when computing the product $P = \prod_{x\in\varset} s(x)$.  We assume that
each term $s(x)$ is represented by a floating-point value
$\approxs(x)$ such that $\aerror[\approxs(x), s(x)] \leq
\roundepsilon$.  In practice, this requires using rational arithmetic
to represent $w(x)$ and $w(\obar{x})$ and to compute their sum.  The
only error introduced will then be when converting the sum into
a floating-point representation.

We can then bound the error of the product as
\begin{lemma}
  The computation of the product $P = \prod_{x\in\varset} s(x)$ having
$|\varset| = n$, with $n \leq 1/(2\sqrt{\roundepsilon})$ using floating-point arithmetic
and where all literals $\ell$ satisfy $s(\lit) \geq 0$,
will yield an approximation $\approxP$ satisfying
  $\aerror[\approxP, P] \leq (3n-2)\,\roundepsilon$.
  \label{lemma:approx:product}
\end{lemma}

The proof of this lemma proceeds much like that for Lemma~\ref{lemma:approx:pos}.  We assume an arbitrary association of the subproducts, and so $P$ can be computed as
$P_1 \cdot P_2$, where $P_1$ is the product of $k$ elements and $P_2$ is the product of $n-k$ elements, with $1 < k < n$.
The smaller coefficient of $3$ arises due to the lack of any addition operations.

Combining these two results, we can state the following result about weighted model counting when all weights are nonnegative:
\begin{theorem}
  \label{thm:approx:pos}
Computing the weighted model count of
a decision-DNNF formula $\phi$, where all literal weights are nonnegative, with $|\dependencyset(\phi)| = n$, and using floating-point arithmetic with a $p$-bit fraction, such that $\log_2 n \leq p/2-1$
will yield an approximation $\approxw(\phi)$ to the true weighted count $w(\phi)$, such that
\begin{eqnarray}
\digitprecision(\approxw(\phi), w(\phi)) & \geq & \log_{10}(2)\cdot p - \log_{10}(n) - c\label{eqn:precision:wmc}
\end{eqnarray}
where $c = \log_{10} 7$ when rescaling is required and $c = \log_{10} 4$ when no rescaling is required
\end{theorem}

Let us examine the practical implications of this theorem.  In
particular, assume we use the MPF library with $p=128$.  Only a single
formula from the 2024 weighted model competition had over one million
variables, and so we can assume a bound of $n \leq 10^7$.  The
conditions for Equation~\ref{eqn:precision:wmc} to hold will 
be satisfied, since $\log_2 n < 23.3$ and $p/2-1 = 63$.  We will
therefore have $\digitprecision(\approxw(\phi), w(\phi)) \geq 38.53 -
0.85 - 7 = 30.68$.  That is, even though the floating-point format
provides only around 38 digits of precision for a single rounding,
it can achieve over 30 digits of precision when
computing the algebraic evaluation of a decision-DNNF formula with 10
million variables and then rescaling to compute a weighted model count

In scaling to larger formulas, each time
the number of variables is increased by a factor of 10, at most one
digit of precision is lost.  For example, we could scale to $n=10^{12}$, while keeping $p=128$, and still have over 25 digits of precision.  That is over nine digits greater than the precision
provided by the IEEE double-precision representation.  If that precision is not sufficient, we could increase $p$ to $192$ and be guaranteed a decimal precision of over $44$.
Importantly, the bound of Equation~\ref{eqn:precision:wmc} holds regardless of the formula size and the weight assignment, 
as long as all weights are nonnegative.

For the remainder of this paper, we set a target precision of 30 decimal digits for
weighted model counting.  This number is chosen because 1) it should be
sufficient for most applications, and 2) it
is achievable, at least for nonnegative weights, with a standard software floating-point package.

\subsection{Allowing non-decision disjunctions}
\label{sect:ddnnf}

Our previous requirement that each disjunction have a decision
variable had the effect of limiting the depth of the sum operations in
a formula to $n$.  Without this requirement, the depth can become
exponential.  For example, consider a d-DNNF formula encoding a
tautology over $n$ variables, constructed as follows.  For each of the
$2^n$ possible assignment $\assign$, we use conjunction operations to
encode subformula $\phi_{\assign} = \bigwedge_{\lit \in \assign} \lit$.  We then
use a linear sequence of $2^n-1$ disjunctions to combine these subformulas to
form $\phi$.  The accumulated error from the resulting addition operations could grow
exponentially with $n$.  Of course, such a formula is completely
impractical for even modest values of $n$, but this construct
demonstrates that, without further restrictions on the structure of a
d-DNNF formula, we cannot guarantee a reasonable bound on the error incurred by an algebraic evaluation.

%% The only
%% cases we know where non-decision conjunctions occur in d-DNNF formulas is with projected model counting, where
%% instances of a decision variable or its negation are removed by projection~\cite{derkinderen:ictai:2024}.
%% In such cases, the bound of Equation~\ref{eqn:precision:wmc} holds,
%% but we must include all variables, included those projected away, in the
%% variable count $n$.

Rather than trying to find a general bound for all d-DNNF formulas, we
can evaluate each formula individually to estimate the maximum
rounding error that might be encountered in its algebraic evaluation.  That is,
define the integer-valued error measure $e(\phi)$ for formula $\phi$
recursively.  For a literal, $\lit$, let $e(\lit) = 1$.
For a subformula of the form $\phi' = \phi_1 \land \phi_2$, let $e(\phi') = e(\phi_1) + e(\phi_2) + 2$.
For a subformula of the form $\phi'= \phi_1 \lor \phi_2$, let $e(\phi') = \max[e(\phi_1),\, e(\phi_2)] + 1$.
Letting $e = e(\phi)$, we can then first perform the test $\log_2 e \leq p/2$.  With that condition satisfied,
we can ensure that an algebraic evaluation of $\phi$ will yield an approximation $\approxW$, such that
$\digitprecision(\approxW, W) \geq \log_{10}(2)\cdot p - \log_{10}(e)$.

For example, with our tautology construction, we would have $e = 2\,n
+ 2^{n-1} -3$. That would indicate that floating-point
evaluation is not feasible.  On the other hand, if we were to
restructure the disjunctions into a balanced binary tree, we would get
$e = 2\,n + n = 3\,n$, indicating that a floating-point evaluation, however impractical, would
return a precise result.


\subsection{Experimental Validation}

\begin{figure}
\centering{%
\begin{tikzpicture}
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10}, 
      legend style={at={(0.30,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/upos-mpf+vars}
    \input{data-formatted/epos-mpf+vars}
    \input{data-formatted/prodp-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$+$},
      \scriptsize \textsf{MC2024, Exponential$+$},
      \scriptsize \textsf{Optimized Product}
    }
    \addplot[mark=none] coordinates{(1,36.69) (1e7,30.69)};
    \node[right] at (axis cs: 2, 34.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Nonnegative Weights.  The precision is guaranteed to be greater than the bound.
We set as a target to have digit precisions of at least 30.0.}
\label{fig:pos:mpf}
\end{figure}

To experimentally test the bound of Equation~\ref{eqn:precision:wmc},
we evaluated 200 benchmark formulas from the public and private portions
of the 2024 Weighted Model Counting Competition.\footnote{\url{https://mccompetition.org/2024/mc_description.html}}  We attempted to
convert these into decision-DNNF using version 2 of the D4
knowledge compiler.\footnote{
Available at \url{https://github.com/crillab/d4v2}}
We were able to compile 100 of them with a time
limit on 3600 minutes on a machine with 64~GB of random-access memory.
We define a problem \emph{instance} to be a combination of a formula
plus a weight assignment for all of its literals.

For each of these formulas, we generated two different collections of instances, each with five weight assignments:
\begin{itemize}
\item \textsf{Uniform$+$}: For each variable $x$, weight $w(x)$ is represented by a 9-digit decimal number selected uniformly in the range
  $[10^{-9},\,1-10^{-9}]$. The weight for $\obar{x}$ is then set to
  $w(\obar{x}) = 1-w(x)$.  Such a weight assignment is typical of those used in recent weighted model counting competitions~\cite{fichte:jea:2020}.
\item \textsf{Exponential$+$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$
  are drawn independently from an exponential distribution in the range
  $[10^{-9},\,10^{+9}]$.  Each weight is represented by a decimal number with 9 digits to the right of the decimal point.
\end{itemize}

%% By comparison, Dilkas~\cite{dilkas:cpaior:2023} devised a number
%% of benchmarks problems for evaluating weighted model counters, but his
%% focus was more on the formula structure than on the weight
%% assignment.  The assignments he generated all had weights represented
%% as two-digit decimal numbers between $0$ and $1$, and such that $w(x)
%% + w(\obar{x}) = 1$ for all variables $x$.  Numerically speaking, these weight
%% assignments should be less challenging than ours.

For each instance, we evaluated
the weighted count using MPF (with $p=128$) to get an approximate weight $\approxw$ and
using MPQ to get an exact weight $w$.  We then evaluated the digit precision according to Equation~\ref{eqn:digitprecision}.   Our implementation used rescaling for all variables
with $w(x) + w(\obar{x}) \not \in \{0, 1\}$.
Although not required for the instances used in this evaluation,
it will insert smoothing terms when $w(x) + w(\obar{x}) = 0$ for variable $x$.

In addition, we evaluated formulas of the form $\bigwedge_{1\leq i
  \leq 10^d} x_i$ for values of $d$ ranging from $0$ to $6$ using a
single weight for every variable.  We swept a parameter space of
weights of the form $1 + 10^{-9}\,k$ for $1 \leq k \leq 100$ and
chose the value of $k$ that minimized the digit precision.  We refer
to this as the \textsf{Optimized Product} collection.

Figure~\ref{fig:pos:mpf} shows the results of these evaluations for
the three collections.  For the two collections of weight assignments for each 2024 competition formula,
we show only the minimum precision for each of the five randomly
generated  weight assignments.  Each data point
represents one combination of formula and weight selection method and is placed
along the X-axis according to the number of variables
and on the Y-axis according to the computed digit precision.
The plot also shows the precision bound of Equation~\ref{eqn:precision:wmc} for $c=\log_{10} 7$.
Results are shown for 98 of the 100 formulas, since the evaluation ran out of memory when using rational arithmetic for two of the formulas.

As can
be expected, all data points stay above the precision bound.  Indeed,
most exceed the bound by several decimal digits.  Our bound assumes
that rounding either consistently decreases or consistently
increases each computed result.  In practice, rounding goes in both directions, and
therefore the computed results stay closer to the true values.

The optimized products, on the other hand, demonstrate that particular
combinations of formula and weight assignment can come within 2 decimal digits of the
precision bound and to track its general trend.

\section{Error Analysis: Negative Weights}
\label{sect:neg}

The analysis of Section~\ref{sect:nonneg} no longer holds when
some literals have negative weights.  With a floating-point
representation, summing combinations of negative and nonnegative
values can cause \emph{cancellation}, where arbitrary levels of
precision are lost.  Consider, for example, the computation
$s + T - T$, where $s$ and $T$ are nonnegative, but $s \ll T$.  Using
bounded-precision arithmetic, evaluating the sum as $s + (T - T)$ will yield a value
that is very close to $s$.
Evaluating it as $(s + T) - T$, however, can yield $0$ or some other value that bears little relation to $s$.

Cancellation can arise when evaluating decision-DNNF formulas to such a degree that no precision $p$ that grows sublinearly with $n$ will suffice.
As an example, consider the following smooth, decision-DNNF formula over $n+1$ variables:
\begin{eqnarray}
\phi_n  & = & z \land \left[\bigwedge_{i = 1}^{n} \obar{x}_i \; \lor \; \bigwedge_{i = 1}^{n} x_i\right] \quad \lor \quad \obar{z} \land \left [\bigwedge_{i = 1}^{n} x_i\right] \label{eqn:max:precision}
\end{eqnarray}
with a weight assignment having only a single literal assigned a negative weight:
\begin{displaymath}
\begin{array}{lllll}
\makebox[1cm]{} &  w(z) \; = \; +1 & \makebox[2cm]{} &  w(x_i) \; = \; 10^{+9} & 1 \leq i \leq n \\
\makebox[1cm]{} &  w(\obar{z}) \; = \; -1 & &  w(\obar{x}_i) \; = \; 10^{-9} & 1 \leq i \leq n \\
\end{array}
\end{displaymath}
Computing $w(\phi_n)$  evaluates the sum $(s + T) - T$, where
$s = 10^{-9n}$ and $T = 10^{+9n}$.  Avoiding cancellation requires using a floating-point representation with a fraction of at least
$p = (18 \cdot \log_2 10)\, n \approx 60 \, n$ bits.
On the other hand, using MPQ, $w(\phi_{10^7})$ can be computed exactly in around 35 seconds, even though the final step requires a total of 1.87~gigabytes to store the arguments
$s+T$ and $-T$, and the result $s$.  In general, however, rational arithmetic can be very time and memory intensive.


\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10},
      legend style={at={(0.32,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/uposneg-mpf+vars}
    \input{data-formatted/eposneg-mpf+vars}
    \input{data-formatted/bposneg-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$\pm$},
      \scriptsize \textsf{MC2024, Exponential$\pm$},
      \scriptsize \textsf{MC2024, Limits$\pm$}
    }
    \addplot[mark=none, dashed] coordinates{(1,36.69) (1e7,30.69)};
    \node[right] at (axis cs: 2, 34.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Negative Weights.  There is no guaranteed bound for precision, but many cases remain above the nonnegative weight bound.
The Limits$\pm$ weight assignment was designed to maximize precision loss.}
\label{fig:posneg:mpf}
\end{figure}

Contrary to the example of
Equation~\ref{eqn:max:precision},
floating-point arithmetic performs surprisingly well for
many real-life
situations, even in the presence of negative weights.
In Figure~\ref{fig:posneg:mpf}, we see a similar plot to that of Figure~\ref{fig:pos:mpf} for
three collections of weight assignments having negative weights.  The first two collections are generalizations of those used earlier:
\begin{itemize}
\item \textsf{Uniform$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes drawn independently
from a uniform distribution in the range  $[10^{-9},\,1-10^{-9}]$ and are represented as 9-digit decimal numbers.  Each is negated with probability $0.5$.
\item \textsf{Exponential$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes
  drawn independently from an exponential distribution in the range $[10^{-9},\,10^{+9}]$ and are represented with 9-digits to the right of the decimal point.  Each is negated with probability $0.5$.
\end{itemize}
As can be see for these plots, the results mostly stay above the precision bound of Equation~\ref{eqn:digitprecision},
even though this bound is not guaranteed to hold.  All stay above the target precision of $30.0$.

On deeper inspection, we can see that setting up the conditions for a
cancellation of the form $(s + T) - T'$, where $T \approx T'$, require
1) a large dynamic range among the computed values to give widely different values $s$ and $T$, and 2) sufficient
homogeneity in the computed values that we get two values $T$ and
$T'$ such that $T \approx T'$.  A uniform distribution has neither of
these properties.  An exponential distribution has a large dynamic
range, but the computed values tend to be very heterogenous.

To create a weight assignment that has more chance of cancellation, we devised the following class of assignments:
\begin{itemize}
\item\textsf{Limits$\pm$}:  Each weight $w(x)$ and $w(\obar{x})$ has a magnitude, chosen at random, of either $10^{-9}$ or $10^{+9}$, and they are each set negative with probability $0.5$.
  However, we exclude assignments with $w(x) + w(\obar{x}) = 0$.
\end{itemize}
The idea here is to give large dynamic ranges plus a high degree of
homogeneity.  The plots for this assignment in
Figure~\ref{fig:posneg:mpf} demonstrate that this strategy achieves
the desired effect, with many results falling below the target precision of $30.0$.

Figure~\ref{fig:posneg:mpf} presents a pessimistic
perspective for these instances, since it only shows the minimum
precision achieved out of five instances for each formula.  Considering all 490 instances
for the 98 formulas evaluated, 221 (45\%) yielded results above the
target precision of $30.0$.  We can also see how our choice of weights
leads to two bands of low precision.  129 instances (26\%) had digit
precisions in a band between $19.0$ and $23.3$.  These were ones where
the evaluation encountered values of $s$ and $T$ that
differ by a factor of around $10^{18}$.  The remaining 140 instances
(29\%) had digit precisions below $5.0$.  These were ones where the
encountered values of $s$ and $T$ differed by a factor of around
 $10^{36}$.

\section{Reliable and Efficient Weight Computation}

\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              xmode=normal,xmin=0, xmax=50,
                              xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              xticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              ymode=normal, ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Estimated Precision}, ylabel={Actual Precision}
            ]

    \draw[ fill={blue!30}, opacity=0.2] (axis cs: 0,30) rectangle (axis cs: 30,50);
    \node at (axis cs:15,40) {Overly pessimistic};
    

    \input{data-formatted/uposneg-mpfi-est+act}
    \input{data-formatted/eposneg-mpfi-est+act}
    \input{data-formatted/bposneg-mpfi-est+act}
    \legend{
      \scriptsize \textsf{MC2024, Uniform $\pm$},
      \scriptsize \textsf{MC2024, Exponential $\pm$},
      \scriptsize \textsf{MC2024, Limits $\pm$}
    }
    \addplot[mark=none] coordinates{(0,0) (50,50)};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Predictive Accuracy of MPFI Interval Arithmetic.  MPFI never has a higher estimate than the actual, but it can
incorrectly predict a precision less than the target of 30.0.}
\label{fig:mpfi}
\end{figure}


We can see from Figure~\ref{fig:posneg:mpf} that floating-point
evaluations generate accurate results in many cases, but we must be
able to discern when those occur.  Given that capability, we can
devise a strategy to use progressively more costly evaluation methods
to reliably compute weighted counts.

\subsection{Interval Computation applied to weighted model counting}

Interval arithmetic provides a mechanism for using the approximate
computations of floating-point arithmetic, while providing a
guaranteed precision for the result.  It will only be beneficial,
however, if the interval bounds remain tight enough that the digit
precision bound of Equation~\ref{eqn:interval:digitprecision} meets
our target digit precision.  Our target bound of 30 seems fairly
aggressive in this respect: the width of the interval $\xmax-\xmin$
must be over 30 orders of magnitude smaller than the magnitudes of
$\xmin$ and $\xmax$.  Even the instances with only nonnegative weights
had digit precisions as low as 34.5, and so there is not much
room for further degradation.

Figure~\ref{fig:mpfi} shows the result of evaluating 100 formulas for
the three weight assignment collections containing negative weights,
with five instances per collection for each formula.  
The evaluation used
MPFI (with $p=128$) to get an estimated digit precision and
a nominal weight (the midpoint of the interval), along with MPQ to get
the exact weight.  The actual precision was then computed based on the
nominal and actual weights.  The evaluations using MPQ ran out of
memory for two of the formulas, and hence the plot shows 1470
data points.  The X-axis shows estimated precision, while the Y-axis
shows the actual precision, with the diagonal line indicating when the
two precisions are equal.  As is expected, every point lies above this
line---the interval computation never overestimated the digit
precision.

Overall, we can see that the interval estimates were quite reliable,
especially for predicting which computed weights exceeded the target
threshold of 30.
The
interval computations determined that 1196 ($81.3\%$) instances were
above the target threshold: 490 from \textsf{Exponential$\pm$}, 487 from
\textsf{Uniform$\pm$}, and 219 from \textsf{Limits$\pm$}.
Points lying in the blue rectangle
indicate instances where the estimate was overly pessimistic: they
estimated a target precision below 30, while the actual precision was
above.  This occurred for only 36 of the 1470 instances ($2.4\%$).  Of
these, 3 were from the \textsf{Uniform$\pm$} collection, while 33 were
from the \textsf{Limits$\pm$} collection.  

The interval analysis captures the general trend shown in
Figure~\ref{fig:posneg:mpf} that, even with negative
weights, floating-point evaluation only degrades significantly due to cancellation
for the weight assignments designed to maximize
this effect.  This gives us hope that we can use interval computation
to handle a large portion of instances having negative weights.

\subsection{A Hybrid Approach}
\label{sect:hybrid}

We are now ready to combine our three approaches---floating-point
arithmetic, interval computation, and rational arithmetic---into a
single, hybrid approach.  We devised the following scheme, and we can
measure its success based on 2500 instances---100 formulas, each with five collections of five instances.
\begin{enumerate}
\item For instances where all weights are nonnegative, use MPF,
  relying on the bound of Equation~\ref{eqn:precision:wmc} to
  guarantee sufficient precision.  This evaluation succeeded for all 1000 such
  instances, including 20 for which the evaluation with rational arithmetic failed.
\item For instances with negative weights, we perform a sequence of evaluations having successively greater cost:
\begin{enumerate}
\item 
  Use MPFI with $p=128$.  If the estimated precision
  bound meets the target bound, then we are done.  This succeeded for
  1222 of the 1500 instances evaluated, including 26 for which the evaluation with rational arithmetic failed.
\item For instances where the estimated precision does not meet the target, perform a second run with MPFI, but with $p=256$.  This succeeded
  for 168 of the 278 instances evaluated, including 4 for which the evaluation with rational arithmetic failed.
\item When the second attempt at interval computation fails, perform
  an evaluation using MPQ to perform rational arithmetic.  This
  succeeded for the remaining 110 instances.
\end{enumerate}
\end{enumerate}
Overall this strategy succeeded for all 2500 instances.


\begin{table}
  \caption{Performance Comparison of Different Implementation Strategies.  Run entries of the form $S$+$F$ indicate that $S$ runs
  were successful and $F$ runs either ran out of memory or failed to meet the target precision.  The strategy yielding the least total time is shown in red.}
  \label{tab:compare}
  \centering{
%  \begin{tabular}{llrrrrrr}
  \begin{tabular}{llrrrrr}
    \toprule
    \multicolumn{1}{c}{Strategy} & & \multicolumn{1}{c}{MPF} & \multicolumn{1}{c}{MPFI-128} & \multicolumn{1}{c}{MPFI-256} & \multicolumn{1}{c}{MPQ} & \multicolumn{1}{c}{Combined}
    % & \multicolumn{1}{c}{Avg (s)}
    \\
    \midrule
    \input{method_table}
    \\[-1em]
   \bottomrule
  \end{tabular}
  } % Centering
\end{table}

Table~\ref{tab:compare} summarizes the total time taken by the
different levels of evaluation and presents a comparison of five
different evaluation strategies.  We can see that if all 2500
instances were evaluated with MPQ, we would complete 2450 of them in a
total of $93.1$ hours, including over $22$ hours for the 50 failed
runs.  Using MPF
for the instances with nonnegative weights enables completing an
additional 20 instances and drops the total time to $55.97$ hours.
Using MPF when
possible has a clear benefit.  Using one pass with MPFI (for $p=128$)
for the instances with negative weights and then using MPQ for those
that do not meet the target precision completes all but 4 instances
and drops the total time to $22.42$ hours
Although this approach yields an
increased time when MPQ is ultimately used, this is more than offset
by the many cases that were successfully evaluated using MPFI\@.  Our
full hybrid approach completes all 2500 instances in a total of
$17.74$ hours.
The performance gain with the second run of MPFI is not dramatic, but
being able to complete all instances is a significant achievement.
The fifth entry shows the performance when skipping the
evaluation using MPFI with $p=128$, using only $p=256$.  It shows
that avoiding 278 failed evaluations using MPFI with $p=128$ does not
compensate for the time required to perform all 1500 evaluations using
MPFI with $p=256$.  Appendix~\ref{app:performance} provides more
details on the performance of the hybrid strategy.

%% Of course, finding an optimal strategy depends heavily on the formulas
%% to be evaluated and the weight assignments.  Our overall strategy of using progressively more
%% precise but costly evaluation methods should work across a large range of problems.


\section{Conclusions}
\label{sect:conclusion}

For many applications, floating-point arithmetic can introduce
significant errors due to rounding and does not provide any way to quantify the error.
This
paper shows that such uncertainty can be avoided for weighted model
counting.  When all weights are nonnegative, results can be computed
using floating point with guaranteed precision.  When some weights
are negative, the program can attempt one or more levels of interval
computation, and these should handle a large fraction of the instances.  Ultimately,
the program may need to use full rational arithmetic, but the number
of such cases should be small.

Several extensions of this work bear investigation.  Considering other representations,
it would be useful to apply these methods when computing weighted model counts based on 
Sentential Decision Diagrams (SDDs), a representation that generalizes ordered BDDs~\cite{darwiche:ijcai:2011}.
An SDD could be converted into a d-DNNF formula and then evaluated with guidance by the error estimation method presented in Section~\ref{sect:ddnnf}.
%% These
%% generalize BDDs such that each node can have $k$ pairs of subformulas
%% (each subformula being a node in the SDD) of the form $p_i, s_i$,
%% where the set of variables occurring in each $p_i$ is disjoint from  the set of those
%% occurring in the corresponding $s_i$, and where the models of $p_i$ and
%% $p_j$ are disjoint for all $i$ and $j$ such that $1 \leq i < j \leq
%% k$.  An SDD can be transformed into a d-DNNF formula by expanding each
%% node as $k$ conjunction operations of the form $p_i \land s_i$ and to
%% use $k-1$ disjunction operations to combine their results.  For nonnegative
%% weight assignments, we could compute the error bound for the formula
%% as described in Section~\ref{sect:ddnnf} and then set a suitable
%% floating-point bit precision $p$ to achieve the desired digit precision.  In performing the
%% conversion from SDDs to d-DNNF, the disjunctions for the nodes should
%% structured as balanced binary trees to minimize the rounding errors
%% introduced by the corresponding addition operations.  For weight
%% assignments with negative weights, we could proceed with the same
%% layered approach as for decision-DNNF formulas.

There are several variants of weighted model counting that require
either negative weights or arithmetic on negative numbers.  These
include the \emph{relaxed Tseitin}
transformation~\cite{meert:starai:2016}, which can be seen as a
variant of a Plaisted-Greenbaum transformation~\cite{plaisted:jsc:1986} that preserves
the
weighted count.  It introduces two auxiliary variables $r$ and $z$ having
$w(z) = w(\obar{z}) = w(r) = 1$ and $w(\obar{r}) = -1$.
As a second example, the \emph{gradient
semiring}~\cite{eisner:acl:2002, kimmig:jal:2017} extends standard
model counting such that each weight is pair $\langle p, d \rangle$,
where $p$ is a probability and $d$ is a partial derivative of the
probability with respect to some variable $x$.  The weights
associated with variable $x$ are $w(x) = \langle p(x), 1\rangle$ and
$w(\obar{x}) = \langle 1-p(x), -1\rangle$, where $p(x)$ equals the probability that variable $x$ is assigned $1$.
We speculate that interval floating-point arithmetic would perform well for both of these variants.
%% This semiring allows the computation
%% of partial derivatives of the weighted count of Equation~\ref{eqn:wmc}
%% with respect to the weights assigned to one or more variable.
%% Most applications of this computation have probabilities for the
%% weights, where variable $x$ has probability $P(x)$ of being assigned $1$, and
%% so the weighted model count of a formula gives the probability that it will
%% evaluate to $1$.  We therefore expect this application to have numeric
%% values similar to those encountered with the $\textsf{Uniform$\pm$}$
%% collection of weight assignments, and therefore evaluations based on interval computations will work well.



\newpage
\bibliography{references}

\newpage
\appendix

\section{Analysis of Evaluation Performance}
\label{app:performance}

\begin{figure}[t]
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.45,0.98)}},
      legend cell align={left},
                              xmode=log,xmin=0.001, xmax=10000.0,
                              xtick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              xticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              ymode=log, ymin=0.001, ymax=10000.0,
                              ytick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              yticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              xlabel={MPQ Runtime (secs.)}, ylabel={Hybrid Runtime (secs.)}
            ]

    \input{data-formatted/combo-mpf+mpq}
    \input{data-formatted/combo-mpfi+mpq}
    \input{data-formatted/combo-mpfi2+mpq}
    \input{data-formatted/combo-mpq+mpq}
    \legend{
      \scriptsize \textsf{MC2024, Hybrid-MPF},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-128},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-256},
      \scriptsize \textsf{MC2024, Hybrid-MPQ}
    }
    \addplot[mark=none] coordinates{(0.001,0.001) (10000.0,10000.0)};
    \node[left] at (axis cs: 11000, 1400) {$1.0\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.01, 0.001) (10000.0, 1000.0)};
    \node[left] at (axis cs: 11000, 140) {$0.1\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.1, 0.001)  (10000.0, 100.0)};
    \node[left] at (axis cs: 11000, 10) {$0.01\times$};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Runtime for hybrid method vs.~for MPQ.  When the value can be computed with MPF or MPFI, the runtime is significantly better than using MPQ.  When it must resort to MPQ, it incurs some overhead.} 
\label{fig:runtime}
\end{figure}

Figure~\ref{fig:runtime} compares the runtimes (Y-axis) for the hybrid
strategy versus that for performing an evaluation using rational
arithmetic (X-axis) for the 2450 instances that succeeded in the
latter case.  These are categorized by the method by which the hybrid
method completed.  The diagonal lines show the relative time for the
hybrid approach versus rational arithmetic.

Of the 2450 instances, the 980 with nonnegative weights could be evaluated using MPF\@.  Many of these also had very small runtimes, even for MPQ\@.
Considering just the 670 instances for which MPQ required more than $1.0$ seconds, we find that MPF ran between $4.8$ and $112.9$ times faster than MPQ, with an average of $23.3$ and a median of $18.6$.  This shows a clear performance benefit in using MPF when all weights are nonnegative.

Of the 1470 instances containing negative weights, 1196 ($81.4\%$)
were successfully evaluated using MPFI with $p=128$.  Considering the
825 instances for which MPQ required more than $1.0$ seconds, we find
that MPFI ran between $2.0$ and $7.1$ times faster, with an average of
$8.8$ and a median of $7.1$.  Again, this level of evaluation had a
clear benefit for performance.  An additional 164 instances
($11.1\%$) were successfully evaluated using MPFI with $p=256$.  Of
the 96 instances for which MPQ required more than $1.0$ seconds, we
find that the combined time for two runs with MPFI was, in the worst
case, $1.04$ times higher than just running MPQ\@.  Overall, however,
the combination ran faster by as much as $9.8\times$, with an average
of $3.1$ and a median of $2.6$.  Again, using this level of evaluation
proved worthwhile.  Finally, 110 instances ($7.5\%$) required an
evaluation using MPQ\@.  In these cases, the hybrid runtime was
greater than that for MPQ alone, since the program also performed two
evaluations using MPFI\@.  Of the 82 instances for which MPQ required
more than $1.0$ seconds, we find that the hybrid approach ran between
$1.1$ and $1.7$ times slower, with an average and a median of $1.2$.
Fortunately, this performance penalty was more than offset by the gains achieved by the less costly evaluation methods.



\end{document}

Comparisons:
\begin{center}
  \begin{tabular}{llr}
    \toprule
    $M_1$ & $M_2$ & Ratio \\
    \midrule
    MPQ & MPF & \avgMpqMpf \\
    MPF & Hybrid & \avgMpfHybrid \\    
    MPQ & Hybrid & \avgMpqHybrid \\
    \bottomrule
  \end{tabular}
\end{center}


%%
%% End of file
