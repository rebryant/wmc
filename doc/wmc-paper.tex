\documentclass[letterpaper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xnewcommand}

%\newtheorem{proposition}{Proposition}

\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lit}{\ell}

\newcommand{\progname}[1]{\textsc{#1}}
\newcommand{\dfour}{\progname{D4}}
\newcommand{\Dfour}{\progname{D4}}

\newcommand{\approximate}[1]{\hat{#1}}
\newcommand{\approxP}{\approximate{P}}
\newcommand{\approxx}{\approximate{x}}
\newcommand{\approxy}{\approximate{y}}
\newcommand{\approxw}{\approximate{w}}
\newcommand{\approxW}{\approximate{W}}
\newcommand{\approxs}{\approximate{s}}
\newcommand{\round}{\mathit{Round}}
\newcommand{\aerror}{\delta}
\newcommand{\digitprecision}{\Delta}
\newcommand{\minvalue}{\omega}
\newcommand{\maxValue}{\Omega}
\newcommand{\roundepsilon}{\varepsilon}
\newcommand{\xmin}{x_{\textrm{min}}}
\newcommand{\xmax}{x_{\textrm{max}}}

\newcommand{\varset}{X}
\newcommand{\dependencyset}{{\cal V}}
\newcommand{\assign}{\alpha}
\newcommand{\modelset}{{\cal M}}
\newcommand{\entails}{\models}


\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Numeric Considerations in Weighted Model Counting}

\titlerunning{Arithmetic Considerations for Weighted Model Counting}

\author{Randal E. Bryant}{Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA}{Randy.Bryant@cs.cmu.edu}{https://orcid.org/0000-0001-5024-6613}{}

\authorrunning{R. E. Bryant} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Randal E. Bryant} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Weighted model counting computes the sum of the rational-valued weights
  associated with the satisfying assignments for a Boolean formula,
  where the weight of an assignment is given by the product of the
  weights associated with the positive and negated variables
  comprising the assignment.  Weighted model counting finds
  applications in a wide variety of domains including machine learning
  and quantitative risk assignment.

  Most weighted model counting programs avoid enumerating all
  assignments to a formula by converting it, either explicitly or
  implicity, into a set of conjunction and disjunction operations,
  such that the count can be evaluating using multiplication for
  conjunctions and addition for disjunctions.

  Although computing these operations with rational arithmetic
  yields exact results, the precision required for most applications
  does not warrant the required space and time.  On
  the other hand, the precision when using floating-point arithmetic
  cannot be guaranteed, especially when some of the weights are
  negative.

  In this paper, we prove that, when all weights are nonnegative, the
  precision of floating-point arithmetic will degrade, in the worst case, as the logarithm
  of the number of variables.  This result, along with our
  experiments, demonstrate that a high level of precision can be
  achieved using a software floating-point implementation.  For
  formulas with negative weights, we show that interval floating-point
  arithmetic can reliably detect cases where a desired precision
  cannot be achieved.  Using a program that combines floating-point,
  interval, and rational arithmetic, we achieve the twin goals of
  efficiency and guaranteed precision.
\end{abstract}

\section{Introduction}

Model counting extends traditional Boolean satisfiability (SAT) solving by
asking not just whether a formula can be satisfied, but to compute the
number of satisfying assignments~\cite{gomes:hs:2009}.  Model counting is a challenging
problem---more challenging than the already NP-hard Boolean
satisfiability.  Several tractable variants of Boolean satisfiability,
including 2-SAT, become intractable when the goal is to count models
and not just determine satisfiability \cite{valiant:siam:1979}.

Weighted model counting extends standard model counting by having
real-valued weights associated with the assignments, and then
computing the sum of the weights of the satisfying assignments.  The
most common variant of weighted model counting to have weights $w(x)$ and $w(\obar{x})$
associated with each variable $x$ and its negation $\obar{x}$.  The
weight of an assignment is then the product of the weights associated
with the positive and negated variables comprising the assignment.
Standard model counting is then just a special case of weighted model
counting with all variables and their complements having unit weights.

Weighted model counting has applications in a variety of domains,
including probabilistic inference~\cite{chavira:ai:2008}, Bayesian
inference~\cite{sang:aaai:2005}, and probabilistic
planning~\cite{domshlak:jair:2007}.  In addition, many of the
applications for binary decision diagrams (BDDs)s~\cite{knuth:bdd:2011}
are, in fact, applications of weighted model counting based on BDD
representations of Boolean formulas.

Despite the intractability, a variety of weighted model counting
programs have been developed that work well in practice.  They
generally fall into two categories. \emph{Top-down} programs
recursively branch on the variables of a formula, performing unit
propagation and conflict analysis similar to a CDCL SAT solver.  Most
of these programs are, in fact, \emph{knowledge compilers} that
generate a Boolean formula in a restricted form that enables efficient
weighted
evaluation~\cite{darwiche:aaai:2002,darwiche:ecai:2004,lagniez:ijcai:2017,muise:cai:2012,oztok:cp:2014}.
Others are based on \emph{bottom-up} approaches, especially using
multi-terminal BDDs~\cite{dudek:aaai:2020,dudek:sat:2021}.  In both
cases, the strategy is to convert the formula into a form for which
weighted model counting becomes tractable.

Weighted model counting can be computationally intensive.  In
experiemental results described in this paper, some evaluations can
require over one billion multiplication and addition operations.
Furthermore, the computed values are often either too small or too
large in magnitude to encode with standard floating-point
representations.  We also want the computed results to provide some
guaranteed level of precision.  Unfortunately, the rounding errors
introduced by standard floating-point computations can lead to results
that bear little relation to the actual values.  In general, even when the results are accurate,
floating-point evaluation does not guarantee any level of precision.

One way to guarantee high precision is to perform all of the
computations using a rational-arithmetic software package, such as the
MPQ libray within the GNU Multiprecision Arithmetic
Library (GMP)~\cite{granlund:gmp:2015}.  It represents each number $x$ as a
pair of integers $p$ and $q$ with $x = p/q$.  It dynamically allocates
the memory for these two integers such that the only limitation on
their sizes is the available memory.  Assuming all of the weights
associated with the variables and their complements are rational, MPQ
can compute the exact rational values of all multiplication and
addition operations, yielding an exact weighted count.  Unfortunately,
both the time and space for manipulating these numbers can be very
large.  In this paper, we report examples requiring over one gigabyte
to store the arguments and result of a single addition operation.

Indeed, rational arithmetic provides more precision than is required
for most applications.  It would be preferable to have floating-point
representation of the weighted count, with a guarantee that this
representation meets some standard of precision with respect to the
actual count.

In this paper, we apply two strategies to providing a
guaranteed-precision weighted model count.  First, we consider the
case where all weights $w(x)$ and $w(\obar{x})$ are nonnegative, and
where the values are computed over a decision-DNNF Boolean
formula~\cite{beame:uai:2013,huang:jair:2007}.  For many applications
of weighted model counting, the weights are probabilities between
$0.0$ and $1.0$; negative weights are never encountered.
Furthermore most top-down and bottom-up weighted model
counters either explicitly or implicitly operate on decision-DNNF
representations.
We prove under these restrictions
that the degradation of
precision caused by rounding errors will be bounded by the logarithm
of the number of variables in the formula.  In practical terms, this
implies that floating-point arithmetic can be fully trusted in these
cases.  For example, by using the MPF software floating-point library
within GMP with a 128-bit fraction, we can guarantee that the computed
count for a formula with up to ten million variables will be correct within
30 decimal digits of precision.

For formulas with negative weights, we demonstrate a family of
formulas where no bounded-precision numeric representation will
suffice.  Rational arithmetic provides the only option in such cases.
On the other hand, we show experimentally that floating-point
arithmetic suffices for standard ways to generate weight assignments,
but there is no way to be certain of the precision.  We also
demonstrate a strategy for generating random weight assignments that
often causes floating-point arithmetic to yield imprecise results.

We address the lack of certainty in a standard floating-point
evaluation by introducing \emph{interval} floating-point arithmetic
using the MPFI software library~\cite{revol:rc:2005}. With this
library, values are approximated as intervals of the form $[\xmin,  \xmax]$ such that the true value lies within the interval, and both
$\xmin$ and $\xmax$ are represented in floating-point.  The result of
every operation is an interval that is guaranteed to include the true
result value, as long as the argument intervals include their true
values.  We show experimentally that the intervals maintained during
the computations of weighted model counting are generally tight enough
that they provide useful guarantees of precision.

Putting these together, we present experimental results from a program
that performs weighted model counting of a decision-DNNF formula
generated by the D4 knowledge compiler~\cite{lagniez:ijcai:2017} using
a hybrid strategy.  When all weights are nonnegative, it uses MPF to
perform floating-point computations, relying on our precision
guarantee.  When some weights are negative, it uses MPFI to perform
interval arithmetic, first with a 128-bit fraction, and then with a
256-bit fraction if the interval analysis does not indicate sufficient
precision.  If this second level fails, it resorts to rational
arithmetic using the MPQ library.  The overall effect is to achieve
the twin goals of efficiency and guaranteed precision.

The paper is organized as follows.  In
Sections~\ref{sect:background:boolean} and
\ref{sect:background:numbers}, we cover background material in Boolean
formulas, weighted model counting, numeric error, and number
representations.  In Section~\ref{sect:nonneg} we present our
theoretical result concerning the case of nonnegative weights.  In
Section~\ref{sect:neg} we describe the challenges that negative
weights can present, but also experimental results showing that
floating-point arithmetic suffices in many cases.
Section~\ref{sect:hybrid} then describes the use of interval
arithmetic and our hybrid approach.  Section~\ref{sect:techniques}
presents some subtleties of using rational arithmetic that we learned
in the course of this work.  Finally, Section~\ref{sect:conclusion}
presents some concluding remarks and ideas for possible areas for
further research.


\section{Boolean Formulas and Weighted Model Counting}
\label{sect:background:boolean}

We consider Boolean formulas over a set of variables $\varset$ in
\emph{negation normal form}, where negation can only be applied to the
variables.  We refer to a variable $x$ or its negation $\obar{x}$ as a
\emph{literal}.  We use the symbol $\lit$ to indicate an arbitrary
literal.  The set of all formulas is defined recursively to consist of
literals, \emph{conjunctions} of the form $\phi_1 \land \phi_2$, and
\emph{disjunctions} of the form $\phi_1 \lor \phi_2$.  The set of
variables occuring in formula $\phi$ is denoted
$\dependencyset(\phi)$.  Typically, a formula is represented as a
directed acyclic graph, allowing a sharing of subformulas.  We
therefore define the \emph{size} of a formula to be the number of
unique subformulas.

A (total) assignment is a mapping $\assign \colon \varset \rightarrow
\{0, 1\}$.  An assignment $\assign$ is said to be a \emph{model} of
formula $\phi$ if the formula evaluates to $1$ under that assignment.
The set of models of a formula $\phi$ is written $\modelset(\phi)$.
We can also consider an assignment to be a set of literals, where $x \in \assign$
when $\assign(x) = 1$, and $\obar{x} \in \assign$ when
$\assign(x) = 0$, for each variable $x \in \varset$.

\emph{Weighted model counting} is defined in terms of a \emph{weight
  assignment} $w$, associating rational values $w(x)$ and
$w(\obar{x})$ for each variable $x \in \varset$.
The weight of an
assignment $\assign$ is then defined to be the product of its literal weights, and the weight
of a formula is the sum of the weights of its satisfying assignments:
\begin{eqnarray}
  w(\phi) & = & \sum_{\assign \in \modelset(\phi)} \prod_{\lit \in \assign} w(\lit) \label{eqn:model:count}
\end{eqnarray}

Computing the weighted count of an arbitrary formula is thought to be intractable.  However, it becomes
feasible when the formula is the in deterministic decomposable negation-normal form (d-DNNF):
\begin{enumerate}
\item The formula is in negation-normal form.  
\item All conjunctions are \emph{decomposable}.  That is, every subformula of the form $\phi_1 \land \phi_2$
  satisfies $\dependencyset(\phi_1) \cap \dependencyset(\phi_2) = \emptyset$.
\item All disjunctions are \emph{deterministic}.  That is, every subformula of the form $\phi_1 \lor \phi_2$ satisfies
  $\modelset(\phi_1) \cap \modelset(\phi_2) = \emptyset$.
\end{enumerate}
As an important subclass of d-DNNF, a formula is said to be in 
decision decomposable negation-normal form (decision-DNNF) when every occurrence of a disjunction has the form 
$(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ for some decision variable $x$.

There are several ways to compute the weighted count of d-DNNF formula $\phi$, all
based on an \emph{algebraic evaluation} of $\phi$ to compute a value $W(\phi)$:
\begin{enumerate}
\item Each literal $\lit$ is a assigned a rational value $W(\lit)$.
\item Each subformula $\phi_1 \land \phi_2$ is evaluated as $W(\phi_1 \land \phi_2) = W(\phi_1) \cdot W(\phi_2)$.
\item Each subformula $\phi_1 \lor \phi_2$ is evaluated as $W(\phi_1 \lor \phi_2) = W(\phi_1) + W(\phi_2)$.
\end{enumerate}
The number of rational operations in this evaluation is linear in the size of the formula.

There are three ways to use this evaluation to perform weighted model counting for weight assignment $w$:
\begin{enumerate}
\item If the weight assignment for each variable and its complement satisfies $w(x) + w(\obar{x}) = 1$,
  then by letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item Formula $\phi$ is said to be \emph{smooth} if every disjunction $\phi_1 \lor \phi_2$ satisfies
  $\dependencyset(\phi_1) = \dependencyset(\phi_2)$.  For a smooth formula, 
by letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item If the weight assignment for each variable and its complement satisfies $w(x) + w(\obar{x}) \not = 0$,
  we can apply \emph{rescaling}, first computing $s(x) = w(x) + w(\obar{x})$ for each variable $x$
  and letting $W(x) = w(x)/s(x)$ and $W(\obar{x}) = w(\obar{x})/s(x)$.  
  Following the algebraic evaluation $W(\phi)$, the weighted count is computed as:
  \begin{eqnarray}
w(\phi) &=& \prod_{x\in\varset} s(x) \; \cdot \; W(\phi) \label{eqn:rescale}
  \end{eqnarray}
\end{enumerate}

An arbitrary formula can be smoothed by inserting \emph{smoothing terms} of the form $x \lor \obar{x}$.
That is,
  a disjunction $\phi_1 \lor \phi_2$ having some variable $x \in \dependencyset(\phi_1)$ but
  $x \not\in \dependencyset(\phi_2)$ is rewritten as $\phi_1 \lor [(x \lor \obar{x}) \land \phi_2]$.
  
  These three methods can be combined by rescaling some variables and
  inserting smoothing terms for others.  In particular, for any
  variable $x$ having $w(x) + w(\obar{x}) = 0$, we can insert smoothing terms to enable the weighted count to be computed using rescaling.
This case is easily handled, since
inserting a smoothing term for $x$
  will effectively cause the subformula to evaluate to $0$.

\section{Approximations and Number Representations}
\label{sect:background:numbers}

When approximating rational number $x$ with value $\approximate{x}$, we
define the \emph{approximation error} $\aerror(\approxx, x)$ as:
\begin{eqnarray}
\aerror[\approxx, x] & = & \left\{ \begin{array}{ll}
  \frac{|\approxx - x|}{|x|}  & x \not = 0\\[0.8em]
  |\approxx| & x = 0\\
  \end{array} \right. \label{eqn:approx:error}
\end{eqnarray}
That is, we consider relative error when $x$ is nonzero, and absolute error when $x=0$.
This value will equal 0 when $\approxx=x$, and it will be greater for weaker approximations.

We then define the \emph{digit precision} of the approximation as
\begin{eqnarray}
\digitprecision(\approxx, x) & = & \max(0, -\log_{10} \aerror[\approxx, x]) \label{eqn:digitprecision} 
\end{eqnarray}
This value will range from $0$ for a poor approximation, up to $+\infty$ when $\approxx=x$.
Intuitively, it indicates the number of digits in a decimal representation of $\approxx$ that represent the true decimal representation of
$x$


The rational arithmetic library MPQ represents a rational number $x$
as a pair $p$, $q$, such that $p$ and $q$ are relatively prime, and $x
= p/q$.  The space for both $p$ and $q$ is allocated dynamically, and
so their size is limited only by the avaiable memory.  We can
therefore use MPQ to compute an exact representation of a weighted
count from a d-DNNF formula, but the space and time required can be very high.

We consider floating-point numbers of the form
\begin{eqnarray}
x & = & (-1)^s \; \times \; f \; \times 2^{e} \label{eqn:floating-point}
\end{eqnarray}
where:
\begin{itemize}
\item Sign bit $s$ equals $0$ for nonnegative numbers and $1$ for negative numbers
\item Fraction $f$ is encoded as a $p$-bit binary number with an implicit binary point on the left.  That is $0 \leq f \leq 1-2^{-p}$.
\item Exponent $e$ is an integer, possibly with some limitation on its range.
\end{itemize}
As examples, consider two different floating-point formats:
\begin{itemize}
\item The IEEE~754 double-precision format uses a slightly different representation, but it maps to the notation of Equation~\ref{eqn:floating-point} with $p=53$ and an exponent range of
  $-1021 \leq e \leq 1024$.  Unfortunately, the small exponent range limits the suitability of
  this representation for weighted model counting.
\item The MPF software floating-point library allows the value of $p$ to be set arbitrarily.
Our default configuration has
$p=128$. On most 64-bit architectures, it represents
  the exponent as a 64-bit signed number.  This provides an ample exponent range.
\end{itemize}

When converting a rational number $x$ into a floating-point encoding, its value must be rounded to a
value $\round(x)$.  In doing so, it introduces a \emph{rounding
error}.  
Letting $\roundepsilon = 2^{-p}$, and assuming that
the exponent range suffices to represent $x$,
we can assume that
$\aerror[\round(x), x] \leq \roundepsilon$.
As examples, the double-precision representation has $\roundepsilon = 2^{-53} \approx 1.11 \times 10^{-16}$, while the 128-bit MPF format
has $\roundepsilon = 2^{-128} \approx 2.94 \times 10^{-39}$.  As a consequence, the lower bound of $\digitprecision(\round(x), x)$ is around $15.95$ and $38.53$, respectively.

Floating-point arithmetic is implemented in such a way that any
operation effectively computes an exact result and then rounds it to
encode the result as a floating-point value.  The maximum error from a sequence of operations
therefore tends to accumulate in multiples of $\roundepsilon$.
This yields error bounds of the form $\aerror[\approxx, x] \leq t\,\roundepsilon$,
which we  refer to as having at most $t$ units of
rounding error.

As a third representation, the MPFI interval arithmetic library
represent a contiguous ranges of numbers $[\xmin, \xmax]$, where
$\xmin$ and $\xmax$ are floating-point numbers.  The endpoints of the
range are represented using the MPFR package, an extension of MPF to
provide more control over rounding.  Arithmetic on intervals is performed
in such a way that the result of any operation is an interval that
spans the range of possible values of that operation applied to any
combination of values spanned by the argument intervals.  The interval representation of $[\xmin, \xmax]$ is converted to
a single value as
the floating-point number closest to $(\xmin + \xmax)/2$.

For interval $[\xmin, \xmax]$, we define the interval approximation error $\aerror([\xmin, \xmax])$ as
\begin{eqnarray}
\aerror([\xmin, \xmax]) & = & \left\{ \begin{array}{ll}
  \frac{|\xmax - \xmin|}{\min(|\xmin|, |\xmax|)}  & 0 \not \in [\xmin, \xmax]\\[0.8em]
  |\xmax - \xmin| &  0 \in [\xmin, \xmax]\\
  \end{array} \right. \label{eqn:interval:error}
\end{eqnarray}
For any values $\approxx, x \in [\xmin, \xmax]$, we can see that
$\aerror[\approxx, x] \leq \aerror(([\xmin, \xmax])$.
We then define the digit precision of the interval as:
\begin{eqnarray}
\digitprecision([\xmin, \xmax]) & = & \max(0, -\log_{10} \aerror([\xmin, \xmax]) \label{eqn:interval:digitprecision} 
\end{eqnarray}
This value will range from $0.0$ for a very large interval to $+\infty$ when the interval is tight with $\xmin = \xmax$.

\section{Error Analysis: Nonnegative Weights}
\label{sect:nonneg}

Here we evaluate how rounding errors accumulate via a series of
arithmetic operations when all arguments are nonnegative.
That is, we assume the exact arguments $x$ and $y$ to each operation satisfy $x \geq 0$ and $y \geq 0$.
Rounding never causes a result to be negated, and therefore, we can
consider the approximations $\approxx$ of  $x$ and $\approxy$ of $y$ to satisfy $\approxx \geq 0$ and $\approxy \geq 0$.
In addition, none of the operations multiplication, addition, or division yield negative results when their arguments are nonnegative.
We can therefore
assume that all actual and approximate values under consideration will
be nonnegative.

Suppose for nonnegative values $s$ and $t$, we have
$\aerror[\approxx, x] \leq s\, \roundepsilon$ and
$\aerror[\approxy, y] \leq t\, \roundepsilon$, respectively.
Therefore
$0 \leq (1-s\,\roundepsilon)\, x \leq \approxx \leq (1+s\,\roundepsilon)\, x$ and
$0 \leq (1-t\,\roundepsilon)\, y \leq \approxy \leq (1+t\,\roundepsilon)\, y$.


\subsection{Multiplication}

When $\approxx$ and $\approxy$ are multiplied, let us impose the constraint that $s\,t \leq 1/\roundepsilon$.
Their product will satisfy
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2)$, and our additional constraint gives
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t+1)\,\roundepsilon)$.
A similar analysis gives
$(x\cdot y) (1 - (s+t+1)\,\roundepsilon) \leq \approxx \cdot \approxy$, and therefore
$\aerror[\approxx \cdot \approxy, x \cdot y] \leq (s+t+1)\,\roundepsilon$.
Rounding this result can introduce an additional error of at most $\roundepsilon$, and therefore
$\aerror[\round(\approxx \cdot \approxy), x \cdot y] \leq (s+t+2)\,\roundepsilon$.

Thus, for suitable values of $s$, $t$, and $\roundepsilon$,
a multiplication operation, at most, propagates the errors of its arguments as their sum and adds two units of rounding error.

\subsection{Addition}

When $\approxx$ and $\approxy$ are added, their sum will satisfy
$(x + y) (1 - r\,\roundepsilon) \leq \approxx + \approxy \leq (x + y) (1 + r\,\roundepsilon)$, where $r = \frac{sx + ty}{x+y}$.  That is, the resulting error $r$ will bounded by a weighted average
of those of its arguments.  For all values of $x$ and $y$, $r$ cannot exceed the maximum of $s$ and $t$.
Rounding the sum can add at most one unit of rounding error, and so we have
$\aerror[\round(\approxx + \approxy), x + y] \leq (\max(s,t)+1)\,\roundepsilon$.

Thus, an addition operation, at most, propagates the maximum error of its arguments and adds a unit of rounding error.

\subsection{Evaluating a decision-DNNF formula}
\label{sect:error:formula}

Suppose we use floating-point arithmetic to compute the sums and
products in an algebraic evaluation of a decision-DNNF formula $\phi$.
We assume that the value $W(\lit)$ for each literal $\lit$ is
represented by a floating-point number $\approxW(\lit)$ such that
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$.  In practice,
this implies that rescaling must use rational arithmetic to
compute exact representations of $s(x)$, $w(x)/s(x)$, and
$w(\obar{x})/s(x)$ for each variable $x$, so that only one unit of
rounding error is introduced when representing each value $W(\lit)$.
We also assume $W(\lit) \geq 0$.
We can then bound the error of the computed value $W(\phi)$ as follows:
\begin{lemma}
  The algebraic evaluation of a decision-DNNF formula $\phi$ having $|\dependencyset(\phi)| = n$, with $n \leq 1/(2\sqrt{\roundepsilon})$ using floating-point arithmetic will yield an approximation $\approxW(\phi)$ satisfying
  $\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.
  \label{lemma:approx:pos}
\end{lemma}

The proof of this lemma proceeds by induction on the structure of $\phi$:
\begin{enumerate}
\item For literal $\lit$ with weight $W(\lit)$, its approximation $\approxW(\lit)$ satisfies
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$, which is within the error bound of $(4n-2)\,\roundepsilon$ for $n=1$.
\item For formula $\phi$ of the form $\phi_1 \land \phi_2$, there must be some $k$, with $1 \leq k < n$, such that $|\dependencyset(\phi_1)| = k$
  and $|\dependencyset(\phi_2)| = n-k$.
\begin{enumerate}
  \item Let us first test whether the conditions for the induction hypothesis hold: for $s = 4k-2$ and $t = 4(n-k)-2$
    we require that $s\,t \leq 1/\roundepsilon$.  We can see that $s\,t \leq 16\,k\,(n-k)$. This quantity will be maximized when $k = n/2$,
    and therefore $s \, t \leq 4\,n^2$.
    Given our limit on $n$ with respect to $\roundepsilon$, we have $s \, t \leq 1/\roundepsilon$.
  \item
We can therefore assume by induction that 
 $\aerror[\approxW(\phi_1), W(\phi_1)] \leq (4 k-2) \,\roundepsilon$
  and $\aerror[\approxW(\phi_2), W(\phi_2)] \leq (4 (n-k)-2) \,\roundepsilon$.  Their product, after rounding
  will satisfy 
$\aerror[\approxW(\phi_1 \land \phi_2), W(\phi_1 \land \phi_2)] \leq [(4 k -2) + (4 (n-k) -2) + 2]\,\roundepsilon = (4n-2) \,\roundepsilon$.
\end{enumerate}
\item For a sum of the form
  $\phi = (x \land \phi_1) \lor (\obar{x} \land \phi_2)$, let us use the notation $\lit_1 = x$ and $\lit_2 = \obar{x}$
  and consider the two subformulas $\lit_i \land \phi_i$ for $i \in \{1,2\}$.
  Since all products are decomposable, we must have $x \not \in \dependencyset(\phi_i)$,
  and therefore $|\dependencyset(\phi_i)| = n-1$.
  By induction, we can therefore assume that
  $\aerror[\approxW(\phi_i), W(\phi_i)] \leq (4(n-1)-2) \,\roundepsilon = (4n-6)\,\roundepsilon$.  Rounding the literal weights will yield
  $\aerror[\approxW(\lit_i), W(\lit_i)] \leq \roundepsilon$.  Let $y_i$ denote the product $W(\lit_i) \cdot W(\phi_i)$ for $i \in \{1,2\}$.
  Its rounded value will satisfy
  $\aerror[\approxy_i, y_i] \leq (4n-3) \,\roundepsilon$.  Summing $\approxy_1$ and $\approxy_2$ and rounding the result will therefore give
  an approximation $\approxW(\phi)$ to $W(\phi) = y_1 + y_2$ with
$\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.  
\end{enumerate}

Observe that this proof relies on the decomposability of the
conjunctions to bound the error induced by multiplication operations.
It relies on the decision structure of the formula only to bound the
depth of the additions.

In the event of rescaling, we must also consider the error introduced
when computing the product $P = \prod_{x\in\varset} s(x)$.  We assume that
each term $s(x)$ is represented by a floating-point value
$\approxs(x)$ such that $\aerror[\approxs(x), s(x)] \leq
\roundepsilon$.  In practice, this requires using rational arithmetic
to represent $w(x)$ and $w(\obar{x})$ and to compute their sum.  The
only error introduced will then be when converting the sum into
a floating-point representation.

We can then bound the error of the product as
\begin{lemma}
  The computation of the product $P = \prod_{x\in\varset} s(x)$ having
$|\varset| = n$, with $n \leq 1/(2\sqrt{\roundepsilon})$ using floating-point arithmetic will yield an approximation $\approxP$ satisfying
  $\aerror[\approxP, P] \leq (3n-2)\,\roundepsilon$.
  \label{lemma:approx:product}
\end{lemma}

The proof of this lemma proceeds much like that for Lemma~\ref{lemma:approx:pos}.  We assume an arbitrary association of the subproducts, and so $P$ can be computed as
$P_1 \cdot P_2$, where $P_1$ is the product of $k$ elements and $P_2$ is the product of $n-k$ elements, with $1 < k < n$.
The smaller coefficient of $3$ arises due to the lack of any addition operations.

Combining these two results, we can state the following result about weighted model counting when all weights are nonnegative:
\begin{theorem}
  \label{thm:approx:pos}
Computing the weighted model count of a
a decision-DNNF formula $\phi$ having $|\dependencyset(\phi)| = n$, using floating-point arithmetic with a $p$-bit fraction, such that $\log_2 n \leq p/2-1$
will yield an approximation $\approxw(\phi)$ to the true weighted count $w(\phi)$, such that
\begin{eqnarray}
\digitprecision(\approxw(\phi), w(\phi)) & \geq & p\,\log_{10} 2 - c - \log_{10} n \label{eqn:precision:wmc}
\end{eqnarray}
where $c = \log_{10} 7$ when rescaling is required and $c = \log_{10} 4$ when no rescaling is required
\end{theorem}

Let us examine the practical implications of this theorem.  In particular, assume we use the MPF library with $p=128$.
Only a single formula from the 2024 weighted model competition had over one million variables, and so we can assume a
bound of $n \leq 10^7$.  The conditions for Equation~\ref{eqn:precision:wmc} to hold will readily be satisfied, since
$\log_2 n \leq 23.3$ and $p/2-1 = 63$.  We will therefore have 
$\digitprecision(\approxw(\phi), w(\phi)) \geq 38.53 - 0.85 - 7 = 30.68$.  That is, even though the floating-point format provides only around 38 digits of precision for a single rounding,
computing the weighted model count of a decision-DNNF formula over 10 million variables, and using rescaling, is still guaranteed to yield a result with 30 digits of precision.

In scaling to larger formulas, each time
the number of variables is increased by a factor of 10, at most one
digit of precision is lost.  For example, we could scale to $n=10^{12}$, while keeping $p=128$, and still have over 25 digits of precision.  That is over nine digits greater than the precision
provided by the IEEE double-precision representation.
Importantly, this bound holds regardless of the formula size, as well as for any weight assignment, 
as long as all weights are nonnegative.

\subsection{Allowing non-decision disjunctions}

Our previous requirement that each disjunction have a decision
variable had the effect of limiting the depth of the sum operations in
a formula to $n$.  Without this requirement, the depth can become
exponential.  For example, consider a d-DNNF formula encoding a
tautology over $n$ variables, constructed as follows.  For each of the
$2^n$ possible assignment $\assign$, we use conjunction operations to
encode $\phi_{\assign} = \bigwedge_{\lit \in \assign} \lit$.  We then
use a sequence of $2^n-1$ disjunctions to combine these products to
form $\phi$.  The accumulated error from the disjunction operations could grow
exponentially with $n$.  Of course, such a formula is completely
impractical for even modest values of $n$, but this construct
demonstrates that, without further restriction on the structure of a
d-DNNF formula, we cannot guarantee a reasonable bound on the error incurred by an algebraic evaluation.

The only
cases we know where non-decision conjunctions occur in d-DNNF formulas is with projected model counting, where
instances of a decision variable or its complement are removed by projection.
In such cases, the bound of Equation~\ref{eqn:precision:wmc} holds,
but we must include all variables, included those projected away, in the
variable count $n$.

\subsection{Experimental Validation}

\begin{figure}
\centering{%
\begin{tikzpicture}
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10}, 
      legend style={at={(0.30,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/upos-mpf+vars}
    \input{data-formatted/epos-mpf+vars}
    \input{data-formatted/prodp-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$+$},
      \scriptsize \textsf{MC2024, Exponential$+$},
      \scriptsize \textsf{Optimized Product}
    }
    \addplot[mark=none] coordinates{(1,36.69) (1e7,30.69)};
    \node[right] at (axis cs: 2, 34.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Nonnegative Weights.  The precision is guaranteed to be greater than the bound.
We set as a target to have digit precisions of at least 30.0.}
\label{fig:pos:mpf}
\end{figure}

To experimentally test the bound of Equation~\ref{eqn:precision:wmc},
we evaluated 200 benchmark formulas from public and private portions
of the 2024 Weighted Model Counting Competition.  We attempted to
convert these into decision-DNNF using version 2 of the the D4
knowledge compiler.  We were able to compile 100 of them with a time
limit on 3600 minutes on a machine with 64~GB of random-access memory.
We define a problem \emph{instance} to be a combination of a formula
plus a weight assignment for all of its literals.

For each of these formulas, we then generated two different collections of instances, each with five weight assignments:
\begin{itemize}
\item \textsf{Uniform$+$}: For each variable $x$, weight $w(x)$ is represented by a 9-digit decimal number selected uniformly in the range
  $[10^{-9},\,1-10^{-9}]$. The weight for $\obar{x}$ is then set to
  $w(\obar{x}) = 1-w(x)$.  Such a weight assignment is typical of those used in recent weighted model counting competitions.
\item \textsf{Exponential$+$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$
  are drawn independently from an exponential distribution in the range
  $[10^{-9},\,10^{+9})$.  Each weight is represented by a decimal number with at most 9 digits to the left of the decimal point, and at most 9 digits to the right.
\end{itemize}
For each instance, we evaluated
the weighted count using MPF to get an approximate weight $\approxw$ and
using MPQ to get an exact weight $w$.  We then evaluated the digit precision according to Equation~\ref{eqn:digitprecision}.   Our implementation used rescaling for all variables
with $w(x) + w(\obar{x}) \not \in \{0, 1\}$.
Although not required for the instances used in this evaluation,
it will insert smoothing terms when $w(x) + w(\obar{x}) = 0$ for variable $x$.

In addition, we evaluated formulas of the form $\bigwedge_{1\leq i
  \leq 10^d} x_i$ for values of $d$ ranging from $0$ to $6$ using a
single weight for every variable.  We swept a parameter space of
weights of the form $1 + k\cdot 10^{-9}$ for $1 \leq k \leq 100$ and
chose the value of $k$ that minimized the digit precision.  We refer
to this as the \textsf{Optimized Product} collection.

Figure~\ref{fig:pos:mpf} shows the results of these evaluations for
the three collections.  For the two collections of weight assignments for each 2024 competition formula,
we show only the minimum precision for each of the five randomly
generated  weight assignments.  Each data point
represents one combination of formula and weight selection method and is placed
along the X-axis according to the number of variables
and on the Y-axis according to the computed digit precision.
The plot also shows the precision bound of Equation~\ref{eqn:precision:wmc} for $c=\log_{10} 7$.

As can
be expected, all data points stay above the precision bound.  Indeed,
most exceed the bound by several decimal digits.  Our bound assumes
that rounding either consistently decreases or consistently
increases each computed result by a single unit of rounding error.  In
practice, rounding goes in both directions, and
therefore the computed results stay closer to the true value.

The optimized products, on the other hand, demonstrate that particular
combinations of formula and weight assignment can come within 2 decimal digits of the
precision bound and to track its general trend.

\section{Error Analysis: Negative Weights}
\label{sect:neg}

The analysis of Section~\ref{sect:nonneg} no longer holds when
some literals have negative weights.  With a floating-point
representation, summing combinations of negative and nonnegative
values can cause \emph{cancellation}, where arbitary levels of
precision are lost.  Consider, for example, the computation
$s + T - T$, where $s$ and $T$ are nonnegative, but $s \ll T$.  Using
bounded-precision arithmetic, evaluating the sum as $s + (T - T)$ will yield a value
that is very close to $s$.
Evaluating it as $(s + T) - T$, however, can yield $0$ or some other value that bears little relation to $s$.

Cancellation can arise when evaluating decision-DNNF formulas to such a degree that no bounded precision $p$ that grows sublinearly with $n$ will suffice.
As an example, consider the following smooth, decision-DNNF formula over $n+1$ variables:
\begin{eqnarray}
\phi_n  & = & z \land \left[\bigwedge_{i = 1}^{n} \obar{x}_i \; \lor \; \bigwedge_{i = 1}^{n} x_i\right] \quad \lor \quad \obar{z} \land \left [\bigwedge_{i = 1}^{n} x_i\right] \label{eqn:max:precision}
\end{eqnarray}
with a weight assignment having only a single literal assigned a negative weight:
\begin{displaymath}
\begin{array}{llll}
  w(z) & = & +1 \\
  w(\obar{z}) & = & -1 \\
  w(x_i) & = & 10^{+9} & 1 \leq i \leq n \\
  w(\obar{x}_i) & = & 10^{-9} & 1 \leq i \leq n \\
\end{array}
\end{displaymath}
Computing $w(\phi_n)$  evaluates the sum $(s + T) - T$, where
$s = 10^{-9n}$ and $T = 10^{+9n}$.  Avoiding cancellation requires using a floating-point representation with a fraction of at least
$p = n \cdot 18 \cdot \log_2 10 \approx 59.79 \, n$ bits.
On the other hand, using MPQ, $w(\phi_{10^7})$ can be computed exactly in around 35 seconds, even though the final step requires a total of 1.87~gigabytes to store the arguments
$s+T$ and $-T$, and the result $s$.


\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10},
      legend style={at={(0.32,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/uposneg-mpf+vars}
    \input{data-formatted/eposneg-mpf+vars}
    \input{data-formatted/bposneg-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$\pm$},
      \scriptsize \textsf{MC2024, Exponential$\pm$},
      \scriptsize \textsf{MC2024, Limits$\pm$}
    }
    \addplot[mark=none, dashed] coordinates{(1,36.69) (1e7,30.69)};
    \node[right] at (axis cs: 2, 34.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Negative Weights.  There is no guaranteed bound for precision, but many cases remain above the nonnegative weight bound.
The Limits~$\pm$ weight assignment was designed to maximize precision loss.}
\label{fig:posneg:mpf}
\end{figure}

Interestingly, however, using MPF with a fixed precision of $p=128$ does surprisingly well on many formulas, even in the presence of negative weights.
In Figure~\ref{fig:posneg:mpf}, we see a similar plot to that of Figure~\ref{fig:pos:mpf}.  The first two collections of weight assignments are simple generalizations of those used earlier:
\begin{itemize}
\item \textsf{Uniform$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes drawn independently
from a uniform distribution in the range  $[10^{-9},\,1-10^{-9}]$.  Each is negated with probability $0.5$.
\item \textsf{Exponential$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes
  drawn independently from an exponential distribution in the range $[10^{-9},\,10^{+9})$.  Each is negated with probability $0.5$.
\end{itemize}
As can be see for these plots, the results mostly stay above the precision bound of Equation~\ref{eqn:digitprecision},
even though that bound does not apply.  All stay above the target precision of $30.0$.

On deeper inspection, we can see that setting up the conditions for a
cancellation of the form $(s + T) - T'$, where $T \approx T'$, require
1) a large dynamic range among the computed values to give widely different values $s$ and $T$, and 2) sufficient
homogeneity in the computed values that we can get two values $T$ and
$T'$ such that $T \approx T'$.  A uniform distribution has neither of
these properties.  An exponential distribution has a large dynamic
range, but the computed values tend to be very heterogenous.

To create a weight assignment that has more chance of cancellation, we devised the following asignment:
\begin{itemize}
\item\textsf{Limits$\pm$}:  Each weight $w(x)$ and $w(\obar{x})$ has a magnitude of either $10^{-9}$ or $10^{+9}$, and they are each set negative with probability $0.5$.
  However, we exclude assignments with $w(x) + w(\obar{x}) = 0$.
\end{itemize}
The idea here is to give large dynamic ranges plus a high degree of
homogeneity.  The plots for this assignment in
Figure~\ref{fig:posneg:mpf} demonstrate that it achieves the desired
effect.  Although over one-half of the assignments yield computed
values that remain above the precision bound, many have digit
precision below the target of $30.0$.  We can also see how our choice
of weights leads to two bands of low precision: those where values $s$ and
$T$ are encountered that differ by a factor of around $10^{18}$, and those where they
differ by a factor of around $10^{36}$.

\section{Reliable and Efficient Weight Computation}
\label{sect:hybrid}

Our results with the formula of Equation~\ref{eqn:max:precision} and
with the \textsf{Limits$\pm$} weight assignment show that simply
computing weights with fixed-precision floating-point arithmetic can
yield inaccurate results.  Even worse, such an approach cannot
detect whether the computed result provides any level of accuracy.

One way to achieve accuracy is to perform all computations with
rational arithmetic.  Unfortunately, this can be very time consuming,
and it provides a higher level of accuracy than is required for most
applications.  Furthermore, the memory required can exceed
the resources available.

A second strategy is to use floating point arithmetic for problems
where all weights are nonnegative and rational arithmetic otherwise.
This would provide guaranteed accuracy, but it can still be resource
intensive.  On the other hand, the example of
Equation~\ref{eqn:max:precision} shows us that some problems can only
be solved with rational arithmetic, and  therefore we must be prepared to fall back on this.




\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              xmode=normal,xmin=0, xmax=50,
                              xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              xticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              ymode=normal, ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Estimated Precision}, ylabel={Actual Precision}
            ]

    \draw[ fill={blue!30}, opacity=0.2] (axis cs: 0,30) rectangle (axis cs: 30,50);
    \node at (axis cs:15,40) {Overly pessimistic};
    

    \input{data-formatted/uposneg-mpfi-est+act}
    \input{data-formatted/eposneg-mpfi-est+act}
    \input{data-formatted/bposneg-mpfi-est+act}
    \legend{
      \scriptsize \textsf{MC2024, Uniform $\pm$},
      \scriptsize \textsf{MC2024, Exponential $\pm$},
      \scriptsize \textsf{MC2024, Limits $\pm$}
    }
    \addplot[mark=none] coordinates{(0,0) (50,50)};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Predictive Accuracy of MPFI Interval Arithmetic.  MPFI never has a higher estimate than the actual, but it can
incorrectly predict a precision less than the target of 30.0.}
\label{fig:mpfi}
\end{figure}

\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.45,0.98)}},
      legend cell align={left},
                              xmode=log,xmin=0.001, xmax=10000.0,
                              xtick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              xticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              ymode=log, ymin=0.001, ymax=10000.0,
                              ytick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              yticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              xlabel={MPQ Runtime (secs.)}, ylabel={Hybrid Runtime (secs.)}
            ]

    \input{data-formatted/combo-mpf+mpq}
    \input{data-formatted/combo-mpfi+mpq}
    \input{data-formatted/combo-mpfi2+mpq}
    \input{data-formatted/combo-mpq+mpq}
    \legend{
      \scriptsize \textsf{MC2024, Hybrid-MPF},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-128},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-256},
      \scriptsize \textsf{MC2024, Hybrid-MPQ}
    }
    \addplot[mark=none] coordinates{(0.001,0.001) (10000.0,10000.0)};
    \node[left] at (axis cs: 10000, 1150) {$1.0\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.01, 0.001) (10000.0, 1000.0)};
    \node[left] at (axis cs: 10000, 115) {$0.1\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.1, 0.001)  (10000.0, 100.0)};
    \node[left] at (axis cs: 10000, 10) {$0.01\times$};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Runtime for hybrid method vs.~for MPQ.  When the value can be computed with MPF or MPFI, the runtime is signficantly better than using MPQ.  When it must resort to MPQ, it incurs some overhead.} 
\label{fig:runtime}
\end{figure}

\subsection{Comparing Implementation Strategies}

\begin{table}
  \caption{Performance Comparision of Different Implementation Strategies}
  \label{tab:compare}
  \centering{
  \begin{tabular}{llrrrrrr}
    \toprule
    \multicolumn{1}{c}{Strategy} & & \multicolumn{1}{c}{MPF} & \multicolumn{1}{c}{MPFI-128} & \multicolumn{1}{c}{MPFI-256} & \multicolumn{1}{c}{MPQ} & \multicolumn{1}{c}{Combined} & \multicolumn{1}{c}{Avg (s)}\\
    \midrule
    \input{method_table}
    \\[-1em]
   \bottomrule
  \end{tabular}
  } % Centering
\end{table}

\section{Implementation Techniques}
\label{sect:techniques}

\section{Conclusions}
\label{sect:conclusion}

\newpage
\bibliography{references}

\end{document}

Comparisons:
\begin{center}
  \begin{tabular}{llr}
    \toprule
    $M_1$ & $M_2$ & Ratio \\
    \midrule
    MPQ & MPF & \avgMpqMpf \\
    MPF & Hybrid & \avgMpfHybrid \\    
    MPQ & Hybrid & \avgMpqHybrid \\
    \bottomrule
  \end{tabular}
\end{center}


%%
%% End of file
