\documentclass[letterpaper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xnewcommand}

%\newtheorem{proposition}{Proposition}

\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lit}{\ell}

\newcommand{\progname}[1]{\textsc{#1}}
\newcommand{\dfour}{\progname{D4}}
\newcommand{\Dfour}{\progname{D4}}

\newcommand{\approximate}[1]{\hat{#1}}
\newcommand{\approxP}{\approximate{P}}
\newcommand{\approxx}{\approximate{x}}
\newcommand{\approxy}{\approximate{y}}
\newcommand{\approxv}{\approximate{v}}
\newcommand{\approxV}{\approximate{V}}
\newcommand{\approxw}{\approximate{w}}
\newcommand{\approxW}{\approximate{W}}
\newcommand{\approxs}{\approximate{s}}
\newcommand{\round}{\mathit{Rnd}}
\newcommand{\aerror}{\delta}
\newcommand{\digitprecision}{\Delta}
\newcommand{\minvalue}{\omega}
\newcommand{\maxValue}{\Omega}
\newcommand{\roundepsilon}{\varepsilon}
\newcommand{\xmin}{x^{-}}
\newcommand{\xmax}{x^{+}}
\newcommand{\vmin}{v^{-}}
\newcommand{\vmax}{v^{+}}
\newcommand{\varset}{X}
\newcommand{\dependencyset}{{\cal V}}
\newcommand{\assign}{\alpha}
\newcommand{\modelset}{{\cal M}}
\newcommand{\entails}{\models}


\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Numeric Considerations in Weighted Model Counting}

\titlerunning{Numeric Considerations for Weighted Model Counting}

%%\author{Randal E. Bryant}{Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA}{Randy.Bryant@cs.cmu.edu}{https://orcid.org/0000-0001-5024-6613}{}

%%\authorrunning{R. E. Bryant} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

%%\Copyright{Randal E. Bryant} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\author{Anonymous}{Unknown Institution}{}{}{}
\authorrunning{Anon.}
\Copyright{Anon.}

\ccsdesc[500]{Theory of computation~Automated reasoning}
\keywords{Weighted model counting, floating-point arithmetic, interval arithmetic, decision diagrams}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 Weighted model counting computes the sum of the rational-valued weights
  associated with the satisfying assignments for a Boolean formula,
  where the weight of an assignment is given by the product of the
  weights associated with the positive and negated variables
  comprising the assignment.  Weighted model counting finds
  applications in a wide variety of domains including machine learning
  and quantitative risk assignment.

  Most weighted model counting programs operate by (explicitly or
  implicitly) converting the input formula into a form that enables
  \emph{algebraic} evaluation, using multiplication for conjunctions
  and addition for disjunctions.  Performing this evaluation using
  floating-point arithmetic can yield inaccurate results, and it
  cannot quantify the level of precision achieved.  Computing with
  rational arithmetic gives exact results, but it is costly in both
  time and space.

  We prove that, when all weights are nonnegative, the precision loss of
  algebraic evaluation using floating point increases only
  logarithmically in the number of variables.  We then show experimentally
  that software-based floating-point arithmetic can provide
  a high and guaranteed level of accuracy for this case.  For problems
  with negative weights, we show that a combination of interval
  floating-point arithmetic and rational arithmetic can achieve the
  twin goals of efficiency and guaranteed precision.

  These results extend to the numeric evaluation of discrete functions represented by a variety of decision diagrams.

\end{abstract}

\section{Introduction}

Model counting extends traditional Boolean satisfiability (SAT) solving by
asking not just whether a formula can be satisfied, but to compute the
number of satisfying assignments~\cite{gomes:hs:2009}.  Model counting is a challenging
problem---more challenging than the already NP-hard Boolean
satisfiability~\cite{valiant:siam:1979}.

Weighted model counting extends standard model counting by having
rational-valued weights associated with the assignments, and then
computing the sum of the weights of the satisfying assignments.  The
most common variant has weights $w(x)$ and $w(\obar{x})$
associated with each variable $x$ and its negation $\obar{x}$.  The
weight of an assignment is then the product of the weights associated
with the positive and negated variables comprising the assignment.
Standard model counting can be seen as a special case of weighted model
counting with all variables and their negations having unit weights: $w(x) = w(\obar{x}) = 1$.

Weighted model counting has applications in a variety of domains,
including probabilistic inference~\cite{chavira:ai:2008}, Bayesian
inference~\cite{sang:aaai:2005}, and probabilistic
planning~\cite{domshlak:jair:2007}.  In addition, many of the
applications of decision diagrams (DDs) for
combinatorics~\cite{knuth:bdd:2011}, quantitative risk
assessment~\cite{andrews:ieeetr:2000,groen:ress:2006,xing:wiley:2015,xing:amm:2025},
Bayesian inference~\cite{minato:ijcai:2007},
optimization~\cite{bergman:book:2016}, and
product line modeling~\cite{andersen:jair:2010} are, at their core, applications of
weighted model counting for discrete functions based on decision-diagram representations.

Despite the intractability, a variety of weighted model counting
programs have been developed that work well in practice.  They
generally fall into two categories~\cite{shaw:kr:2024}. \emph{Top-down} programs
recursively branch on the variables of a formula, performing unit
propagation and conflict analysis similar to CDCL SAT solvers.  Most
of these programs operate as \emph{knowledge compilers}, converting
the input Boolean formula into a restricted form that enables efficient
weighted and unweighted counting~\cite{darwiche:aaai:2002,darwiche:ecai:2004,lagniez:ijcai:2017,muise:cai:2012,oztok:cp:2014,sharma:ijcai:2019}.
Others apply \emph{bottom-up} approaches, including ones using
multi-terminal BDDs~\cite{dudek:aaai:2020,dudek:sat:2021}.  In both
cases, the strategy is to convert the formula into a form for which
weighted model counting becomes tractable.

Weighted model counting can be computationally intensive.  In
experimental results described in this paper, some evaluations 
require over one billion arithmetic operations.
Floating-point arithmetic can provide the needed level of performance,
but the computed values are often either too small or too
large in magnitude to encode with standard floating-point
representations.  In addition, the rounding errors
introduced by floating-point computations can lead to results
that bear little relation to the actual values.  In general, even when the results are accurate,
floating-point evaluation cannot quantify the level of precision achieved.

High precision can be guaranteed by performing the
computations with a rational-arithmetic software package~\cite{knuth:rational:1981}, such as the
MPQ library within the GNU Multiprecision Arithmetic
Library (GMP)~\cite{granlund:gmp:2015}.  It represents a rational number $v$ as a
pair of multiprecision integers $p$ and $q$ with $v = p/q$.
%% It dynamically allocates
%% the memory for these two integers such that the only limitation on
%% their sizes is the available memory.  Assuming all of the weights
%% associated with the variables and their negations are rational,
MPQ
can compute the exact rational values of all multiplication and
addition operations, yielding an exact weighted count.  Unfortunately,
both the time and space required for storing and manipulating these numbers can be very
large.
In this paper, for example, we report experiments requiring over one gigabyte
to store the arguments and result of a single addition operation.
For most applications, rational arithmetic provides more precision than is required.
It would be preferable to have a floating-point
representation, but with a high and quantified level of precision.

In this paper, we apply two strategies to compute 
guaranteed-precision weighted model counts.  First, we consider the
case where the weights $w(x)$ and $w(\obar{x})$ for all variables $x$ are nonnegative, and
where the values are computed over a decision-DNNF Boolean
formula~\cite{beame:uai:2013,huang:jair:2007}.
We prove under these restrictions
that the degradation of
precision caused by rounding errors will be bounded by the logarithm
of the number of variables in the formula.  In practical terms, this
implies that floating-point arithmetic can be fully trusted in these
cases.  For example, by using the MPF software floating-point library
within GMP with a 128-bit fraction, we can guarantee that the computed
count for a formula with up to ten million variables will provide at least
30 decimal digits of precision.
Its 64-bit exponent suffices to represent the full range of values in weighted model counting.

This result has broad applicability.
For many applications
of weighted model counting, the weights are probabilities between
$0.0$ and $1.0$, or they are unit weights.  For these applications,  negative weights are never encountered.
Furthermore, most top-down and bottom-up weighted model
counters either explicitly or implicitly operate on decision-DNNF
representations~\cite{beame:uai:2013}.  
Decision diagrams with binary branching structure
also have direct translations into decision-DNNF formulas~\cite{huang:jair:2007,oztok:cp:2014}.

To extend this capability to less restricted classes of Boolean
formulas and to decision diagrams with nonbinary branching
structures~\cite{darwiche:ijcai:2011,srinivasan:iccad:1990}, we
present a method for computing an error bound prior to algebraic
evaluation.  This bound can guide the selection of the floating-point precision to
achieve the desired precision.

For formulas with negative weights, 
we show experimentally that floating-point
arithmetic suffices for the common ways weight assignments are generated in benchmark evaluations.
On the other hand, we
describe a strategy for generating random weight assignments that
often causes floating-point arithmetic to yield erroneous results.
We also
demonstrate a family of
formulas where no bounded-precision numeric representation will
suffice.  Rational arithmetic provides the only option in such cases.


We address the lack of certainty in floating-point
evaluation by introducing \emph{interval} floating-point
arithmetic~\cite{hickey:jacm:2001} using the MPFI software
library~\cite{revol:rc:2005}. With this library, values are
approximated as intervals of the form $[\vmin, \vmax]$ such that the
true value lies within the interval, and both $\vmin$ and $\vmax$ are
represented in floating point.  The result of every operation is an
interval that is guaranteed to include the true result value, as long
as the argument intervals include their true values~\cite{hickey:jacm:2001,muller:hfpa:2018}.
When an interval must be converted to a single value, the floating-point number nearest the midpoint $(\vmin+\vmax)/2$ is chosen.
We show
experimentally that the intervals maintained during the computations
of weighted model counting are generally tight enough to provide
useful precision guarantees.

Putting these together, we present experimental results for a program
that employs a hybrid strategy to compute the weighted count of a decision-DNNF formula
generated by the D4 knowledge compiler~\cite{lagniez:ijcai:2017}.
When all weights are nonnegative, it uses MPF to
perform floating-point computations, relying on our precision
guarantee.  When some weights are negative, it performs up to two
levels of interval computation with MPFI, using increasing levels of
precision.  If these evaluations fail to provide the target precision,
it resorts to rational arithmetic using MPQ\@.  The overall effect
is to achieve the twin goals of efficiency and guaranteed precision.
Although we only present experimental results for D4, similar results will hold for other top-down and bottom-up weighted model counters, as well as
for numeric computations on decision diagrams.

This work is motivated by both application need and technical
opportunity.  On the need side, there is evidence that the standard of
precision for current weighted model counters is low.  In the 2020
weighted model counting competition, a count was considered correct if
it was within $10\%$ of a precomputed result~\cite{fichte:jea:2020}.
That standard has been raised to $0.1\%$ in more recent
years~\cite{hecher:mc:2024}.  Even so, that is a very low standard for
accuracy.  By contrast, our work demonstrates the opportunity to
compute floating-point values that are guaranteed to have over 30 decimal
digits of precision, while generally avoiding the high cost of
rational arithmetic.  To give a sense for this level of precision,
30 digits suffices to represent the distance from
Earth to Alpha Centauri (around 4.37 light years) to the nearest \emph{picometer},
around one one-hundredths the diameter of a hydrogen atom.

Regarding previous work, we are unaware of any 
systematic study of the numeric computations
performed in weighted model counting.  Special properties of
decision-DNNF formulas enable stronger precision guarantees than is
normally the case for floating-point arithmetic.  Some tools provide a
general capability to certify the accuracy of floating-point computations
via interval arithmetic~\cite{becker:fmcad:2016}, but we have not seen an experimental
evaluation of interval or rational arithmetic for
weighted model counting.

Sections~\ref{sect:background:boolean} and
\ref{sect:background:numbers} of this paper cover background material in Boolean
formulas, weighted model counting, numeric error, and number
representations.  Section~\ref{sect:nonneg} covers the case of nonnegative weights,
with our main theoretical result,
a means of computing error bounds for more general formulas and decision diagrams,
and an experimental validation.
Section~\ref{sect:neg} describes the challenges that negative
weights can present, but also experimental results showing that
floating-point arithmetic suffices in many cases.
Section~\ref{sect:reliable} describes the use of interval
arithmetic and our hybrid approach.  
Finally, Section~\ref{sect:conclusion}
presents some concluding remarks.


\section{Boolean Formulas and Weighted Model Counting}
\label{sect:background:boolean}

We consider Boolean formulas over a set of variables $\varset$ in
\emph{negation normal form}, where negation can only be applied to the
variables.  We refer to a variable $x$ or its negation $\obar{x}$ as a
\emph{literal}.  We use the symbol $\lit$ to indicate an arbitrary
literal.  The set of all formulas is defined recursively to consist of
literals, \emph{conjunctions} of the form $\phi_1 \land \phi_2$, and
\emph{disjunctions} of the form $\phi_1 \lor \phi_2$.  The set of
variables occurring in formula $\phi$ is denoted
$\dependencyset(\phi)$.  Typically, a formula is represented as a
directed acyclic graph, allowing a sharing of subformulas.  We
therefore define the \emph{size} of a formula to be the number of
unique subformulas.

A (total) assignment is a mapping $\assign \colon \varset \rightarrow
\{0, 1\}$.  Assignment $\assign$ is said to be a \emph{model} of
formula $\phi$ if the formula evaluates to $1$ under that assignment.
The set of models of a formula $\phi$ is written $\modelset(\phi)$.
We can also consider an assignment to be a set of literals, where $x \in \assign$
when $\assign(x) = 1$, and $\obar{x} \in \assign$ when
$\assign(x) = 0$, for each variable $x \in \varset$.

\emph{Weighted model counting} is defined in terms of a \emph{weight
  assignment} $w$, associating rational values $w(x)$ and
$w(\obar{x})$ with each variable $x \in \varset$.
The weight of an
assignment $\assign$ is then defined to be the product of its literal weights, and the weight
of a formula is the sum of the weights of its satisfying assignments:
\begin{eqnarray}
  w(\phi) & = & \sum_{\assign \in \modelset(\phi)} \;\;\prod_{\lit \in \assign} w(\lit) \label{eqn:wmc}
\end{eqnarray}

Computing the weighted count of an arbitrary formula is thought to be intractable.  However, it becomes
feasible when the formula is in \emph{deterministic decomposable} negation-normal form (d-DNNF):
\begin{enumerate}
\item The formula is in negation-normal form.  
\item All conjunctions are \emph{decomposable}~\cite{darwiche:jacm:2001,darwiche:jair:2002}.  That is, every subformula $\phi'$ of the form $\phi' = \phi_1 \land \phi_2$
  satisfies $\dependencyset(\phi_1) \cap \dependencyset(\phi_2) = \emptyset$.
\item All disjunctions are \emph{deterministic}~\cite{darwiche:jancl:2001,darwiche:jair:2002}.  That is, every subformula $\phi'$ of the form $\phi' =\phi_1 \lor \phi_2$ satisfies
  $\modelset(\phi_1) \cap \modelset(\phi_2) = \emptyset$.
\end{enumerate}
As an important subclass of d-DNNF, a formula is said to be in 
\emph{decision decomposable} negation-normal form (decision-DNNF)~\cite{huang:jair:2007} when every occurrence of a disjunction has the form 
$(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ for some variable $x$, referred to as the \emph{decision variable}.

There are several ways to compute the weighted count of d-DNNF formula $\phi$, all
based on an \emph{algebraic evaluation} of $\phi$ to compute a value $W(\phi)$:
\begin{enumerate}
\item Each literal $\lit$ is a assigned a rational value $W(\lit)$.
\item Each subformula $\phi' = \phi_1 \land \phi_2$ is evaluated as $W(\phi') = W(\phi_1) \cdot W(\phi_2)$.
\item Each subformula $\phi' = \phi_1 \lor \phi_2$ is evaluated as $W(\phi') = W(\phi_1) + W(\phi_2)$.
\end{enumerate}
The number of arithmetic operations in this evaluation is linear in the size of the formula.

The following methods use algebraic evaluation to compute a weighted model count $w(\phi)$ of formula $\phi$ for weight assignment $w$:
\begin{itemize}
\item If the weight assignment satisfies $w(x) + w(\obar{x}) = 1$  for every variable $x$,
  then by letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item Formula $\phi$ is said to be \emph{smooth} if every disjunction $\phi_1 \lor \phi_2$ satisfies
  $\dependencyset(\phi_1) = \dependencyset(\phi_2)$~\cite{darwiche:jancl:2001,darwiche:jair:2002}.  For a smooth formula, 
by letting $W(\lit) = w(\lit)$ for each literal $\lit$, the algebraic evaluation $W(\phi)$ will yield the weighted model count $w(\phi)$.
\item If the weight assignment satisfies $w(x) + w(\obar{x}) \not = 0$ for every variable $x$,
  we can apply \emph{rescaling}, first computing $s(x) = w(x) + w(\obar{x})$ for each variable $x$
  and letting $W(x) = w(x)/s(x)$ and $W(\obar{x}) = w(\obar{x})/s(x)$.  
  Following the algebraic evaluation, the weighted count is computed as:
  \begin{eqnarray}
w(\phi) &=& W(\phi)\; \cdot \;  \prod_{x\in\varset} s(x)  \label{eqn:rescale}
  \end{eqnarray}
\end{itemize}

An arbitrary formula can be smoothed by inserting \emph{smoothing terms} of the form $x \lor \obar{x}$~\cite{darwiche:jancl:2001}.
That is,
  a disjunction $\phi_1 \lor \phi_2$ having $x \in \dependencyset(\phi_1)$ but
  $x \not\in \dependencyset(\phi_2)$ is rewritten as $\phi_1 \lor [(x \lor \obar{x}) \land \phi_2]$.
  Adding smoothing terms can expand the size of a formula significantly, and it can be time consuming.
  More precisely, for $n = |X|$, and a formula with $m$ unique subformulas, it can require time $\Theta(m\cdot n)$ and increase the formula size by a factor of $n$.
  Some restricted formula classes allow more space- and time-efficient smoothing~\cite{shih:neurips:2019}, but the required properties do not hold for the formulas generated by most weighted model counters.
  
  These three methods can be combined by rescaling some variables,
  inserting smoothing terms for others, and taking no action for the rest.
  For example,
  for any variable $x$ having $w(x) + w(\obar{x}) = 0$, we can insert
  smoothing terms, while applying rescaling for other variables $y$ such that $w(y) + w(\obar{y}) \not= 1$.
These particular smoothing terms
  effectively cause the subformula to evaluate to
  $0$.

\section{Approximations and Number Representations}
\label{sect:background:numbers}

When approximating rational number $v$ with value $\approxv$, we
define the \emph{approximation error} $\aerror[\approxv, v]$ as:
\begin{eqnarray}
\aerror[\approxv, v] & = & \left\{ \begin{array}{ll}
  \frac{|\approxv - v|}{|v|}  & v \not = 0\\
  0 & v  = \approxv = 0\\
  1 & v = 0 \; \textrm{and} \; \approxv \not = 0
  \end{array} \right. \label{eqn:approx:error}
\end{eqnarray}
That is, we consider relative error when $v \not = 0$, and we require an exact representation when $v = 0$.
The approximation error will equal 0 when $\approxv=v$, and it will be greater for weaker approximations.

We then define the \emph{digit precision} of the approximation as
\begin{eqnarray}
\digitprecision(\approxv, v) & = & \max(0, -\log_{10} \aerror[\approxv, v]) \label{eqn:digitprecision} 
\end{eqnarray}
This value will range from $0$ for a poor approximation, up to $+\infty$ when $\approxv=v$.
Intuitively, it indicates the number of matching digits in the decimal representations of $\approxv$ and $v$.

We consider floating-point numbers of the form
\begin{eqnarray}
v & = & (-1)^s \; \times \; f \; \times 2^{e} \label{eqn:floating-point}
\end{eqnarray}
where:
\begin{itemize}
\item Sign bit $s$ equals $0$ for nonnegative numbers and $1$ for negative numbers
\item Fraction $f$ is encoded as a $p$-bit binary number with an implicit binary point on the left.  That is $0 \leq f \leq 1-2^{-p}$.
\item Exponent $e$ is an integer, possibly with some limitation on its range.
\end{itemize}
As examples, consider two different floating-point formats:
\begin{itemize}
\item The IEEE~754 double-precision format uses a slightly different
  representation, but it maps to the notation of
  Equation~\ref{eqn:floating-point} with $p=53$ and an exponent range
  of $-1021 \leq e \leq 1024~\cite{overton:siam:2001}$.
  Unfortunately, the small exponent range (giving a magnitude range
  for the numbers of around $10^{\pm 308}$) limits the suitability of
  this representation for weighted model counting.  For example, as
  part of the evaluation of weighted model counting when all weights
  are nonnegative, described in Section~\ref{sect:nonneg}, we computed
  the counts for 980 combinations of formula and weight assignment
  using double-precision arithmetic and compared them to the exact
  values computed using rational arithmetic.  Fully 622 ($63.5\%$) of
  the double-precision weights had digit precision equal to $0.0$,
  with 413 overflowing to infinity and 209 underflowing to zero.
\item The MPF software floating-point library allows the value of $p$
  to be set arbitrarily.  Our default configuration has $p=128$. On
  most 64-bit architectures, it represents the exponent as a 64-bit
  signed number.  This provides an ample exponent range, giving a
  magnitude range of over $10^{\pm 10^{18}}$.
  For example, the weighted model count for a tautology with $n$ variables, where all literals are assigned weight $w$, equals
  $2^n\cdot w^n = (2w)^n$.  Consider literal weight $w=10^{1000}$,
 far larger than can even be represented in IEEE-754 double precision, and let $n$ equal one trillion, over five orders of magnitude
 larger than the largest formulas solved by current weighted model counters.  The weighted count is
 $(2 \times 10^{1000})^{10^{12}} \approx 10^{10^{15.00013}}$.  In the other direction, the conjunction of one trillion variables, each having a weight
 of $10^{-1000}$ has a weighted count of $10^{-10^{15}}$.  Even these extreme values are
 well within the range of the MPF representation.
\end{itemize}

When encoding rational number $v$ in floating point, its value must be rounded to a
value $\round(v)$.  Doing so can introduce rounding
error~\cite{knuth:fp:1981,muller:hfpa:2018}.  
Letting $\roundepsilon = 2^{-p}$, and assuming 
the exponent range suffices to represent $v$,
we can assume that
$\aerror[\round(v), v] \leq \roundepsilon$.
As examples, the double-precision representation has $\roundepsilon = 2^{-53} \approx 1.11 \times 10^{-16}$, while the 128-bit MPF format
has $\roundepsilon = 2^{-128} \approx 2.94 \times 10^{-39}$.  As a consequence, the lower bounds of $\digitprecision(\round(v), v)$ are around $15.95$ and $38.53$, respectively.

Floating-point arithmetic is implemented in such a way that any
operation effectively computes an exact result and then rounds it to
encode the result as a floating-point value.  The maximum error from a sequence of operations
therefore tends to accumulate in multiples of $\roundepsilon$.
This yields error bounds of the form $\aerror[\approxv, v] \leq t\,\roundepsilon$,
which we  refer to as having at most $t$ units of
rounding error.

For interval $[\vmin, \vmax]$, we define the interval approximation error $\aerror([\vmin, \vmax])$ as
\begin{eqnarray}
\aerror([\vmin, \vmax]) & = & \left\{ \begin{array}{ll}
  \frac{\vmax - \vmin}{\min(|\vmin|, |\vmax|)}  & 0 \not \in [\vmin, \vmax]\\[0.8em]
  0 & \vmin = \vmax = 0 \\
  1 & 0 \in [\vmin, \vmax] \;\; \textrm{and} \;\; |\vmax - \vmin|  > 0 
  \end{array} \right. \label{eqn:interval:error}
\end{eqnarray}
For any values $\approxv, v \in [\vmin, \vmax]$, we can see that
$\aerror[\approxv, v] \leq \aerror([\vmin, \vmax])$.
We then define the digit precision of the interval as:
\begin{eqnarray}
\digitprecision([\vmin, \vmax]) & = & \max[0, -\log_{10} \aerror([\vmin, \vmax])] \label{eqn:interval:digitprecision} 
\end{eqnarray}
This value will range from $0.0$ for a very large interval, relative to the magnitudes of its endpoints, to $+\infty$ when the interval is tight with $\vmin = \vmax$.

\section{Only Nonnegative Weights}
\label{sect:nonneg}

Here we evaluate how rounding errors accumulate via a series of
arithmetic operations when all arguments are nonnegative.
That is, assume the exact arguments $v$ and $w$ for each operation satisfy $v \geq 0$ and $w \geq 0$.
Rounding never causes a nonnegative number to become negative, and therefore 
the approximations $\approxv$ of  $v$ and $\approxw$ of $w$ satisfy $\approxv \geq 0$ and $\approxw \geq 0$.
None of the operations multiplication, addition, or division yield negative results when their arguments are nonnegative.
We can therefore
assume that all actual and approximate values under consideration are
nonnegative.

Our analysis builds on historic work for bounding the error produced
by a series of floating-point
multiplications~\cite{muller:hfpa:2018,wilkinson:nm:1960,wilkinson:rounding:1963} or
additions~\cite{higham:siam:1993}.  Our formulation simplifies the analysis
by weakening the
bound, and it applies only when
all arguments are nonnegative.

Suppose for nonnegative values of $v$ and $w$ and nonnegative values $s$ and $t$, we have
$\aerror[\approxv, v] \leq s\, \roundepsilon$ and
$\aerror[\approxw, w] \leq t\, \roundepsilon$, respectively.
Therefore
$(1-s\,\roundepsilon)\, v \leq \approxv \leq (1+s\,\roundepsilon)\, v$ and
$(1-t\,\roundepsilon)\, w \leq \approxw \leq (1+t\,\roundepsilon)\, w$.


\subsection{Multiplication}

Assume that $v > 0$ and $w > 0$ and consider the effect of multiplying their approximations
$\approxv$ and $\approxw$.  To simply the analysis, let us impose as an additional constraint that $s\,t \leq 1/\roundepsilon$.
The product $\approxv \cdot \approxw$  satisfies
$\approxv \cdot \approxw \leq (v\cdot w) [1 + (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2]$, and we can use the additional constraint to replace $s\,t\,\roundepsilon^2$ by $\roundepsilon$,
giving
$\approxv \cdot \approxw \leq (v\cdot w) [1 + (s+t+1)\,\roundepsilon]$.
In the other direction, $\approxv \cdot \approxw$ satisfies
$(v\cdot w) [1 - (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2] \leq \approxv \cdot \approxw$.  We can drop the term
$s\,t\,\roundepsilon^2$ to give
$(v\cdot w) [1 - (s+t)\,\roundepsilon] \leq \approxv \cdot \approxw$.  These two bounds guarantee that
$\aerror[\approxv \cdot \approxw, v \cdot w] \leq (s+t+1)\,\roundepsilon$.
Rounding this result can introduce an additional error of at most $\roundepsilon$, and therefore
$\aerror[\round(\approxv \cdot \approxw), v \cdot w] \leq (s+t+2)\,\roundepsilon$.

When $v=0$ (respectively, $w=0$), we will have $\approxv=0$ (resp., $\approxw = 0$) and therefore $v \cdot w = \approxv \cdot \approxw = 0$.
We can therefore state that  for any nonnegative values of $v$ and $w$, the three conditions $\aerror[\approxv, v] \leq s\,\roundepsilon$,
$\aerror[\approxw, w] \leq t\,\roundepsilon$, and
$s\,t \leq 1/\roundepsilon$, imply that
$\aerror[\round(\approxv \cdot \approxw), v \cdot w] \leq (s+t+2)\,\roundepsilon$.

Thus, for values of $s$, $t$, and $\roundepsilon$ satisfying our
additional constraint, a multiplication operation, at most, propagates
the sum of the errors of its arguments, and it adds two units of
rounding error.

\subsection{Addition}

When positive values $v$ and $w$ are added, their approximations  $\approxv$ and $\approxw$ satisfy
$(v + w) (1 - r\,\roundepsilon) \leq \approxv + \approxw \leq (v + w) (1 + r\,\roundepsilon)$, where $r = \frac{s\,v + t\,w}{v+w}$.  That is, the resulting error $r$ is bounded by a weighted average
of those of its arguments.  For all values of $v$ and $w$, $r$ cannot exceed the maximum of $s$ and $t$.
Rounding the sum can add at most one unit of rounding error, and so we have
$\aerror[\round(\approxv + \approxw), v + w] \leq (\max(s,t)+1)\,\roundepsilon$.

If $v = 0$ (respectively, $w = 0$), we will have $\round(\approxv + \approxw) = \approxw$ (resp., $= \approxv$).
We can therefore state that for any nonnegative values of $v$ and $w$, the two conditions $\aerror[\approxv, v] \leq s\,\roundepsilon$ and
$\aerror[\approxw, w] \leq t\,\roundepsilon$ imply that 
$\aerror[\round(\approxv + \approxw), v + w] \leq (\max(s,t)+1)\,\roundepsilon$.

Thus, an addition operation, at most, propagates the maximum error of its arguments, and it adds one unit of rounding error.

\subsection{Evaluating a Decision-DNNF Formula}
\label{sect:error:formula}

Suppose we use floating-point arithmetic to compute the sums and
products in an algebraic evaluation of a decision-DNNF formula $\phi$.
We assume that the value $W(\lit)$ for each literal $\lit$ is
represented by a floating-point number $\approxW(\lit)$ such that
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$.  In practice,
this implies that rescaling must use rational arithmetic to
compute exact representations of $s(x)$, $w(x)/s(x)$, and
$w(\obar{x})/s(x)$ for each variable $x$, so that only one unit of
rounding error is introduced when representing each value $W(\lit)$.
We can then bound the error of the computed value $W(\phi)$ as follows:
\begin{lemma}
  The algebraic evaluation of a decision-DNNF formula $\phi$  having $|\dependencyset(\phi)| = n$, with
  $n \leq 1/(2\sqrt{\roundepsilon})$
  using floating-point arithmetic,
  and where all literals $\ell$ satisfy $W(\lit) \geq 0$,
  will yield an approximation $\approxW(\phi)$ satisfying
  $\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.
  \label{lemma:approx:pos}
\end{lemma}

The proof of this lemma proceeds by induction on the structure of $\phi$:
\begin{enumerate}
\item For literal $\lit$ with weight $W(\lit)$, its approximation $\approxW(\lit)$ satisfies
$\aerror[\approxW(\lit), W(\lit)] \leq \roundepsilon$, which is within the error bound of $(4n-2)\,\roundepsilon$ for $n=1$.
\item For conjunction $\phi$ of the form $\phi_1 \land \phi_2$, there must be some $k$, with $1 \leq k < n$, such that $|\dependencyset(\phi_1)| = k$
  and $|\dependencyset(\phi_2)| = n-k$.
\begin{enumerate}
\item Let us first test whether the requirement that $n \leq 1/(2\sqrt{\roundepsilon})$ 
  guarantees that the conditions on $s$, $t$, and $\roundepsilon$ required for the multiplication bound  hold.
For $s = 4k-2$ and $t = 4(n-k)-2$
    we require that $s\,t \leq 1/\roundepsilon$.  We can see that $s\,t \leq 16\,k\,(n-k)$. This quantity will be maximized when $k = n/2$,
    and therefore $s \, t \leq 4\,n^2$.
    Given our limit on $n$ with respect to $\roundepsilon$, we have $s \, t \leq 1/\roundepsilon$.
  \item
    We can also see that if $n \leq 1/(2\sqrt{\roundepsilon})$, then both
    $k \leq 1/(2\sqrt{\roundepsilon})$ and  $n-k \leq 1/(2\sqrt{\roundepsilon})$.
  \item
We can therefore assume by induction that 
 $\aerror[\approxW(\phi_1), W(\phi_1)] \leq (4 k-2) \,\roundepsilon$
  and also that $\aerror[\approxW(\phi_2), W(\phi_2)] \leq (4 (n-k)-2) \,\roundepsilon$.  Their product, after rounding
  will satisfy 
$\aerror[\approxW(\phi_1 \land \phi_2), W(\phi_1 \land \phi_2)] \leq [(4 k -2) + (4 (n-k) -2) + 2]\,\roundepsilon = (4n-2) \,\roundepsilon$.
\end{enumerate}
\item For disjunction $\phi$ of the form
  $\phi = (x \land \phi_1) \lor (\obar{x} \land \phi_2)$, let us use the notation $\lit_1 = x$ and $\lit_2 = \obar{x}$
  and consider the two subformulas $\lit_i \land \phi_i$ for $i \in \{1,2\}$.
  Since all products are decomposable, we must have $x \not \in \dependencyset(\phi_i)$,
  and therefore $|\dependencyset(\phi_i)| = n-1$.  We can also see that the condition 
  $n \leq 1/(2\sqrt{\roundepsilon})$ implies that   $n-1 \leq 1/(2\sqrt{\roundepsilon})$.
  By induction, we can therefore assume that
  $\aerror[\approxW(\phi_i), W(\phi_i)] \leq (4(n-1)-2) \,\roundepsilon = (4n-6)\,\roundepsilon$.  Rounding the literal weights will yield
  $\aerror[\approxW(\lit_i), W(\lit_i)] \leq \roundepsilon$.  Let $v_i$ denote the product $W(\lit_i) \cdot W(\phi_i)$ for $i \in \{1,2\}$.
  Its rounded value will satisfy
  $\aerror[\approxv_i, v_i] \leq (4n-3) \,\roundepsilon$.  Summing $\approxv_1$ and $\approxv_2$ and rounding the result will therefore give
  an approximation $\approxW(\phi)$ to $W(\phi) = v_1 + v_2$ with
$\aerror[\approxW(\phi), W(\phi)] \leq (4n-2)\,\roundepsilon$.  
\end{enumerate}

Observe that this proof relies on the decomposability of the
conjunctions to bound the error induced by multiplication operations.
It relies on the decision structure of the formula only to bound the
depth of the additions.  It does not rely on the formula being deterministic.

In the event of rescaling, we must also consider the error introduced
when computing the product $P = \prod_{x\in\varset} s(x)$.  We assume that
each term $s(x)$ is represented by a floating-point value
$\approxs(x)$ such that $\aerror[\approxs(x), s(x)] \leq
\roundepsilon$.  In practice, this requires using rational arithmetic
to represent $w(x)$ and $w(\obar{x})$ and to compute their sum.  The
only error introduced will then be when converting the sum into
a floating-point representation.

We can then bound the error of the product as
\begin{lemma}
  The computation of the product $P = \prod_{x\in\varset} s(x)$ having
$|\varset| = n$, with $n \leq 1/(2\sqrt{\roundepsilon})$ using floating-point arithmetic
and where all literals $\ell$ satisfy $s(\lit) \geq 0$,
will yield an approximation $\approxP$ satisfying
  $\aerror[\approxP, P] \leq (3n-2)\,\roundepsilon$.
  \label{lemma:approx:product}
\end{lemma}

The proof of this lemma proceeds much like that for Lemma~\ref{lemma:approx:pos}.  We assume an arbitrary association of the subproducts, and so $P$ can be computed as
$P_1 \cdot P_2$, where $P_1$ is the product of $k$ elements and $P_2$ is the product of $n-k$ elements, with $1 < k < n$.
The smaller coefficient of $3$ arises due to the lack of any addition operations.

Combining these two results, we can state the following result about weighted model counting when all weights are nonnegative:
\begin{theorem}
  \label{thm:approx:pos}
Computing the weighted model count of
a decision-DNNF formula $\phi$, where all literal weights are nonnegative, with $|\dependencyset(\phi)| = n$, and using floating-point arithmetic with a $p$-bit fraction, such that $\log_2 n \leq p/2-1$
will yield an approximation $\approxw(\phi)$ to the true weighted count $w(\phi)$, such that
\begin{eqnarray}
\digitprecision(\approxw(\phi), w(\phi)) & \geq & p \cdot \log_{10}2 - \log_{10}n - c\label{eqn:precision:wmc}
\end{eqnarray}
where $c = \log_{10} 7$ when rescaling is required and $c = \log_{10} 4$ when no rescaling is required.
\end{theorem}

Let us examine the practical implications of this theorem.  In
particular, assume we use the MPF library with $p=128$.  Only a single
formula from the 2024 weighted model competition had over one million
variables, and so let us assume  $n \leq 10^7$.  The
conditions for Equation~\ref{eqn:precision:wmc} to hold are easily
satisfied, since $\log_2 n < 23.3$ and $p/2-1 = 63$.  We will
therefore have
$\digitprecision(\approxw(\phi), w(\phi)) \geq 38.53 - 7 - 0.85 = 30.68$.
Even though the floating-point format
provides only around 38 digits of precision for a single rounding,
it maintains over 30 digits of precision when performing billions of operations to
compute the algebraic evaluation of a formula with 10
million variables and to rescale to compute a weighted model count.

In scaling to larger formulas, each time
the number of variables is increased by a factor of 10, at most one
digit of precision is lost.  For example, we could scale to $n=10^{12}$, while keeping $p=128$, and still have over 25 digits of precision.  That is over nine digits greater than the precision
provided by the IEEE double-precision representation.  If that precision is not sufficient, we could increase $p$ to $192$ and be guaranteed a decimal precision of over $44$.
Importantly, the bound of Equation~\ref{eqn:precision:wmc} holds regardless of the formula size and the weight assignment, 
as long as all weights are nonnegative.

For the remainder of this paper, we set a target precision of 30 decimal digits for
weighted model counting.  This number is chosen because 1) it should be
sufficient for most applications, and 2) it
is achievable, at least for nonnegative weights, with a standard software floating-point package.

\subsection{Generalizing to Other Representations}
\label{sect:ddnnf}

The bound of Equation~\ref{eqn:precision:wmc} applies specifically to decision-DNNF
formulas.  Having a decision variable associated with each disjunction
bounds the depth of the sum operations in a formula to $n$.  Some
decision diagrams with a binary branching structure, including Free
Binary Decision Diagrams (FBDDs)~\cite{wegener:siam:2000}
(a generalization of Ordered BDDs~\cite{bryant:ieeetc:1986,knuth:bdd:2011}), and Zero-suppressed Decision
Diagrams (ZDDs)~\cite{minato:sttt:2001,minato:ijcai:2007} can be
translated into smooth decision-DNNF formulas with a size expansion of
most $n$, and hence the bound of Equation~\ref{eqn:precision:wmc}
holds for these.

For more general d-DNNF formulas, and for decision diagrams with
nonbinary branching structures, including multi-valued decision
diagrams (MDDs)~\cite{srinivasan:iccad:1990} and sentential decision
diagrams (SDDs)~\cite{darwiche:ijcai:2011}, it is difficult to find a
useful error bound that applies to entire classes of formulas.
Instead, given a formula to evaluate,
we propose computing an error bound based on the
structure of the formula, and using this bound to guide the selection of
the precision $p$ used in a floating-point evaluation.

We can see with all of these representations that the core requirement
is to compute a rational value $V$ by evaluating an arithmetic
expression $\psi$ consisting of rational constants, products, and
sums, where some of the product and sum operations may have more than
two arguments.  Computing this value with floating-point operations
having precision $p$ will yield an approximation $\approxV$ to the
true value $V$.  We can recursively compute a bound $e(\psi)$, such that
$\aerror[\approxV, V] \leq e(V) \,\roundepsilon$.  We assume we
can convert each constant $v$ into its floating-point representation
$\approxv$ with at most one rounding, and therefore $e(v) = 1$.  For a
product of the form $\psi' = \prod_{1 \leq i \leq k} \psi_i$, we can
recursively compute $e(\psi') = 2(k-1) + \sum_{1 \leq i \leq k}
e(\psi_i)$.  Here, the multiplications can be performed via any
association.
For a sum of the form
$\psi' = \sum_{1 \leq i \leq k} \psi_i$, we can compute
$e(\psi') = \log_2(k-1) + \max\limits_{1\leq i \leq k} e(\psi_i)$.  Here, the sums should be performed as a balanced tree of binary additions.

For expression $\psi$, we can use the computed bound $e(\psi)$ to
select a fraction size $p$ that guarantees a desired level of
precision.  That is, we require $p \geq 2\,\log_2 e(\psi)$, and to
achieve digit precision $D$, we require $p \geq D \cdot \log_2 10 +\log_2 e(\psi)$.
%% Importantly, this bound depends only on the structure of the arithmetic expression, and not on the values of the
%% constants.

\subsection{Experimental Validation}

\begin{figure}
\centering{%
\begin{tikzpicture}
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10}, 
      legend style={at={(0.30,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/upos-mpf+vars}
    \input{data-formatted/epos-mpf+vars}
    \input{data-formatted/prodp-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$+$},
      \scriptsize \textsf{MC2024, Exponential$+$},
      \scriptsize \textsf{Optimized Product}
    }
    \addplot[mark=none] coordinates{(1,36.69) (1e7,30.69)};
    \node[right] at (axis cs: 2, 34.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Nonnegative Weights.  The precision is guaranteed to be greater than the bound.
We set as a target to have digit precisions of at least 30.0.}
\label{fig:pos:mpf}
\end{figure}

To experimentally test the bound of Equation~\ref{eqn:precision:wmc},
we evaluated 200 benchmark formulas from the public and private portions
of the 2024 Weighted Model Counting Competition.\footnote{\url{https://mccompetition.org/2024/mc_description.html}}
We ran version 2 of the
D4 knowledge compiler\footnote{Available at \url{https://github.com/crillab/d4v2}}
to convert these into decision-DNNF\@.
We were able to compile 100 of them with a time
limit on 3600 minutes on a machine with 64~GB of random-access memory.

Define a problem \emph{instance} to be a combination of a formula
plus a weight assignment for all of its literals.
For each formula, we generated two different collections of instances, each with five weight assignments:
\begin{itemize}
\item \textsf{Uniform$+$}: For each variable $x$, weight $w(x)$ is represented by a 9-digit decimal number selected uniformly in the range
  $[10^{-9},\,1-10^{-9}]$. The weight for $\obar{x}$ is then set to
  $w(\obar{x}) = 1-w(x)$.  Such a weight assignment is typical of those used in recent weighted model counting competitions~\cite{fichte:jea:2020}.
\item \textsf{Exponential$+$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$
  are drawn independently from an exponential distribution in the range
  $[10^{-9},\,10^{+9}]$.  Each weight is represented by a decimal number with 9 digits to the right of the decimal point.
\end{itemize}

%% By comparison, Dilkas~\cite{dilkas:cpaior:2023} devised a number
%% of benchmarks problems for evaluating weighted model counters, but his
%% focus was more on the formula structure than on the weight
%% assignment.  The assignments he generated all had weights represented
%% as two-digit decimal numbers between $0$ and $1$, and such that $w(x)
%% + w(\obar{x}) = 1$ for all variables $x$.  Numerically speaking, these weight
%% assignments should be less challenging than ours.

For each instance, we evaluated
the weighted count using MPF (with $p=128$) to get an approximate weight $\approxw$ and
using MPQ to get an exact weight $w$.  We then evaluated the digit precision according to Equation~\ref{eqn:digitprecision}.   Our implementation used rescaling for all variables
with $w(x) + w(\obar{x}) \not \in \{0, 1\}$.
Although not required for the instances used in this evaluation,
it will insert smoothing terms when $w(x) + w(\obar{x}) = 0$ for variable $x$.

In addition, we evaluated formulas of the form $\bigwedge_{1\leq i
  \leq 10^d} x_i$ for values of $d$ ranging from $0$ to $6$ using a
single weight for every variable.  We swept a parameter space of
weights of the form $1 + 10^{-9}\,k$ for $1 \leq k \leq 100$ and
chose the value of $k$ that minimized the digit precision.  We refer
to this as the \textsf{Optimized Product} collection.

Figure~\ref{fig:pos:mpf} shows the results of these evaluations for
the three collections.  For the two collections of weight assignments for each 2024 competition formula,
we show only the minimum precision for each of the five randomly
generated  weight assignments.  Each data point
represents one combination of formula and weight selection method and is placed
along the X~axis according to the number of variables
and on the Y~axis according to the computed digit precision.
The plot also shows the precision bound of Equation~\ref{eqn:precision:wmc} for $c=\log_{10} 7$.
Results are shown for 98 of the 100 formulas, since the evaluation consistently ran out of memory when using rational arithmetic for two of the formulas.

As can
be expected, all data points stay above the precision bound.  Indeed,
most exceed the bound by several decimal digits.  Our bound assumes
that rounding either consistently decreases or consistently
increases each computed result.  In practice, rounding goes in both directions, and
therefore the computed results stay closer to the true values.

The optimized products, on the other hand, demonstrate that particular
combinations of formula and weight assignment can come within 2 decimal digits of the
precision bound and to track its general trend.

\section{Negative, Zero, and Positive Weights}
\label{sect:neg}

The analysis of Section~\ref{sect:nonneg} no longer holds when
some literals have negative weights, while others have positive weights.  With a floating-point
representation, summing combinations of negative and positive
values can cause \emph{cancellation}, where arbitrary levels of
precision are lost~\cite{knuth:fp:1981}.  Consider, for example, the computation
$s + T - T$, where $s$ and $T$ are nonnegative floating-point values, with $s \ll T$.  Using
bounded-precision arithmetic, evaluating the sum as $s + (T - T)$ will yield $s$.
Evaluating it as $(s + T) - T$, however, can yield $0$ or some other value that bears little relation to $s$.
Cancellation can also occur when evaluating a sum $s + T - T'$, where $T \approx T'$.

Cancellation can arise when evaluating decision-DNNF formulas to such a degree that no precision $p$ that grows sublinearly with $n$ will suffice.
As an example, consider the following smooth, decision-DNNF formula $\tau_n$ over $n+1$ variables:
\begin{eqnarray}
\tau_n  & = & z \land \left[\bigwedge_{i = 1}^{n} \obar{x}_i \; \lor \; \bigwedge_{i = 1}^{n} x_i\right] \quad \lor \quad \obar{z} \land \left [\bigwedge_{i = 1}^{n} x_i\right] \label{eqn:max:precision}
\end{eqnarray}
with a weight assignment having only a single literal assigned a negative weight:
\begin{displaymath}
\begin{array}{lllll}
\makebox[1cm]{} &  w(z) \; = \; +1 & \makebox[2cm]{} &  w(x_i) \; = \; 10^{+9} & 1 \leq i \leq n \\
\makebox[1cm]{} &  w(\obar{z}) \; = \; -1 & &  w(\obar{x}_i) \; = \; 10^{-9} & 1 \leq i \leq n \\
\end{array}
\end{displaymath}
Computing $w(\tau_n)$  evaluates the sum $(s + T) - T$, where
$s = 10^{-9n}$ and $T = 10^{+9n}$.  Avoiding cancellation requires using a floating-point representation with a fraction of at least
$p = (18 \cdot \log_2 10)\, n \approx 60 \, n$ bits.
Using MPQ, we were able to compute $w(\tau_{10^7})$ exactly in around 35 seconds, even though the final step requires a total of 1.87~gigabytes to store the arguments
$s+T$ and $-T$, and the result $s$.  In general, however, rational arithmetic can be very time and memory intensive.


\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10},
      legend style={at={(0.32,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/uposneg-mpf+vars}
    \input{data-formatted/eposneg-mpf+vars}
    \input{data-formatted/bposneg-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$\pm$},
      \scriptsize \textsf{MC2024, Exponential$\pm$},
      \scriptsize \textsf{MC2024, Limits$\pm$}
    }
    \addplot[mark=none, dashed] coordinates{(1,36.69) (1e7,30.69)};
    \node[right] at (axis cs: 2, 34.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Negative Weights.  There is no guaranteed bound for precision, but many cases remain above the nonnegative weight bound.
The Limits$\pm$ weight assignment is designed to maximize precision loss.}
\label{fig:posneg:mpf}
\end{figure}

Contrary to the example of
Equation~\ref{eqn:max:precision},
floating-point arithmetic performs surprisingly well for
many real-world
situations, even in the presence of negative weights.
In Figure~\ref{fig:posneg:mpf}, we see a similar plot to that of Figure~\ref{fig:pos:mpf} for
three collections of weight assignments with negative weights.  The first two are generalizations of those used earlier:
\begin{itemize}
\item \textsf{Uniform$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes drawn independently
from a uniform distribution in the range  $[10^{-9},\,1-10^{-9}]$ and are represented as 9-digit decimal numbers.  Each is negated with probability $0.5$.
\item \textsf{Exponential$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes
  drawn independently from an exponential distribution in the range $[10^{-9},\,10^{+9}]$ and are represented with 9-digits to the right of the decimal point.  Each is negated with probability $0.5$.
\end{itemize}
As can be see with these plots, the results mostly stay above the precision bound of Equation~\ref{eqn:digitprecision},
even though this bound need not hold.  All stay above the target precision of $30.0$.

On deeper inspection, we can see that setting up the conditions for a
cancellation of the form $(s + T) - T'$, where $T \approx T'$, require
1) a large dynamic range among the computed values to give widely different values $s$ and $T$, and 2) sufficient
homogeneity in the computed values that we get two values $T$ and
$T'$ such that $T \approx T'$.  A uniform distribution has neither of
these properties.  An exponential distribution has a large dynamic
range, but the computed values tend to be very heterogenous.

To create a weight assignment that has more chance of cancellation, we devised the following class of assignments:
\begin{itemize}
\item\textsf{Limits$\pm$}:  Each weight $w(x)$ and $w(\obar{x})$ has a magnitude, chosen at random, of either $10^{-9}$ or $10^{+9}$, and they are each set negative with probability $0.5$.
  However, we exclude assignments with $w(x) + w(\obar{x}) = 0$.
\end{itemize}
The idea here is to give large dynamic ranges plus a high degree of
homogeneity.  The plots for this assignment in
Figure~\ref{fig:posneg:mpf} demonstrate the success of this strategy,
with many results falling below the target precision of $30.0$.

Figure~\ref{fig:posneg:mpf} presents a pessimistic
perspective for these instances, since it only shows the minimum
precision achieved out of five instances in a collection for each formula.  Considering all 490 instances
for the 98 formulas evaluated, 221 (45\%) yielded results above the
target precision of $30.0$.  We can also see how our choice of weights
leads to two bands of low precision.  129 instances (26\%) had digit
precisions in a band between $19.0$ and $23.3$.  These were ones where
the evaluation encountered values of $s$ and $T$ that
differ by a factor of around $10^{18}$.  The remaining 140 instances
(29\%) had digit precisions below $5.0$.  These were ones where the
encountered values of $s$ and $T$ differed by a factor of around
 $10^{36}$.

\section{Reliable and Efficient Weight Computation}
\label{sect:reliable}

\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              xmode=normal,xmin=0, xmax=50,
                              xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              xticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              ymode=normal, ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Estimated Precision}, ylabel={Actual Precision}
            ]

    \draw[ fill={blue!30}, opacity=0.2] (axis cs: 0,30) rectangle (axis cs: 30,50);
    \node at (axis cs:15,40) {Overly pessimistic};
    

    \input{data-formatted/uposneg-mpfi-est+act}
    \input{data-formatted/eposneg-mpfi-est+act}
    \input{data-formatted/bposneg-mpfi-est+act}
    \legend{
      \scriptsize \textsf{MC2024, Uniform $\pm$},
      \scriptsize \textsf{MC2024, Exponential $\pm$},
      \scriptsize \textsf{MC2024, Limits $\pm$}
    }
    \addplot[mark=none] coordinates{(0,0) (50,50)};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Predictive Accuracy of MPFI Interval Arithmetic.  MPFI never has a higher estimate than the actual, but it can
incorrectly predict a precision less than the target of 30.0.}
\label{fig:mpfi}
\end{figure}


We can see from Figure~\ref{fig:posneg:mpf} that floating-point
evaluations generate accurate results in many cases, but we must be
able to discern when those occur.  Given that capability, we can
devise a strategy to use progressively more costly evaluation methods
to reliably compute weighted counts.

\subsection{Interval Computation Applied to Weighted Model Counting}

Interval arithmetic provides a mechanism for using the approximate
computations of floating-point arithmetic, while providing a
guaranteed precision for the result.  It will only be beneficial,
however, if the interval bounds remain tight enough that the digit
precision bound of Equation~\ref{eqn:interval:digitprecision} meets
our target digit precision.  Our target bound of 30 seems fairly
aggressive in this respect: the width of the interval $\vmax-\vmin$
must be over 30 orders of magnitude smaller than the magnitudes of
$\vmin$ and $\vmax$.  Even the instances with only nonnegative weights
had digit precisions as low as 34.5, and so there is not much
room for further degradation.

Figure~\ref{fig:mpfi} shows the result of evaluating 100 formulas for
the three weight assignment collections containing negative weights,
with five instances per collection for each formula.  
The evaluation used
MPFI (with $p=128$) to get an estimated digit precision (X~axis) and
a nominal weight (the midpoint of the interval), along with MPQ to get
the exact weight.  The actual precision (Y~axis) was computed based on the
nominal and actual weights.  The evaluations using MPQ consistently ran out of
memory for two of the formulas, and hence the plot shows 1470
data points.  
As is expected, every point lies above the diagonal line where the two precisions are equal---the interval computation never overestimated the digit
precision.


Overall, we can see that the interval estimates were quite reliable,
especially for predicting which computed weights exceed the target
threshold of 30.
The
interval computations determined that 1196 ($81.3\%$) instances were
above the target threshold: 490 from \textsf{Exponential$\pm$}, 487 from
\textsf{Uniform$\pm$}, and 219 from \textsf{Limits$\pm$}.
Points lying in the blue rectangle
indicate instances where the estimate was overly pessimistic: they
estimated a target precision below 30, while the actual precision was
above.  This occurred for only 36 of the 1470 instances ($2.4\%$).  Of
these, 3 were from the \textsf{Uniform$\pm$} collection, while 33 were
from the \textsf{Limits$\pm$} collection.  

The interval analysis captures the general trend shown in
Figure~\ref{fig:posneg:mpf} that, even with negative
weights, floating-point evaluation only degrades significantly due to cancellation
for the weight assignments designed to maximize
this effect.  This gives us hope that we can use interval computation
to handle a large portion of instances having negative weights.

\subsection{A Hybrid Approach}
\label{sect:hybrid}

\begin{table}
  \caption{Performance Comparison of Different Implementation Strategies.  Run entries of the form $S$+$F$ indicate that $S$ runs
  were successful and $F$ runs either ran out of memory or failed to meet the target precision.  Our hybrid strategy is shown in red.}
  \label{tab:compare}
  \centering{
%  \begin{tabular}{llrrrrrr}
  \begin{tabular}{llrrrrr}
    \toprule
    \multicolumn{1}{c}{Strategy} & & \multicolumn{1}{c}{MPF} & \multicolumn{1}{c}{MPFI-128} & \multicolumn{1}{c}{MPFI-256} & \multicolumn{1}{c}{MPQ} & \multicolumn{1}{c}{Combined}
    % & \multicolumn{1}{c}{Avg (s)}
    \\
    \midrule
    \input{method_table}
    \\[-1em]
   \bottomrule
  \end{tabular}
  } % Centering
\end{table}

We are now ready to combine our three approaches---floating-point
arithmetic, interval computation, and rational arithmetic---into a
single, hybrid approach.  We devised the following scheme, and we can
measure its success based on 2500 instances---100 formulas, each with five collections of five instances, as shown in the fourth entry in Table~\ref{tab:compare}
\begin{enumerate}
\item For instances where all weights are nonnegative, use MPF,
  relying on the bound of Equation~\ref{eqn:precision:wmc} to
  guarantee sufficient precision.  This evaluation succeeded for all 1000 such
  instances, including 20 for which the evaluation with rational arithmetic failed.
\item For instances with negative weights, we perform a sequence of evaluations having successively greater cost:
\begin{enumerate}
\item 
  Use MPFI with $p=128$.  If the estimated precision
  bound meets the target bound, then we are done.  This succeeded for
  1222 of the 1500 instances evaluated, including 26 for which the evaluation with rational arithmetic failed.
\item For instances where the estimated precision does not meet the target, perform a second run with MPFI, but with $p=256$.  This succeeded
  for 168 of the 278 instances evaluated, including 4 for which the evaluation with rational arithmetic failed.
\item When the second attempt at interval computation fails, perform
  an evaluation using MPQ to perform rational arithmetic.  This
  succeeded for the remaining 110 instances.
\end{enumerate}
\end{enumerate}
Overall this strategy succeeded for all 2500 instances.

Table~\ref{tab:compare} summarizes the performance of five different strategies, with our hybrid strategy as the fourth.
Evaluating all 2500
instances with MPQ completes
2450 of them, requiring a
total of $93.1$ hours, of which over 
$22$ hours is spent on the 50 failed runs.
Combining MPF for the instances with nonnegative weights with MPQ for the rest
completes an
additional 20 instances and drops the total time to $55.97$ hours.
Using one pass with MPFI (for $p=128$)
for the instances with negative weights and then using MPQ for those
that do not meet the target precision completes all but 4 instances
and drops the total time to $22.42$ hours.
%Although this approach yields an
% increased time when MPQ is ultimately used, this is more than offset
% by the many cases that were successfully evaluated using MPFI\@.
Our proposed hybrid approach completes all 2500 instances in a total of
$17.74$ hours.
%% The performance gain with the second run of MPFI is not dramatic, but
%% being able to complete all instances is a significant achievement.
Finally, skipping the MPFI evaluation with $p=128$ and instead going directly to $p=256$ avoids
wasted effort, but that
does not
compensate for the time required to perform all 1500 evaluations using
$p=256$.  The appendix provides more
details on the performance of the hybrid strategy.

%% Of course, finding an optimal strategy depends heavily on the formulas
%% to be evaluated and the weight assignments.  Our overall strategy of using progressively more
%% precise but costly evaluation methods should work across a large range of problems.


\section{Conclusions}
\label{sect:conclusion}

For many applications, floating-point arithmetic can introduce
significant errors due to rounding, and it does not provide any way to quantify the error.
This
paper shows that such uncertainty can be avoided for weighted model
counting.  When all weights are nonnegative, results can be computed
using floating point with guaranteed precision.  When some weights
are negative, the program can attempt one or more levels of interval
computation, and these should handle a large fraction of the instances.  Ultimately,
the program may need to use full rational arithmetic, but the number
of such cases should be small.

%% Several extensions of this work bear investigation.  Considering other representations,
%% it would be useful to apply these methods when computing weighted model counts based on 
%% Sentential Decision Diagrams (SDDs), a representation that generalizes ordered BDDs~\cite{darwiche:ijcai:2011}.
%% An SDD could be converted into a d-DNNF formula and then evaluated with guidance by the error estimation method presented in Section~\ref{sect:ddnnf}.
%% These
%% generalize BDDs such that each node can have $k$ pairs of subformulas
%% (each subformula being a node in the SDD) of the form $p_i, s_i$,
%% where the set of variables occurring in each $p_i$ is disjoint from  the set of those
%% occurring in the corresponding $s_i$, and where the models of $p_i$ and
%% $p_j$ are disjoint for all $i$ and $j$ such that $1 \leq i < j \leq
%% k$.  An SDD can be transformed into a d-DNNF formula by expanding each
%% node as $k$ conjunction operations of the form $p_i \land s_i$ and to
%% use $k-1$ disjunction operations to combine their results.  For nonnegative
%% weight assignments, we could compute the error bound for the formula
%% as described in Section~\ref{sect:ddnnf} and then set a suitable
%% floating-point bit precision $p$ to achieve the desired digit precision.  In performing the
%% conversion from SDDs to d-DNNF, the disjunctions for the nodes should
%% structured as balanced binary trees to minimize the rounding errors
%% introduced by the corresponding addition operations.  For weight
%% assignments with negative weights, we could proceed with the same
%% layered approach as for decision-DNNF formulas.

Several variants of weighted model counting require
arithmetic operations on negative numbers.  Applying our hybrid approach to these bears further investigation.

First,
the \emph{relaxed Tseitin}
transformation~\cite{meert:starai:2016} provides  a form
of Plaisted-Green\-baum encoding~\cite{plaisted:jsc:1986} that preserves
weighted counts.  When encoding a Boolean formula $\phi_1 \circ \phi_2$ into clauses,
for Boolean operation $\circ$,
it introduces auxiliary variables $r$ and $z$ having
$w(z) = w(\obar{z}) = w(r) = 1$ and $w(\obar{r}) = -1$.

Second, the \emph{gradient
semiring}~\cite{eisner:acl:2002, kimmig:jal:2017,maene:aaai:2025} extends conventional weighted
model counting such that each weight is pair $\langle p, d \rangle$,
where $p$ is a probability and $d$ is a partial derivative of the
probability with respect to some variable $x$.  The weights
associated with variable $x$ are $w(x) = \langle p(x), 1\rangle$ and
$w(\obar{x}) = \langle 1-p(x), -1\rangle$, where $p(x)$ equals the probability that variable $x$ is assigned $1$.
The weights associated with variable $y$ such that $y \not = x$ are
$w(y) = \langle p(y), 0\rangle$ and
$w(\obar{y}) = \langle 1-p(y), 0\rangle$.  Arithmetic operations are defined according to the chain rule for derivatives,
with $\langle p_1, d_1\rangle + \langle p_2, d_2\rangle = \langle p_1 + p_2, d_1 + d_2\rangle$, and
$\langle p_1, d_1\rangle \cdot \langle p_2, d_2\rangle = \langle p_1 \cdot p_2, p_1 \cdot d_2 + p_2 \cdot d_1\rangle$.

Even with the operations on negative values, we speculate that interval floating-point arithmetic would perform well for both variants.


\newpage
\bibliography{references}

\newpage
\appendix

\section{Analysis of Evaluation Performance}
\label{app:performance}

\begin{figure}[t]
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.45,0.98)}},
      legend cell align={left},
                              xmode=log,xmin=0.001, xmax=10000.0,
                              xtick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              xticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              ymode=log, ymin=0.001, ymax=10000.0,
                              ytick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              yticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              xlabel={MPQ Runtime (secs.)}, ylabel={Hybrid Runtime (secs.)}
            ]

    \input{data-formatted/combo-mpf+mpq}
    \input{data-formatted/combo-mpfi+mpq}
    \input{data-formatted/combo-mpfi2+mpq}
    \input{data-formatted/combo-mpq+mpq}
    \legend{
      \scriptsize \textsf{MC2024, Hybrid-MPF},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-128},
      \scriptsize \textsf{MC2024, Hybrid-MPFI-256},
      \scriptsize \textsf{MC2024, Hybrid-MPQ}
    }
    \addplot[mark=none] coordinates{(0.001,0.001) (10000.0,10000.0)};
    \node[left] at (axis cs: 11000, 1400) {$1.0\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.01, 0.001) (10000.0, 1000.0)};
    \node[left] at (axis cs: 11000, 140) {$0.1\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.1, 0.001)  (10000.0, 100.0)};
    \node[left] at (axis cs: 11000, 10) {$0.01\times$};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Runtime for hybrid method vs.~for MPQ.  When the value can be computed with MPF or MPFI, the runtime is significantly better than using MPQ.  When it must resort to MPQ, it incurs some overhead.} 
\label{fig:runtime}
\end{figure}

Figure~\ref{fig:runtime} compares the runtimes (Y~axis) for the hybrid
strategy versus that for performing an evaluation using rational
arithmetic (X~axis) for the 2450 instances that succeeded in the
latter case.  These are categorized by the method by which the hybrid
method completed.  The diagonal lines show the relative time for the
hybrid approach versus rational arithmetic.

Of the 2450 instances, the 980 with nonnegative weights could be evaluated using MPF\@.  Many of these also had very small runtimes, even for MPQ\@.
Considering just the 670 instances for which MPQ required more than $1.0$ seconds, we find that MPF ran between $4.8$ and $112.9$ times faster than MPQ, with an average of $23.3$ and a median of $18.6$.  This shows a clear performance benefit in using MPF when all weights are nonnegative.

Of the 1470 instances containing negative weights, 1196 ($81.4\%$)
were successfully evaluated using MPFI with $p=128$.  Considering the
825 instances for which MPQ required more than $1.0$ seconds, we find
that MPFI ran between $2.0$ and $41.1$ times faster, with an average of
$8.8$ and a median of $7.1$.  Again, this level of evaluation had a
clear benefit for performance.  An additional 164 instances
($11.1\%$) were successfully evaluated using MPFI with $p=256$.  Of
the 96 instances for which MPQ required more than $1.0$ seconds, we
find that the combined time for two runs with MPFI was, in the worst
case, $1.04$ times higher than just running MPQ\@.  Overall, however,
the combination ran faster by as much as $9.8\times$, with an average
of $3.1$ and a median of $2.6$.  Again, using this level of evaluation
proved worthwhile.  Finally, 110 instances ($7.5\%$) required an
evaluation using MPQ\@.  In these cases, the hybrid runtime was
greater than that for MPQ alone, since the program also performed two
evaluations using MPFI\@.  Of the 82 instances for which MPQ required
more than $1.0$ seconds, we find that the hybrid approach ran between
$1.1$ and $1.7$ times slower, with an average and a median of $1.2$.
Fortunately, this performance penalty was more than offset by the gains achieved by the less costly evaluation methods.



\end{document}

Comparisons:
\begin{center}
  \begin{tabular}{llr}
    \toprule
    $M_1$ & $M_2$ & Ratio \\
    \midrule
    MPQ & MPF & \avgMpqMpf \\
    MPF & Hybrid & \avgMpfHybrid \\    
    MPQ & Hybrid & \avgMpqHybrid \\
    \bottomrule
  \end{tabular}
\end{center}


%%
%% End of file
