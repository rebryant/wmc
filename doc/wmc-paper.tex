\documentclass[letterpaper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xnewcommand}

%\newtheorem{proposition}{Proposition}

\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lit}{\ell}

\newcommand{\progname}[1]{\textsc{#1}}
\newcommand{\dfour}{\progname{D4}}
\newcommand{\Dfour}{\progname{D4}}

\newcommand{\approximate}[1]{\hat{#1}}
\newcommand{\approxx}{\approximate{x}}
\newcommand{\approxy}{\approximate{y}}
\newcommand{\approxw}{\approximate{w}}
\newcommand{\approxs}{\approximate{s}}
\newcommand{\round}{\mathit{Round}}
\newcommand{\aerror}{\delta}
\newcommand{\digitprecision}{\Delta}
\newcommand{\minvalue}{\omega}
\newcommand{\maxValue}{\Omega}
\newcommand{\roundepsilon}{\varepsilon}

\newcommand{\varset}{X}
\newcommand{\dependencyset}{{\cal V}}
\newcommand{\assign}{\alpha}
\newcommand{\modelset}{{\cal M}}
\newcommand{\entails}{\models}


\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Arithmetic Considerations in Weighted Model Counting}

\titlerunning{Arithmetic Considerations for Weighted Model Counting}

\author{Randal E. Bryant}{Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA}{Randy.Bryant@cs.cmu.edu}{https://orcid.org/0000-0001-5024-6613}{}

\authorrunning{R. E. Bryant} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Randal E. Bryant} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Weighted model counting effectively computes the sum of the weights
  associated with the satisfying assignments for a Boolean formula,
  where the weight of an assignment is given by the product of the
  weights associated with the positive and negated variables
  comprising the assignment.  Weighted model counting finds
  applications in a wide variety of domains including machine learning
  and quantitative risk assignment.

  Most weighted model counting programs avoid enumerating all
  assignments to a formula by converting it, either explicitly or
  implicity, into a set of conjunction and disjunction operations,
  such that the count can be evaluating using multiplication for
  conjunctions and addition for disjunctions.

  Although computing these operations with rational arithmetic
  yields exact results, the precision required for most applications
  does not warrant the required space and time.  On
  the other hand, the precision when using floating-point arithmetic
  cannot be guaranteed, especially when some of the weights are
  negative.

  In this paper, we prove that, when all weights are nonnegative, the
  precision of floating-point arithmetic will degrade, in the worst case, as the logarithm
  of the number of variables.  This result, along with our
  experiments, demonstrate that a high level of precision can be
  achieved using a software floating-point implementation.  For
  formulas with negative weights, we show that interval floating-point
  arithmetic can reliably detect cases where a desired precision
  cannot be achieved.  Using a program that combines floating-point,
  interval, and rational arithmetic, we achieve the twin goals of
  effiency and guaranteed precision.
\end{abstract}

\section{Preliminaries}

When approximating real number $x$ with value $\approximate{x}$, we
define the \emph{approximation error} $\aerror(\approxx, x)$ as:
\begin{eqnarray}
\aerror[\approxx, x] & = & \left\{ \begin{array}{ll}
  \frac{|\approxx - x|}{|x|} \label{eqn:approx:error} & x \not = 0\\[0.8em]
  |\approxx| & x = 0\\
  \end{array} \right.
\end{eqnarray}
That is, we consider relative error when $x$ is nonzero, and absolute error when $x=0$.  This value will equal 0 when $\approxx=x$, and it will be greater for weaker approximations.

We then define the \emph{digit precision} of the approximation as
\begin{eqnarray}
\digitprecision(\approxx, x) & = & \max(0, -\log_{10} \aerror[\approxx, x]) \label{eqn:digitprecision} 
\end{eqnarray}
This value will range from $0$ for a poor approximation, up to $\infty$ when $\approxx=x$.
When both $\approxx$ and $x$ are written as (possibly infinite-length)
decimal numbers, $\digitprecision(\approxx,x)$ indicates the number of
leading digits they have in common.  

We consider floating-point numbers of the form
\begin{eqnarray}
x & = & (-1)^s \; \times \; f \; \times 2^{e} \label{eqn:floating-point}
\end{eqnarray}
where:
\begin{itemize}
\item Sign bit $s$ equals $0$ for nonnegative numbers and $1$ for negative numbers
\item Fraction $f$ is a $p$-bit binary number with an implicit binary point on the left.  That is $0 \leq f \leq 1-2^{-p}$.
\item Exponent $e$ is an integer, possibly with some limitation on its range.
\end{itemize}
As examples, consider these two different floating-point formats:
\begin{itemize}
\item The IEEE~754 double-precision format uses a slightly different representation, but it maps to the notation of Equation~\ref{eqn:floating-point} with $p=53$ and an exponent range of
  $-1021 \leq e \leq 1024$.  Unfortunately, the small exponent range limits the suitability of
  this representation for weighted model counting.
\item As will be shown, the MPF software floating-point package can be very effective for weighted model counting.
We configured it to have $p=128$. On most 64-bit architectures, it represents
  the exponent as a 64-bit signed number.  This provides an ample exponent range.
\end{itemize}

When representing a real-number $x$, its value must be rounded to a
value $\round(x)$.  In doing so, it introduces a \emph{rounding
error}.  We use $\roundepsilon$ to denote the maximum value of
$\aerror[\round(x), x]$.  Depending how the rounding
is performed, $\roundepsilon$ is either $2^{-p}$ or $2^{-p-1}$.  Here, we assume that the exponent range suffices to represent $x$.
As examples, the double-precision representation has $\roundepsilon = 2^{-54} \approx 5.55 \times 10^{-17}$, while the 128-bit MPF format
has $\roundepsilon = 2^{-128} \approx 2.94 \times 10^{-39}$.  As a consequence, the lower bound of $\digitprecision(\round(x), x)$ is around $15.95$ and $38.53$, respectively.

\section{Error analysis: Nonnegative weights}
\label{sect:nonneg}

Here we evaluate how rounding errors accumulate via a series of arithmetic operations.  We assume that each operation is performed in a way that effectively computes the exact
arithmetic value, but this value is then rounded to produce the final result.  We assume that $\roundepsilon$ is sufficiently small that $\roundepsilon^2 \approx 0$.

Suppose during some computation, an accumulation of rounding errors has yielded an approximation $\approxx$ to a true value $x > 0$.
Let $\aerror[\approxx, x] \leq s\, \roundepsilon$, i.e., $(1-s\,\roundepsilon)\, x \leq \approxx \leq (1+s\,\roundepsilon)\, x$.  Similarly, let true value $y > 0$ be
approximated by a value $\approxy$ such that 
$\aerror[\approxy, y] \leq t\, \roundepsilon$, i.e., $(1-t\,\roundepsilon)\, y \leq \approxy \leq (1+t\,\roundepsilon)\, y$.

In the following, we consider only the case where the true values of
all arguments are nonnegative.  That is $x \geq 0$ and $y \geq 0$.
Rounding never causes a result to be negated, and therefore, we can
consider the approximations to satisfy $\approxx \geq 0$ and $\approxy \geq 0$.
In addition, neither the multiplication nor the addition of
nonnegative values can yield a negative result.  We can therefore
assume that all actual and approximate values under consideration will
be nonnegative.

\subsection{Multiplication}

When $\approxx$ and $\approxy$ are multiplied, let us impose the constraint that $s\,t \leq 1/\roundepsilon$.
Their product will satisfy
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t)\,\roundepsilon + s\,t\,\roundepsilon^2)$, and our assumption gives
$\approxx \cdot \approxy \leq (x\cdot y) (1 + (s+t+1)\,\roundepsilon)$.
A similar analysis gives
$(x\cdot y) (1 - (s+t+1)\,\roundepsilon) \leq \approxx \cdot \approxy$, and therefore
$\aerror[\approxx \cdot \approxy, x \cdot y] \leq (s+t+1)\,\roundepsilon$.
Rounding this result introduces an additional error of $\roundepsilon$, and therefore
$\aerror[\round(\approxx \cdot \approxy), x \cdot y] \leq (s+t+2)\,\roundepsilon$.

Thus, multiplication, at most, propagates the errors of its arguments as their sum and adds two units of rounding error.

\subsection{Addition}

When $\approxx$ and $\approxy$ are added, their sum will satisfy
$(x + y) (1 - r\,\roundepsilon) \leq \approxx + \approxy \leq (x + y) (1 + r\,\roundepsilon)$, where $r = \frac{sx + ty}{x+y}$.  That is, the resulting error $r$ will bounded by a weighted average
of those of its arguments.  For all values of $x$ and $y$, $r$ cannot exceed the maximum of $s$ and $t$.
Rounding the sum can add one more unit of rounding error, and so we have
$\aerror[\round(\approxx + \approxy), x + y] \leq (\max(s,t)+1)\,\roundepsilon$.

Thus, addition, at most,  propagates the maximum error of its arguments and adds a unit of rounding error.

\subsection{Evaluating a decision-DNNF formula}
\label{sect:error:formula}

Consider a Boolean formula over a set of Boolean variables $\varset$
with operations conjunction $\land$, disjunction $\lor$, and negation
$\neg$, satisfying the following conditions:
\begin{itemize}
\item The formula is in negation normal form, i.e., negation is only
  applied to the variables.  We refer to positive and negated
  variables as \emph{literals}.  The negation of variable $x$ is
  written $\obar{x}$. We use the symbol $\lit$ to denote a literal.
\item Conjunctions are \emph{decomposable}.  That is, for formula $\phi$, let $\dependencyset(\phi)$ denote the set of variables that occur in it.  Then for a conjunction of the form
  $\phi_1 \land \phi_2$, we require $\dependencyset(\phi_1) \cap \dependencyset(\phi_2) = \emptyset$.
\item Every disjunction has a \emph{decision variable}.  That is, disjunctions are of the form $(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ for some decision variable $x$. 
\item The formula is \emph{smooth}: Every disjunction of the form $(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ 
also satisfies the property that $\dependencyset(\phi_1) = \dependencyset(\phi_2)$.
\end{itemize}

A (total) assignment is a mapping $\assign \colon \varset \rightarrow \{0, 1\}$.  An assignment $\assign$ is said to be a \emph{model} of formula
$\phi$ if the formula evaluates to $1$ under that assignment.  The
set of models of a formula $\phi$ is written $\modelset(\phi)$.  We
can also consider an assignment to be a set of literals, where
$x \in \assign$ when $\assign(x) = 1$, and  $\obar{x} \in \assign$ when $\assign(x) = 0$, for each variable $x$.

With \emph{weighted model counting}, each literal $\lit$ is assigned a
real-valued weight $w(\lit)$.  We then compute the weight of an
assignment $\assign$ to be the product of its literals. and the weight
of a formula to be the sum of the weights of its satisfying assignments:
\begin{eqnarray}
  w(\phi) & = & \sum_{\assign \in \modelset(\phi)} \prod_{\lit \in \assign} w(\lit) \label{eqn:model:count}
\end{eqnarray}

We use the following recursive algorithm to \emph{evaluate} a decision-DNNF formula $\phi$ with respect to a weight assignment, yielding a value $w(\phi)$:
\begin{enumerate}
\item The weight for literal $\lit$ is computed as $w(\lit)$.
\item The weight for  conjunction $\phi$ of the form $\phi_1 \land \phi_2$ is computed as $w(\phi) = w(\phi_1) \cdot w(\phi_2)$.
\item The weight for disjunction $\phi$ of the form 
  $(x \land \phi_1) \lor (\obar{x} \land \phi_2)$ is computed as
  $w(\phi) = [w(x) \cdot w(\phi_1)] + [w(\obar{x}) \cdot w(\phi_2)]$.
\end{enumerate}

Suppose we use floating-point arithmetic to compute the sums and products in the evaluation of decision-DNNF formula $\phi$, yielding
an approximation $\approxw(\phi)$ to the true value $w(\phi)$.  When all literal weights are nonnegative, and we limit the number of variables $n$ with respect to $\roundepsilon$ appropriately,
we can bound the error of this approximation as follows:
\begin{lemma}
  Evaluating a decision-DNNF formula $\phi$ having $|\dependencyset(\phi)| = n$, with $n \leq 1/(2\sqrt{\roundepsilon})$, according to a weight assignment where every literal $\lit$ has weight $w(\lit) \geq 0$,
the approximation $\approxw(\phi)$ satisfies
  $\aerror[\approxw(\phi), w(\phi)] \leq (4n-2)\,\roundepsilon$.
  \label{lemma:approx:pos}
\end{lemma}

The proof of this theorem proceeds by induction on the structure of $\phi$:
\begin{enumerate}
\item For Literal with weight $w(\lit)$, its approximation $\approxw(\lit)$ will be due to a single rounding error:
$\aerror[\approxw(\lit), w(\lit)] \leq \roundepsilon$, which is within the error bound of $(4n-2)\,\roundepsilon$ for $n=1$.
\item For formula $\phi$ of the form $\phi_1 \land \phi_2$, there must be some $k$, with $1 \leq k < n$, such that $|\dependencyset(\phi_1)| = k$
  and $|\dependencyset(\phi_2)| = n-k$.
\begin{enumerate}
  \item Let us first test whether the conditions for the induction hypothesis hold: for $s = 4(k-2)$ and $t = 4(n-k)-2$
    we require that $s\,t \leq 1/\roundepsilon$.  We can see that $s\,t \leq 16\,n\,(n-k)$. This quantity will be maximized when $k = n/2$, and therefore $s \, t \leq 4\,n^2$.
    Given our limit on $n$ with respect to $\roundepsilon$, we have $s \, t \leq 1/\roundepsilon$.
  \item
We can therefore assume by induction that 
 $\aerror[\approxw(\phi_1), w(\phi_1)] \leq (4 k-2) \,\roundepsilon$
  and $\aerror[\approxw(\phi_2), w(\phi_2)] \leq (4 (n-k)-2) \,\roundepsilon$.  Their product, after rounding
  will satisfy 
$\aerror[\approxw(\phi_1 \land \phi_2), w(\phi_1 \land \phi_2)] \leq [(4 k -2) + (4 (n-k) -2) + 2]\,\roundepsilon = (4n-2) \,\roundepsilon$.
\end{enumerate}
\item For a sum of the form
  $\phi = (x \land \phi_1) \lor (\obar{x} \land \phi_2)$, let us use the notation $\lit_1 = x$ and $\lit_2 = \obar{x}$
  and consider the two subformulas $\lit_i \land \phi_i$ for $i \in \{1,2\}$.
  Since all products are decomposable, we must have $x \not \in \dependencyset(\phi_i)$,
  and therefore $|\dependencyset(\phi_i)| = n-1$.
  By induction, we can therefore assume that
  $\aerror[\approxw(\phi_i), w(\phi_i)] \leq (4(n-1)-2) \,\roundepsilon = (4n-6)\,\roundepsilon$.  Rounding the literal weights will yield
  $\aerror[\approxw(\lit_i), w(\lit_i)] \leq \roundepsilon$.  Let $y_i$ denote the product $w(\lit_i) \cdot w(\phi_i)$ for $i \in \{1,2\}$.
  Its rounded value will satisfy
  $\aerror[\approxy_i, y_i] \leq (4n-3) \,\roundepsilon$.  Summing $\approxy_1$ and $\approxy_2$ and rounding the result will therefore give
  an approximation $\approxw(\phi)$ to $w(\phi) = y_1 + y_2$ with
$\aerror[\approxw(\phi), w(\phi)] \leq (4n-2)\,\roundepsilon$.  
\end{enumerate}

\begin{theorem}
  \label{thm:approx:pos}
Computing the weighted model count of a
a decision-DNNF formula $\phi$ having $|\dependencyset(\phi)| = n$, using floating-point arithmetic with a $p$-bit fraction, such that $\log_2 n \leq p/2-1$
will yield an approximation $\approxw(\phi)$ to the true weighted count $w(\phi)$, such that
\begin{eqnarray}
\digitprecision(\approxw(\phi), w(\phi)) & \geq & 0.30\,p - c - \log_{10}(n) \label{eqn:precision:wmc}
\end{eqnarray}
where $c = 0.85$ when rescaling is required and $c = 0.61$ when no rescaling is required.
\end{theorem}

This theorem shows that, in many cases, floating-point arithmetic will suffice for weighted model counting.  In particular, consider $p=128$, and let us set as a target to have a decimal precision of at least $30$.
Even using rescaling, we could achieve the target precision for $n \leq 3 \times 10^7$, and that would give $\log_2 n \leq 25$, which satisfies the constraint on $n$ relative to $p$.

Overall, each time
the number of variables is increased by a factor of 10, at most one
digit of precision is lost.
Importantly, this bound holds independent of the formula size, as well as of the values of the weights, as long as they are nonnegative.


\subsection{Eliminating smoothing}

As long as no variable $x$ has $w(x) + w(\obar{x}) = 0$, we can 
compute the weighted model count of non-smoothed formula $\phi$ by a method we call
\emph{rescaling}.  That is, for each variable $x$, define $s(x) = w(x)
+ w(\obar{x})$.  We then rescale the weights for $x$ and its
complement as $w'(x) = w(x)/s(x)$ and $w'(\obar{x}) = w(\obar{x})/s(x)$.  The formula is evaluated as before, but with
scaled literal weights, to compute a scaled weight for the formula $w'(\phi)$.  We then
compute $w(\phi) = \prod_{x\in\varset} s(x) \; \cdot \; w'(\phi)$.

This approach has the advantage that it avoids the need to add
smoothing terms to the formula, a time-consuming process that can
increase the formula size by up to a factor of $n$.  To handle the
case where some variable $x$ has weights summing to zero:
$w(x) + w(\obar{x}) = 0$, we can employ \emph{selective disabling}.  That is,
we traverse the formula and identify subformulas of the form
$\phi_1 \lor \phi_2$ such that
$x in \dependencyset(\phi_1)$, but $x \not \in \dependencyset(\phi_2)$.
Smoothing would replace $\phi_2$ by $\phi'_2 \land (x \lor \obar{x})$.
Instead, we simply record that $w(\phi_2) = 0$.  A similar case holds when the roles of $\phi_1$ and $\phi_2$ are reversed.


Let us consider the possible error introduced by rescaling.  To maximize precision, we
use a rational arithmetic package to exactly compute $s(x)$, $w'(x)$, and $w'(\obar{x})$.  We then round these to produce
approximations $\approxs(x)$, $\approxw'(x)$, and $\approxw'(\obar{x})$ with at most one unit of rounding error.
Computing the product $y = \prod_{x\in\varset} s(x)$ will yield an approximation $\approxy$ such that
$\aerror[\approxy, y] \leq (2n-1) \,\roundepsilon$, due to the rounding errors of the arguments, plus the $n-1$ rounding errors of the successive products.
The formula evaluation will yield an approximation $\approxw'(\phi)$, such that
$\aerror[\approxw'(\phi), w'(\phi)] \leq (3n-1) \,\roundepsilon$.  The final product will combine these and add one more unit of rounding error to give
$\aerror[\approxw(\phi), w(\phi)] \leq (5n-1) \,\roundepsilon$.

Thus, computing the count by rescaling can have greater error than by smoothing, but the impact will be very small.
Compared to smoothing, rescaling will reduce the bound of Equation~\ref{eqn:precision:wmc} by $\log_{10} 5/3 \approx 0.22$ digits of precision.

\subsection{Allowing non-decision disjunctions}

Weighted model counting can be computed as we have shown, as long as
all sum operations are \emph{deterministic}.  That is, the formula can
contain disjunctions of the form $\phi_1 \lor \phi_2$, as long as
$\modelset(\phi_1) \cap \modelset(\phi_2) = \emptyset$.  This class of formulas is referred to as ``d-DNNF.''

Our previous requirement that each disjunction have a decision variable
had the effect of limiting the depth of the sum operations in a
formula to $n$.  Without this requirement, the depth can become exponential.
For example, consider a d-DNNF formula encoding a tautology over $n$
variables, constructed as follows.  For each possible assignment
$\assign$, we use conjunction operations to encode 
$\phi_{\assign} = \bigwedge_{\lit \in \assign} \lit$.  We then use a sequence of $2^n-1$ disjunctions to combine these products to form $\phi$.
The accumulated error from these operations could grow exponentially with $n$.
Of course, such a formula is completely impractical for $n \geq 30$, and there are better ways to encode tautology.

The only
cases we know where non-decision conjunctions occur in d-DNNF formulas is with projected model counting, where
instances of a decision variable or its complement are removed by projection.
In such cases, the bound of Equation~\ref{eqn:precision:wmc} holds,
but we must include all variables, included those projected away, in the
variable count $n$.

\subsection{Experimental Validation}

\begin{figure}
\centering{%
\begin{tikzpicture}
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10}, 
      legend style={at={(0.30,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/upos-mpf+vars}
    \input{data-formatted/epos-mpf+vars}
    \input{data-formatted/prodp-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$+$},
      \scriptsize \textsf{MC2024, Exponential$+$},
      \scriptsize \textsf{Optimized Product}
    }
    \addplot[mark=none] coordinates{(1,38.05) (1e7,31.05)};
    \node[right] at (axis cs: 2, 35.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Nonnegative Weights.  The precision is guaranteed to be greater than the bound.
We set as a target to have digit precisions of at least 30.0.}
\label{fig:pos:mpf}
\end{figure}

To evaluate the implications of Equation~\ref{eqn:precision:wmc}, we
evaluated the benchmark formulas from the 2024 Weighted Model Counting
Competition.  We attempted to convert these into decision-DNNF using
version 2 of the the D4 knowledge compiler.  We were able to compile
100 of them with a time limit on 3600 minutes on a machine with 64~GB
of random-access memory.  For each of these formulas, we then generated two different collections, each with five sets of literal weights:
\begin{itemize}
\item \textsf{Uniform$+$}: For each variable $x$, weight $w(x)$
  is drawn from a uniform distribution in the range
  $[10^{-9},\,1-10^{-9}]$. The weight for $\obar{x}$ is then set to
  $w(\obar{x}) = 1-w(x)$.  Such a weight assignment is typical of those used in recent weighted model counting competitions
\item \textsf{Exponential$+$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$
  are drawn from an exponential distribution in the range
  $[10^{-9},\,10^{9}]$.
\end{itemize}
For each combination of formula and weight assignment, we evaluated
the weights using MPF to get an approximate weight $\approxw$ and
using MPQ to get an exact weight $w$.  We then evaluated the digit precision according to Equation~\ref{eqn:digitprecision}.
In addition, we evaluated formulas of the form $\bigwedge_{1\leq i \leq 10^p} x_i$ for values of $p$ ranging from $0$ to $6$ using a single weight for every variable.
We swept a parameter space of weights of the form $1 + k\cdot 10^{-9}$ for $1 \leq k \leq 100$ and chose the value of $k$ that minimized the digit precision.

Figure~\ref{fig:pos:mpf} shows the results of these evaluations for
the three collections.  For the two collections of weight assignments for each 2024 competition formula,
we show only the minimum precision for each of the five randomly
generated sets of weights.  Each data point
represents one combination of formula and weight selection method and is placed
along the X-axis according to the number of variables
and on the Y-axis according to the computed digit precision.

As can
be expected, all data points stay above the precision bound.  Indeed,
most exceed the bound by several decimal digits.  Our bound assumes
that rounding either consistently decreases or consistently
increases each computed result by a single unit of rounding error.  In
practice, rounding goes in both directions, and
therefore the computed results stay closer to the true value.

The optimized products, on the other hand, demonstrate that particular
combinations of formula and weight assignment can come very close to
the precision bound.


\section{Negative weights}
\label{sect:neg}

The analysis of Section~\ref{sect:nonneg} is no longer valid when
some literals have negative weights.  With a floating-point
representation, summing combinations of negative and nonnegative
values can cause \emph{cancellation}, where arbitary levels of
precision are lost.  Consider, for example, the computation
$s + T - T$, where $s$ and $T$ are nonnegative, but $s \ll T$.  Using
bounded-precision arithmetic, evaluating the sum as $s + (T - T)$ will yield a value
that is very close to $s$.
Evaluating it as $(s + T) - T$, however, can yield $0$ or some other value that bears little relation to $s$.

Cancellation can arise when evaluating decision-DNNF formulas to such a degree that no bounded precision $p$ that grows sublinearly with $n$ will suffice.
In particular, consider the formula
\begin{eqnarray}
\phi_n  & = & z \land \left[\bigwedge_{i = 1}^{n} \obar{x}_i \; \lor \; \bigwedge_{i = 1}^{n} x_i\right] \quad \lor \quad \obar{z} \land \left [\bigwedge_{i = 1}^{n} x_i\right] \label{eqn:max:precision}
\end{eqnarray}
with the following weight assignment:
\begin{center}
\begin{displaymath}
\begin{array}{llll}
  w(z) & = & +1 \\
  w(\obar{z}) & = & -1 \\
  w(x_i) & = & 10^9 & 1 \leq i \leq n \\
  w(\obar{x}_i) & = & 10^{-9} & 1 \leq i \leq n \\
\end{array}
\end{displaymath}
\end{center}
Then computing $w(\phi_n)$  evaluates the sum $(s + T) - T$, where
$s = 10^{-9n}$ and $T = 10^{9n}$.  Avoiding cancellation requires using a floating-point representation with a fraction of at least
$p = n \cdot 18 \cdot \log_2 10 \approx n \cdot 59.79$ bits.
On the other hand, using MPQ, $w(\phi_{10^7})$ can be computed exactly in around 35 seconds, even though the final step requires a total of 1.87~gigabytes to store the arguments
$s+T$, $-T$, and the result $s$.


\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=8cm,width=12cm,grid=both, grid style={black!10},
      legend style={at={(0.32,0.25)}},
      legend cell align={left},
                              %x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1,xmax=1e6,
                              xtick={1,10,100,1000,1e4,1e5,1e6,1e7}, xticklabels={1, $10^1$, $10^2$, $10^3$, $10^4$, $10^5$, $10^6$, $10^7$},
                              ymode=normal,ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Number of variables $n$}, ylabel={Digit Precision}
            ]

    \input{data-formatted/uposneg-mpf+vars}
    \input{data-formatted/eposneg-mpf+vars}
    \input{data-formatted/bposneg-mpf+vars}
    \legend{
      \scriptsize \textsf{MC2024, Uniform$\pm$},
      \scriptsize \textsf{MC2024, Exponential$\pm$},
      \scriptsize \textsf{MC2024, Limits$\pm$}
    }
    \addplot[mark=none, dashed] coordinates{(1,38.05) (1e7,31.05)};
    \node[right] at (axis cs: 2, 35.0) {\rotatebox{-4}{Precision Bound}};
    \addplot[mark=none, color=darkred] coordinates{(1,30) (1e7,30)};
    \node[right] at (axis cs: 2, 28.5) {Target Precision};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Digit Precision Achieved by MPF for Benchmarks with Negative Weights.  There is no guaranteed bound for precision, but many cases remain above the nonnegative weight bound.
The Limits~$\pm$ weight assignment was designed to maximize precision loss.}
\label{fig:posneg:mpf}
\end{figure}

Interestingly, however, using MPF with a fixed precision of $p=128$ does surprisingly well on many formulas, even in the presence of negative weights.
In Figure~\ref{fig:posneg:mpf}, we see a similar plot to that of Figure~\ref{fig:pos:mpf}.  The first two weight distributions are simple generalizations of those used earlier:
\begin{itemize}
\item \textsf{Uniform$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes drawn independently
from a uniform distribution in the range  $[10^{-9},\,1-10^{-9}]$.  Each is negated with probability $0.5$.
\item \textsf{Exponential$\pm$}: For each variable $x$, weights $w(x)$ and $w(\obar{x})$ have magnitudes
  drawn indepdently from an exponential distribution in the range $[10^{-9},\,10^{9}]$.  Each is negated with probability $0.5$.
\end{itemize}
As can be see for these plots, the results mostly stay above the precision bound of Equation~\ref{eqn:digitprecision},
even though that bound does not apply.  All stay above the target precision of $30.0$.

On deeper inspection, we can see that setting up the conditions for a
cancellation of the form $(s + T) - T'$ (where $T \approx T'$) require
1) a large dynamic range among the computed values, and 2) sufficient
homogeneity in the computed values that we can get two values $T$ and
$T'$ such that $T \approx T'$.  A uniform distribution has neither of
these properties.  An exponential distribution has a large dynamic
range, but the computed values tend to be very heterogenous.

To create a weight assignment that has more chance of cancellation, we devised the following asignment:
\begin{itemize}
\item\textsf{Limits$\pm$}:  Each weight $w(x)$ and $w(\obar{x})$ has a magnitude of either $10^{-9}$ or $10^{+9}$, and they are each set negative with probability $0.5$.
  However, we exclude assignments with $w(x) + w(\obar{x}) = 0$.
\end{itemize}
The idea here is to give large dynamic ranges plus a high degree of
homogeneity.  The plots for this assignment in
Figure~\ref{fig:posneg:mpf} demonstrate that it achieves the desired
effect.  Although over one-half of the assignments yield computed
values that remain above the precision bound, many have digit
precision below the target of $30.0$.  We can also see how our choice
of weights leads to two bands of low precision: those where $s$ and
$T$ differ by a factor of around $10^{18}$, and those where they
differ by a factor of around $10^{36}$.

\section{Reliable and Efficient Weight Computation}

Our results with the formula of Equation~\ref{eqn:max:precision} and
with the \textsf{Limits$\pm$} weight assignment show that simply
computing weights with fixed-precision floating-point arithmetic can
yield inaccurate results.  Even worse, such an approach cannot even
detect whether the computed result provides any level of accuracy.

One way to achieve accuracy is to perform all computations with
rational arithmetic.  Unfortunately, this can be very time consuming,
and it provides a higher level of accuracy than is required for most
applications.  Furthermore, the time and memory required can exceed
the resources available.

A second strategy is to use floating point arithmetic for problems
where all weights are nonnegative and rational arithmetic otherwise.
This would provide guaranteed accuracy, but it can still be resource
intensive.  On the other hand, the example of
Equation~\ref{eqn:max:precision} shows us that some problems can only
be solved with rational arithmetic.




\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.95,0.25)}},
      legend cell align={left},
                              xmode=normal,xmin=0, xmax=50,
                              xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              xticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              ymode=normal, ymin=0, ymax=50,
                              ytick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                              yticklabels={0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0},
                              xlabel={Estimated Precision}, ylabel={Actual Precision}
            ]

    \draw[ fill={blue!30}, opacity=0.2] (axis cs: 0,30) rectangle (axis cs: 30,50);
    \node at (axis cs:15,40) {Overly pessimistic};
    

    \input{data-formatted/uposneg-mpfi-est+act}
    \input{data-formatted/eposneg-mpfi-est+act}
    \input{data-formatted/bposneg-mpfi-est+act}
    \legend{
      \scriptsize \textsf{MC2024, Uniform $\pm$},
      \scriptsize \textsf{MC2024, Exponential $\pm$},
      \scriptsize \textsf{MC2024, Limits $\pm$}
    }
    \addplot[mark=none] coordinates{(0,0) (50,50)};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Predictive Accuracy of MPFI Interval Arithmetic.  MPFI never has a higher estimate than the actual, but it can
incorrectly predict a precision less than the target of 30.0.}
\label{fig:mpfi}
\end{figure}

\begin{figure}
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[mark options={scale=1.0},height=10cm,width=10cm,grid=both, grid style={black!10},
      legend style={at={(0.40,0.98)}},
      legend cell align={left},
                              xmode=log,xmin=0.001, xmax=10000.0,
                              xtick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              xticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              ymode=log, ymin=0.001, ymax=10000.0,
                              ytick={1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4},
                              yticklabels={$0.001$, $0.01$, $0.1$, $1.0$, $10.0$, $100.0$, $1{,}000.0$, $10{,}000$},
                              xlabel={MPQ Runtime (secs.)}, ylabel={Hybrid Runtime (secs.)}
            ]

    \input{data-formatted/combo-mpf+mpq}
    \input{data-formatted/combo-mpfi+mpq}
    \input{data-formatted/combo-mpfi2+mpq}
    \input{data-formatted/combo-mpq+mpq}
    \legend{
      \scriptsize \textsf{MC2024, Hybrid-MPF},
      \scriptsize \textsf{MC2024, Hybrid-MPFI128},
      \scriptsize \textsf{MC2024, Hybrid-MPFI256},
      \scriptsize \textsf{MC2024, Hybrid-MPQ}
    }
    \addplot[mark=none] coordinates{(0.001,0.001) (10000.0,10000.0)};
    \node[left] at (axis cs: 10000, 1150) {$1.0\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.01, 0.001) (10000.0, 1000.0)};
    \node[left] at (axis cs: 10000, 115) {$0.1\times$};
    \addplot[mark=none, color=lightblue] coordinates{(0.1, 0.001)  (10000.0, 100.0)};
    \node[left] at (axis cs: 10000, 10) {$0.01\times$};
 \end{axis}
\end{tikzpicture}
} % centering
\caption{Runtime for hybrid method vs.~for MPQ.  When the value can be computed with MPF or MPFI, the runtime is signficantly better than using MPQ.  When it must resort to MPQ, it incurs some overhead.} 
\label{fig:runtime}
\end{figure}

\subsection{Comparing Implementation Strategies}

\begin{table}
  \caption{Performance Comparision of Different Implementation Strategies}
  \label{tab:compare}
  \centering{
  \begin{tabular}{llrrrrrr}
    \toprule
    \multicolumn{1}{c}{Strategy} & & \multicolumn{1}{c}{MPF} & \multicolumn{1}{c}{MPFI-128} & \multicolumn{1}{c}{MPFI-256} & \multicolumn{1}{c}{MPQ} & \multicolumn{1}{c}{Combined} & \multicolumn{1}{c}{Avg (s)}\\
    \midrule
    \input{method_table}
    \\[-1em]
   \bottomrule
  \end{tabular}
  } % Centering
\end{table}

\newpage
\bibliography{references}

\end{document}

Comparisons:
\begin{center}
  \begin{tabular}{llr}
    \toprule
    $M_1$ & $M_2$ & Ratio \\
    \midrule
    MPQ & MPF & \avgMpqMpf \\
    MPF & Hybrid & \avgMpfHybrid \\    
    MPQ & Hybrid & \avgMpqHybrid \\
    \bottomrule
  \end{tabular}
\end{center}


%%
%% End of file
