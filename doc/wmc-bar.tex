\documentclass[letterpaper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xnewcommand}

%\newtheorem{proposition}{Proposition}

\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lit}{\ell}

\newcommand{\progname}[1]{\textsc{#1}}
\newcommand{\dfour}{\progname{D4}}
\newcommand{\Dfour}{\progname{D4}}

\newcommand{\approximate}[1]{\hat{#1}}
\newcommand{\approxP}{\approximate{P}}
\newcommand{\approxx}{\approximate{x}}
\newcommand{\approxy}{\approximate{y}}
\newcommand{\approxv}{\approximate{v}}
\newcommand{\approxV}{\approximate{V}}
\newcommand{\approxw}{\approximate{w}}
\newcommand{\approxW}{\approximate{W}}
\newcommand{\approxs}{\approximate{s}}
\newcommand{\round}{\mathit{Rnd}}
\newcommand{\aerror}{\delta}
\newcommand{\digitprecision}{\Delta}
\newcommand{\minvalue}{\omega}
\newcommand{\maxValue}{\Omega}
\newcommand{\roundepsilon}{\varepsilon}
\newcommand{\xmin}{x^{-}}
\newcommand{\xmax}{x^{+}}
\newcommand{\vmin}{v^{-}}
\newcommand{\vmax}{v^{+}}
\newcommand{\interval}[1]{[\![#1]\!]}
\newcommand{\varset}{X}
\newcommand{\dependencyset}{{\cal V}}
\newcommand{\assign}{\alpha}
\newcommand{\modelset}{{\cal M}}
\newcommand{\entails}{\models}


\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

%% \definecolor{dbl}{rgb}{0.8, 0.8, 0.8}
%% \definecolor{mpflow}{rgb}{0.8, 0.4, 0.8}
%% \definecolor{mpfmed}{rgb}{0.5, 0.9, 0.9}
%% \definecolor{mpfhigh}{rgb}{0.8, 0.4, 0.4}
%% \definecolor{mpfilow}{rgb}{0.4, 0.7, 0.4}
%% \definecolor{mpfimed}{rgb}{0.4, 0.4, 0.8}
%% \definecolor{mpfihigh}{rgb}{0.9, 0.9, 0.3}
%% \definecolor{mpq}{rgb}{0.4, 0.4, 0.4}

\definecolor{dbl}{RGB}{49,97,160}
\definecolor{mpflow}{RGB}{96,165,200}
\definecolor{mpfmed}{RGB}{161,207,223}
\definecolor{mpfhigh}{RGB}{224,243,248}
\definecolor{mpfilow}{RGB}{215,48,39}
\definecolor{mpfimed}{RGB}{253,174,97}
\definecolor{mpfihigh}{RGB}{254,224,144}
\definecolor{mpq}{RGB}{100,100,100}



\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Numerical Considerations \\in Weighted Model Counting}

\titlerunning{Numerical Considerations for Weighted Model Counting}

%%\author{Randal E. Bryant}{Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA}{Randy.Bryant@cs.cmu.edu}{https://orcid.org/0000-0001-5024-6613}{}

%%\authorrunning{R. E. Bryant} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

%%\Copyright{Randal E. Bryant} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\author{Anonymous}{Unknown Institution}{}{}{}
\authorrunning{Anon.}
\Copyright{Anon.}

\ccsdesc[500]{Theory of computation~Automated reasoning}
\keywords{Weighted model counting, floating-point arithmetic, interval arithmetic, decision diagrams}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%% \begin{figure}
%% \centering{
%%   \begin{tikzpicture}
%%   \node at (0, 0.0) {\includegraphics{wmc-tabulate-count}};
%%   \node at (3.7, -0.45) {\includegraphics{wmc-tabulate-effort}};
%%   \end{tikzpicture}
%% } %% centering
%% \caption{Number of instances solved (left) and total execution time (right) for each evaluation method, for selected values of target precision $D$}
%% \label{fig:solution:tabulation}
%% \end{figure}


Of the 1470 instances  containing negative weights where the MPQ evaluation completed, 1196 ($81.4\%$)
were successfully evaluated using MPFI with $p=128$.  Considering the
825 instances for which MPQ required more than $1.0$ seconds, we find
that MPFI ran between $2.0$ and $41.1$ times faster, with an average of
$8.8$ and a median of $7.1$.  Again, this level of evaluation had a
clear benefit for performance.  An additional 164 instances
($11.1\%$) were successfully evaluated using MPFI with $p=256$.  Of
the 96 instances for which MPQ required more than $1.0$ seconds, we
find that the combined time for two runs with MPFI was, in the worst
case, $1.04$ times higher than just running MPQ\@.  Overall, however,
the combination ran faster by as much as $9.8\times$, with an average
of $3.1$ and a median of $2.6$.  Again, using this level of evaluation
proved worthwhile.  Finally, 110 instances ($7.5\%$) required an
evaluation using MPQ\@.  In these cases, the hybrid runtime was
greater than that for MPQ alone, since the program also performed two
evaluations using MPFI\@.  Of the 82 instances for which MPQ required
more than $1.0$ seconds, we find that the hybrid approach ran between
$1.1$ and $1.7$ times slower, with an average and a median of $1.2$.
Fortunately, this performance penalty was more than offset by the gains achieved by the less costly evaluation methods.



\begin{figure}[t]
\centering{%
\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[
      ybar stacked,
      width = 12cm,
      height=8cm,
      bar width=15pt,
%%      nodes near coords,
%%      enlargelimits=0.15,
      legend style={at={(0.435,1.05)},
      anchor=south, legend columns=-1},
      ylabel = {Instance count},
%      xlabel = {Target precision $D$},
      ytick = {500, 1000, 1500, 2000, 2500},
      yticklabels ={500, 1000, 1500, 2000, 2500},
      symbolic x coords={1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70},
      xtick=data,
      ]
\input{data-formatted/tabulate-count}
\legend{\strut Double, \strut MPF-64, \strut MPF-128, \strut MPF-256, \strut MPFI-64, \strut MPFI-128, \strut MPFI-256, \strut MPQ}
  \end{axis}
\end{tikzpicture}

\begin{tikzpicture}%[scale = 0.70]
  \begin{axis}[
      ybar stacked,
      width = 12cm,
      height=8cm,
      bar width=15pt,
%%      nodes near coords,
%%      enlargelimits=0.15,
%      legend style={at={(0.42,-0.20)},
%        anchor=north, legend columns=-1},
      ylabel = {Total Time (hours)},
      xlabel = {Target precision $D$},
      ymax = 30,
      ytick = {5, 10, 15, 20, 25, 30},
      yticklabels = {5, 10, 15, 20, 25, 30},
      symbolic x coords={1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70},
      xtick=data,
      ]
\input{data-formatted/tabulate-effort}
%\legend{\strut Double, \strut MPF-64, \strut MPF-128, \strut MPF-256, \strut MPFI-64, \strut MPFI-128, \strut MPFI-256, \strut MPQ}
  \end{axis}
\end{tikzpicture}
} % centering
\caption{Number of instances solved (top) and total execution time (bottom) for each method, as a function of target precision $D$}
\label{fig:solution:effort}
\end{figure}




\end{document}

